<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[常用算法总结]]></title>
    <url>%2F2019%2F06%2F14%2F%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[二分查找 给定一个数组（有序列表）和一个数字，要求查出该数字所在的索引位置。 def binary_search(list, item): # lowh和high用于跟踪要在其中查找的列表部分 low = 0 high = len(list) - 1 # 只要范围没有缩小到只包含一个元素，就检查中间的元素 while low &lt;= high: mid = (low + high) // 2 guess = list[mid] # 找到了元素 if guess == item: return mid # 猜的数字大了 if guess &gt; item: high = mid - 1 # 猜的数字小了 else: low = mid + 1 # 没有指定的元素 return None# 测试my_list = [1, 3, 5, 7, 9]print(binary_search(my_list, 3)) # =&gt; 1print(binary_search(my_list, -1)) # =&gt; None 总结： 二分查找是对数时间，时间复杂度为O(logn) 简单查找是线性时间，时间复杂度为O(n) O(logn)比O(n)快。需要搜索的元素越多，前者比后者就快越多 算法运行时间并不以秒为单位 算法运行时间是从其增速的角度度量的 选择排序 将数组元素按从小到大的顺序排列。 # 找出数组中的最小元素def findSmallest(arr): # 存储最小的值 smallest = arr[0] # 存储最小元素的索引 smallest_index = 0 for i in range(1, len(arr)): if arr[i] &lt; smallest: smallest_index = i smallest = arr[i] return smallest_index# 对数组进行排序def selectionSort(arr): newArr = [] for i in range(len(arr)): # 找出数组中最小元素的索引，并将其加入到新数组中 smallest_index = findSmallest(arr) newArr.append(arr.pop(smallest_index)) # 原数组arr不断的删除最小的元素 return newArrprint(selectionSort([5, 3, 6, 2, 10])) 选择排序，每检查一次数组，找出最小元素，运行时间都为O(n)，而这个操作需要执行n次，因此其时间复杂度为O(n2)（其中的常数1/2要省略） 总结： 需要存储多个元素时，可使用数组或链表 数组的元素都在一起，链表的元素是分开的，其中每个元素都存储了下一个元素的地址 数组的读取速度很快，链表的插入和删除速度很快 递归基线条件和递归条件def countdown(i): # 基线条件 if i &lt;= 0: return 0 # 递归条件 else: print(i) return countdown(i-1)countdown(5) 调用栈（call stack）def greet2(name): print("how are you, ", name, "?") def bye(): print("ok bye!") def greet(name): print("hello, ", name, "!") greet2(name) print("getting ready to say bye...") bye() greet("adit") 递归调用栈def fact(x): if x == 1: return 1 else: return x * fact(x-1) print(fact(5)) 说明： 每个fact函数调用都有自己的x变量，但是不能访问其他函数调用的变量x 最后一次被调用的函数先返回，然后接着返回之前的调用 总结： 递归指的是调用自己的函数 每个递归函数都有两个条件：基线条件和递归条件 栈有两种操作：压入和弹出 所有函数调用都进入调用栈 调用栈可能很长，这将占用大量的内存 快速排序分治算法（D&amp;C算法）# 循环求和def sum(arr): total = 0 for x in arr: total += x return totalprint(sum([1, 2, 3, 4])) # 递归求和def sum(list): if list == []: return 0 return list[0] + sum(list[1:])print(sum([1, 2, 3])) # 递归计算列表包含的元素数def count(list): if list == []: return 0 return 1 + count(list[1:])print(count([1, 2, 3])) # 递归找出列表中最大的数字def max_(lst): if len(lst) == 0: return 0 if len(lst) == 1: return lst[0] else: sub_max = max_(lst[1:]) return lst[0] if lst[0] &gt; sub_max else sub_max print(max_([1, 2, 5, 3])) 快速排序def quicksort(array): # 基线条件：为空或只包含一个元素的数组是“有序”的 if len(array) &lt; 2: return array else: # 递归条件 pivot = array[0] # 由所有小于基准值的元素组成的子数组 less = [i for i in array[1:] if i &lt;= pivot] # 由所有大于基准值的元素组成的子数组 greater = [i for i in array[1:] if i &gt; pivot] return quicksort(less) + [pivot] + quicksort(greater) print(quicksort([10, 5, 2, 3])) 总结： D&amp;C将问题逐步分解。使用D&amp;C处理列表时，基线条件很可能是空数组或只包含一个元素的数组 实现快速排序时，请随机地选择用作基准值的元素。快速排序的平均运行时间为O(n log n) 大O表示法中的常量有时候事关重大，这就是快速排序比合并排序快的原因所在 比较简单查找和二分查找时，常量几乎无关紧要，因为列表很长时，O(log n)的速度比O(n)快得多 散列表# 散列表book = &#123;"apple": 0.67, "milk": 1.49, "avocado": 1.49&#125;print(book)print(book["apple"]) # 散列表防止重复voted = &#123;&#125;def check_voter(name): if voted.get(name): print("kick them out!") else: voted[name] = True print("let them vote!") check_voter("tom")check_voter("mike")check_voter("mike") 总结： 散列表的查找、插入和删除速度都非常快 散列表适合用于模拟映射关系 散列表可用于缓存数据（例如，在Web服务器上） 散列表非常适合用于防止重复 广度优先搜索广度优先搜索解决了两类问题： 第一类问题：从节点A出发，有前往节点B的路径吗？ 第二类问题：从节点A出发，前往节点B的哪条路径最短？ from collections import dequedef person_is_seller(name): return name[-1] == 'm'graph = &#123;&#125;graph["you"] = ["alice", "bob", "claire"]graph["bob"] = ["anuj", "peggy"]graph["alice"] = ["peggy"]graph["claire"] = ["thom", "jonny"]graph["anuj"] = []graph["peggy"] = []graph["thom"] = []graph["jonny"] = []def search(name): # 创建一个队列 search_queue = deque() # 将你的邻居都加入到这个搜索队列中 search_queue += graph[name] # 这个数组用于记录检查过的人 searched = [] while search_queue: # 只要队列不为空，就取出其中的第一个人，并从队列中移除 person = search_queue.popleft() # 仅当这个人没检查过时才检查 if person not in searched: # 检查这个人是否是芒果销售商 if person_is_seller(person): print(person + " is a mango seller!") return True else: # 不是芒果销售商。将这个人的朋友都加入搜索队列 search_queue += graph[person] # 将这个人标记为检查过 searched.append(person) return Falsesearch("you") 总结： 广度优先搜索指出是否有从A到B的路径，如果有，广度优先搜索将找出最短路径 面临类似于寻找最短路径的问题时，可尝试使用图来建立模型，再使用广度优先搜索来解决问题 有向图中的边为箭头，箭头的方向指定了关系的方向，例如，rama→adit表示rama欠adit钱 无向图中的边不带箭头，其中的关系是双向的，例如，ross - rachel表示“ross与rachel约会，而rachel也与ross约会” 队列是先进先出（FIFO）的，栈是后进先出（LIFO）的 你需要按加入顺序检查搜索列表中的人，否则找到的就不是最短路径，因此搜索列表必须是队列 对于检查过的人，务必不要再去检查，否则可能导致无限循环 狄克斯特拉算法对比广度优先搜索，狄克斯特拉算法采用了“加权图”的概念。 # the graphgraph = &#123;&#125;graph["start"] = &#123;&#125;graph["start"]["a"] = 6graph["start"]["b"] = 2graph["a"] = &#123;&#125;graph["a"]["fin"] = 1graph["b"] = &#123;&#125;graph["b"]["a"] = 3graph["b"]["fin"] = 5graph["fin"] = &#123;&#125;# the costs tableinfinity = float("inf")costs = &#123;&#125;costs["a"] = 6costs["b"] = 2costs["fin"] = infinity# the parents tableparents = &#123;&#125;parents["a"] = "start"parents["b"] = "start"parents["fin"] = Noneprocessed = []def find_lowest_cost_node(costs): lowest_cost = float("inf") lowest_cost_node = None # 遍历所有的节点 for node in costs: cost = costs[node] # 如果当前节点的开销更低且未处理过 if cost &lt; lowest_cost and node not in processed: # 就将其视为开销最低的节点 lowest_cost = cost lowest_cost_node = node return lowest_cost_node# 在未处理的节点中找出开销最小的节点node = find_lowest_cost_node(costs)# 这个while循环在所有节点都被处理过后结束while node is not None: cost = costs[node] neighbors = graph[node] # 遍历当前节点的所有邻居 for n in neighbors.keys(): new_cost = cost + neighbors[n] # 如果经当前节点前往该邻居更近 if costs[n] &gt; new_cost: # 就更新该邻居的开销 costs[n] = new_cost # 同时将该邻居的父节点设置为当前节点 parents[n] = node # 将当前节点标记为处理过 processed.append(node) # 找出接下来要处理的节点，并循环 node = find_lowest_cost_node(costs) print("cost from the start to each node:")print(costs) 总结： 广度优先搜索用于在非加权图中查找最短路径 狄克斯特拉算法用于在加权图中查找最短路径 仅当权重为正时狄克斯特拉算法才管用 如果图中包含负权边，请使用贝尔曼福德算法 贪婪算法# 需要覆盖的州states_needed = set(["mt", "wa", "or", "id", "nv", "ut", "ca", "az"])# 广播台覆盖的州stations = &#123;&#125;stations["kone"] = set(["id", "nv", "ut"])stations["ktwo"] = set(["wa", "id", "mt"])stations["kthree"] = set(["or", "nv", "ca"])stations["kfour"] = set(["nv", "ut"])stations["kfive"] = set(["ca", "az"])final_stations = set()while states_needed: # 覆盖了最多的未覆盖州的广播台 best_station = None # 包含该广播台覆盖的所有未覆盖的州 states_covered= set() for station, states_for_station in stations.items(): # 计算交集，同时出现在states_needed和states_for_station中的州 covered = states_needed &amp; states_for_station # 该广播台覆盖的州比best_station多 if len(covered) &gt; len(states_covered): best_station = station states_covered = covered states_needed -= states_covered final_stations.add(best_station) print(final_stations) 总结： 贪婪算法寻找局部最优解，企图以这种方式获得全局最优解 对于NP完全问题，还没有找到快速解决方案 面临NP完全问题时，最佳的做法是使用近似算法 贪婪算法易于实现、运行速度快，是不错的近似算法 动态规划# 最长公共子串# 两个字母相同if word_a[i] == word_b[j]: # 将当前单元格的值设置为左上方单元格的值加1 cell[i][j] = cell[i-1][j-1] + 1# 两个字母不同else: # 值为0 cell[i][j] = 0 # 最长公共子序列# 两个字母相同if word_a[i] == word_b[j]: # 将当前单元格的值设置为左上方单元格的值加1 cell[i][j] = cell[i-1][j-1] + 1# 两个字母不同else: # 选择上方和左方邻居中较大的那个 cell[i][j] = max(cell[i-1][j], cell[i][j-1]) 总结： 需要在给定约束条件下优化某种指标时，动态规划很有用 问题可分解为离散子问题时，可使用动态规划来解决 每种动态规划解决方案都涉及网格 单元格中的值通常就是你要优化的值 每个单元格都是一个子问题，因此你需要考虑如何将问题分解为子问题 没有放之四海皆准的计算动态规划解决方案的公式 K最近邻算法总结： KNN用于分类和回归，需要考虑最近的邻居 分类就是编组 回归就是预测结果（如数字） 特征抽取意味着将物品（如水果或用户）转换为一系列可比较的数字 能否挑选合适的特征事关KNN算法的成败]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop知识点总结]]></title>
    <url>%2F2019%2F04%2F30%2FHadoop%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[HDFS 和YARN 的基本概念HDFS分布式文件系统，主/从架构 NameNode：负责管理元数据（文件名称，副本数量，文件位置，块大小）。HDFS 存储是以块存储 默认块大小 128MB，hadoop1 中默认的块大小是64MB。一个节点。 DataNode：主要存储真正的数据，多节点。 secondaryNamenode：辅助节点，用于合并两类文件。 Fsimage, edits：作为元数据的镜像和操作的日志记录。 在HDFS第一次使用的时候需要对其进行格式化，目的是生成fsimage 和 edits 文件。 YARN资源管理器：YARN为这些运行在操作系统上的任务分配资源，管理。 主节点（resourceManager）：负责全部集群中的资源管理，和任务分配 从节点（nodeManager）：负责每个机器上的资源管理 HDFS的优缺点HDFS优点 高容错性：数据自动保存多个副本，副本丢失后，会自动恢复。 适合批处理：移动计算而非数据、数据位置暴露给计算框架。 适合大数据处理：GB、TB、甚至PB级数据、百万规模以上的文件数量，1000以上节点规模。 流式文件访问：一次性写入，多次读取；保证数据一致性。 可构建在廉价机器上：通过多副本提高可靠性，提供了容错和恢复机制。 HDFS缺点 低延迟数据访问：比如毫秒级、低延迟与高吞吐率。 小文件存取：占用NameNode大量内存，寻道时间超过读取时间。 并发写入、文件随机修改：一个文件只能有一个写者，仅支持append HDFS的副本复制策略 HDFS读写数据流程HDFS读数据流程 客户端通过向namenode请求下载文件 ，namenode 收到请求之后查询元数据信息,找到datanode数据块的信息。 客户端挑选一台就近的datanode,进行请求数据。 datanode开始传输数据给客户端，是以packet 为单位进行读取。 客户端 接收packet 数据,先在本地缓存，最后写入到目标文件。 HDFS写数据流程 客户端向namenode请求上传文件，namenode检测该文件是否已存在，父目录是否存在，然后返回是否可以上传。 客户端请求上传第一个block，namenode返回三个节点（dn1,dn2,dn3）。 客户端向dn1请求上传数据，dn1收到请求后会调用dn2，dn2调用dn3，建立传输通道，dn1、dn2、dn3逐级应答。 客户端开始往dn1上传第一个block（先从磁盘读取放到一个本地内存缓存），以packet为单位。dn1收到一个block就会传给dn2，dn2传给dn3。dn1每传完一个packet会被放入一个应答队列等待应答。 当一个block传输完成之后，客户端再次向namenode请求上传第二、第三个block，重复上面的步骤（2-4步），直至文件上传完成。 NameNode与DataNode的工作机制NameNode工作机制 第一次启动:第一次启动都需要格式化nameNode ,创建fsimage,edits. 第一次启动只需要加载fsiamge。 如果不是第一次启动： 直接加载edits ,fsimage镜像文件 ，合并成一个新的fsimage 文件，再创建edits 文件记录新的操作行为。 启动的过程中，会存在30秒钟等待时间 ，这个等待的时间就是安全模式。 DataNode工作机制 一个数据块在datanode上是以文件形式存储在磁盘上的，包括了两个文件，一个数据本身，一个是元数据包 包括数据块的长度，,数据块的校验和,由于HDFS上的数据是不允许被重复上传的所以在上传之前会对上传的数据进行检查 ,时间戳。 DataNode启动后会向nameNode进行注册，通过后，会周期性的向namenode上报自己的datanode上的块信息。 心跳报告，每3秒钟向nameNode进行汇报,心跳的返回结果中带有NameNode 带给该datanode复制数据块，移动数据块的命令, 如果说超过了10分钟datanode没有响应 ，则就会认为这个datanode节点不可用,会选择其他的机器。 如果NameNode意外终止，secondaryNameNode的工作是什么？它是如何工作的？ 并非NameNode的热备； 辅助NameNode，分担其工作量； 定期合并fsimage和edits，推送给NameNode； 在紧急情况下，可辅助恢复NameNode。 HDFS安全模式在系统的正常操作期间，namenode会在内存中保留所有块位置的映射信息。在安全模式下，各个datanode会向namenode发送最新的块列表信息，namenode了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”，namenode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以namenode不会进入安全模式。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase知识点总结]]></title>
    <url>%2F2019%2F04%2F30%2FHBase%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[HBase的架构 HMaster： 负责HBase中table和region的管理，regionserver的负载均衡，region分布调整，region分裂以及分裂后的region分配，regionserver失效后的region迁移等。 Zookeeper： 存储root表的地址和master地址，regionserver主动向zookeeper注册，使得master可随时感知各regionserver的健康状态。避免master单点故障。 RegionServer： HRegion Server主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。 HRegion Server内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，Region中由多个Store组成。每个Store对应了Table中的一个Column Family的存储，即一个Store管理一个region上的一个列簇。每个Store包含一个MemStore和0到多个StoreFile。Store是HBase存储核心，由MemStore和StoreFiles组成。 MemStore： MemStore是Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。 HBase中的rowkey以及热点问题HBase热点现象：检索hbase的记录首先要通过rowkey来定义数据行，当大量的client访问hbase集群的一个或少数几个节点，造成少数region server的读/写请求过多，负载过大，最终导致单个主机负载过大，引来性能下降甚至region不可用。 热点产生原因：有大量连续编号的rowkey，导致大量记录集中在个别region。 避免热点方法： 加盐：在rowkey的前面增加随机数。此方式适用于将hbase作为海量存储数据而不频繁查询的业务场景。 哈希：Hash散列，将数据打乱。 反转：包括rowkey字段反转和时间戳反转。例如联通就是这样。时间戳加手机号20181123_13191***，引入一张索引表存储以上字段，hbase中的rowkey则是以上字段的反转。 rowkey设计原则： rowkey唯一原则 rowkey长度原则： 二进制数，可以是任意字符，最多给到64kb，建议10-100个字节，但是越短越好，最好不要超过16个字节。原因如下： 数据都是存在Hfile中按照key-value进行存储的，如果rowkey超过了100个字节，1000万条数据，100*1000万=10亿个字节，约为1G，极大浪费Hfile的存储资源。 memstore将缓存部分数据到内存，如果rowkey过大，内存的有效利用率就会降低，从而降低检索效率。 HBase的应用场景 需对数据进行随机读写操作； 大数据上高并发操作，比如每秒对PB级数据进行上千次操作； 读写访问均是非常简单的操作。 HBase的寻址过程（读写数据过程）client–zookeeper–root–meta–region客户端先通过zookeeper获取到root表的地址，通过root表获取.meta表的地址，.meta表上记录了具体的region的地址。]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive知识点总结]]></title>
    <url>%2F2019%2F04%2F30%2FHive%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[hive与mysql的区别 回答思路：hive背景（原理、本质）–&gt;两者操作、本质的差别–&gt;读写差别–&gt;其它差别。 Hive的诞生背景：学mysql的也想入门大数据，但又不会java，于是hive就诞生了。Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表。本质是将HQL语句转化为MR程序。 hive总体来说操作等方面和MySQL没有太大差别。但是本质却有差别，Hive注重联机分析的处理，mysql注重事务的处理。Hive注重的是分析，mysql注重的是处理。 MySQL在写的时候检查字段，hive在读的时候检查字段。所以在新增数据时，hive比较快，只需要直接load就行，而MySQL需要先检查字段。 hive与mysql的其它差别如下： 数据存储位置：Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。 执行：Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。 执行延迟：hive高延迟，mysql低延迟。 可扩展性：hive的可扩展性高，而数据库由于ACID语义限制，扩展性有限。 数据规模：hive可以支持很大规模的数据，数据库可以支持的数据规模较小。 hive内部表和外部表内部表和外部表的区别未被external修饰的是内部表（managed table），被external修饰的为外部表（external table）。 区别： 内部表数据由Hive自身管理，外部表数据由HDFS管理。 删除内部表会直接删除元数据（metadata）及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除。 对内部表的修改会将修改直接同步给元数据，而对外部表的表结构和分区进行修改，则需要修复（MSCK REPAIR TABLE table_name;） 内部表和外部表的转换alter table t_newuser set TBLPROPERTIES(‘EXTERNAL’=‘TRUE’); ###true一定要大写alter table t_newuser set TBLPROPERTIES(‘EXTERNAL’=‘false’); ###false大小写都没关系 hive分区的三种类型 静态分区：加载数据的时候指定分区的值。 动态分区：数据未知，根据分区的值确定创建分区。 混合分区：静态加动态。 hive的存储格式 Textfile未经过压缩的。 Sequencefile：hive为用户提供的二进制存储，本身就压缩，不能使用load方式加载数据。 Rcfile：hive提供的行列混合存储，hive在该格式下，会尽量将附近的行和列的块存储到一起，仍然是压缩格式，查询效率比较高。 Orc是rcfile的升级版本。 Parquet列式存储。 hive窗口函数 FIRST_VALUE：取分组内排序后，截止到当前行，第一个值 。 LAST_VALUE： 取分组内排序后，截止到当前行，最后一个值 。 LEAD(col,n,DEFAULT) ：用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL） 。 LAG(col,n,DEFAULT) ：与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）。 hive的4种排序方式 order by全局排序。全部数据划分到一个reduce上。与sql中的order by类似，不同的是，hive中的order by在严格模式下，必须跟limit。 sort by 每个mapreduce内部排序。 distributed by分区排序，与sql中的group by类似，常与sort by组合使用，distributed by控制map的输出在reduce中如何划分，sort by控制reduce中的数据如何排序。hive要求distributed by语句出现在sort by语句之前。 cluster by，当distributed by与sort by字段相同，可以用cluster by代替该组合，但cluster by 不能跟desc，asc。补充：可以这样书写select a.* from (select * from test cluster by id ) a order by a.id; hive调优方式 Fetch抓取：把不需要MR任务计算的查询语句设置成不执行MR任务。三个参数，none表示禁用Fetch，所有查询都执行MR任务；more在进行select/filter/limit查询时不会运行MR任务；minimal在select/limit的时候不会运行MR任务，但是filter会运行MR任务。hive.fetch.task.conversion 本地模式：让输入的数据量特别小的任务直接在本地节点上进行处理，而不提交到集群。本地模式通过判断文件的大小（默认128MB）和已输入文件的个数（默认4个）来判断是否在本地执行。set hive.exec.mode.local.auto=true; //开启本地 mrset hive.exec.mode.local.auto.inputbytes.max=50000000;set hive.exec.mode.local.auto.input.files.max=10; 表的优化：优化手段有join、行列过滤、分区、分桶、动态分区等等。 避免数据倾斜：通过合理设置map和reduce数、小文件合并等方式尽量保证负载均衡。或为了避免因为map或reduce任务卡死导致数据倾斜，通常也设置推测执行。 推测执行：为了避免因为程序的BUG/负载不均衡/资源分布不均等原因导致同一作业中某一任务运行速度过慢，设置推测执行，为该任务启动一个备份任务，同时执行，最先运行完成的计算结果作为最终结果。分为map端和reduce端的推测执行。set mapreduce.map.speculative=trueset mapreduce.reduce.speculative=true 并行执行：把没有依赖关系的MR任务设置为并行执行，提高多任务运行时的效率。set hive.exec.parallel=true ; // 开启任务并行执行set hive.exec.parallel.thread.number=8; //默认值为8个任务可以同时运行 严格模式：为了防止一些不正常的查询语句的执行。hive.mapred.mode= strict JVM重用：当有很多小文件的时候，每次运行MR任务都会开启一个JVM进程，JVM频繁的开启关闭消耗大量的性能，所以在处理小文件的时候，可以设置JVM重用，让一个JVM处理多个任务后再关闭。mapreduce.job.jvm.numtasks 压缩：通过压缩对项目进行优化。例如开启 map 输出阶段压缩可以减少 job 中 map 和 Reduce task 间数据传输量。 执行计划：Hive中提供的可以查看Hql语句的执行计划，在执行计划中会生成抽象语法树，在语法树中会显示HQL语句之间的依赖关系以及执行过程。通过这些执行的过程和依赖可以对HQL语句进行优化。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库知识点总结]]></title>
    <url>%2F2019%2F04%2F30%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1. 事务四大特性（ACID）原子性、一致性、隔离性、持久性？ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency） 事务开始前和结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 2. 事务的并发？事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别？从理论上来说, 事务应该彼此完全隔离, 以避免并发事务所导致的问题，然而, 那样会对性能产生极大的影响, 因为事务必须按顺序运行，在实际开发中, 为了提升性能, 事务会以较低的隔离级别运行， 事务的隔离级别可以通过隔离事务属性指定。 2.1 事务的并发问题 脏读 事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据. 不可重复读 事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。 幻读 幻读解决了不重复读，保证了同一个事务里，查询的结果都是事务开始时的状态（一致性）。 例如：事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作 这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。 而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有跟没有修改一样，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 2.2 事务的隔离级别*数据库隔离级别：是在在数据库操作中，为了有效保证并发读取数据的正确性提出的。* 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 READ UNCOMMITTED（读未提交数据） 允许事务读取未被其他事务提交的变更数据，会出现脏读、不可重复读和幻读问题。 READ COMMITTED（读已提交数据） 只允许事务读取已经被其他事务提交的变更数据，可避免脏读，仍会出现不可重复读和幻读问题。 REPEATABLE READ（可重复读） 确保事务可以多次从一个字段中读取相同的值，在此事务持续期间，禁止其他事务对此字段的更新，可以避免脏读和不可重复读，仍会出现幻读问题。 SERIALIZABLE（序列化） 确保事务可以从一个表中读取相同的行，在这个事务持续期间，禁止其他事务对该表执行插入、更新和删除操作，可避免所有并发问题，但性能非常低。 Oracle支持两种事务隔离级别：READ COMMITTED（默认事务隔离级别），SERIALIZABLE。 MySQL支持四种事务隔离级别，其中REPEATABLE READ为默认事务隔离级别。 3. MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别？ 3.1 InnoDB存储引擎InnoDB是事务型数据库的首选引擎，支持事务安全表（ACID），其它存储引擎都是非事务安全表，支持行锁定和外键，MySQL5.5以后默认使用InnoDB存储引擎。 InnoDB特点： 支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。 如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）。 3.2 MyISAM存储引擎MyISAM基于ISAM存储引擎，并对其进行扩展。它是在Web、数据仓储和其他应用环境下最常使用的存储引擎之一。MyISAM拥有较高的插入、查询速度，但不支持事务，不支持外键。 MyISAM特点： 插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择MyISAM能实现处理高效率。如果应用的完整性、并发性要求比较低，也可以使用 3.3 MEMORY存储引擎MEMORY存储引擎将表中的数据存储到内存中，为查询和引用其他表数据提供快速访问。 MEMORY特点： 所有的数据都在内存中，数据的处理速度快，但是安全性不高。如果需要很快的读写速度，对数据的安全性要求较低，可以选择MEMOEY。 它对表的大小有要求，不能建立太大的表。所以，这类数据库只使用在相对较小的数据库表。 4. MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景？4.1 事务处理上方面MyISAM：强调的是性能， 每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 4.2 锁级别MyISAM：只支持表级锁， 用户在操作MyISAM表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 InnoDB： 支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 5. 查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序？5.1 执行顺序 from:需要从哪个数据表检索数据 where:过滤表中数据的条件 group by:如何将上面过滤出的数据分组 having:对上面已经分组的数据进行过滤的条件 select:查看结果集中的哪个列，或列的计算结果 order by :按照什么样的顺序来查看返回的数据 5.2 语句解析顺序from后面的表关联，是自右向左解析 而where条件的解析顺序是自下而上的。 也就是说，在写SQL的时候，尽量把数据量小的表放在最右边来进行关联（用小表去匹配大表），而把能筛选出小量数据的条件放在where语句的最左边 （用小表去匹配大表） 6. 什么是临时表，临时表什么时候删除？MySQL 临时表在我们需要保存一些临时数据时是非常有用的。临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。 使用其他MySQL客户端程序连接MySQL数据库服务器来创建临时表，那么只有在关闭客户端程序时才会销毁临时表，当然也可以手动删除。 7. MySQL B+Tree索引和Hash索引的区别？CREATE TABLE act_info(id BIGINT NOT NULL AUTO_INCREMENT,act_id VARCHAR(50) NOT NULL COMMENT "活动id",act_name VARCHAR(50) NOT NULL COMMENT "活动名称",act_date datetime NOT NULL,PRIMARY KEY(id),KEY idx_actid_name(act_id,act_name) USING BTREE) ENGINE=INNODB DEFAULT CHARSET=UTF8 ROW_FORMAT=COMPACT COMMENT "活动记录表"; 7.1 B-TREE索引 B-TREE索引的特点 B-TREEB-TREE以B+树结构存储数据，大大加快了数据的查询速度 B-TREE索引在范围查找的SQL语句中更加适合（顺序存储） B-TREE索引使用场景 全值匹配的查询SQL，如 where act_id= ‘1111_act’ 联合索引汇中匹配到最左前缀查询，如联合索引 KEY idx_actid_name(act_id,act_name) USING BTREE，只要条件中使用到了联合索引的第一列，就会用到该索引，但如果查询使用到的是联合索引的第二列act_name，该SQL则便无法使用到该联合索引（注：覆盖索引除外） 匹配模糊查询的前匹配，如where act_name like ‘11_act%’ 匹配范围值的SQL查询，如where act_date &gt; ‘9865123547215’（not in和&lt;&gt;无法使用索引） 覆盖索引的SQL查询，就是说select出来的字段都建立了索引 7.2 HASH索引HASH的特点 Hash索引基于Hash表实现，只有查询条件精确匹配Hash索引中的所有列才会用到hash索引 存储引擎会为Hash索引中的每一列都计算hash码，Hash索引中存储的即hash码，所以每次读取都会进行两次查询 Hash索引无法用于排序 Hash不适用于区分度小的列上，如性别字段 8. 聚集索引和非聚集索引区别？聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。 8.1 聚集索引聚集索引表记录的排列顺序和索引的排列顺序一致，所以查询效率快，只要找到第一个索引值记录，其余就连续性的记录在物理也一样连续存放。聚集索引对应的缺点就是修改慢，因为为了保证表中记录的物理和索引顺序一致，在记录插入的时候，会对数据页重新排序。 聚集索引类似于新华字典中用拼音去查找汉字，拼音检索表于书记顺序都是按照a~z排列的，就像相同的逻辑顺序于物理顺序一样，当你需要查找a,ai两个读音的字，或是想一次寻找多个傻(sha)的同音字时，也许向后翻几页，或紧接着下一行就得到结果了。 8.2 非聚集索引非聚集索引制定了表中记录的逻辑顺序，但是记录的物理和索引不一定一致，两种索引都采用B+树结构，非聚集索引的叶子层并不和实际数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针方式。非聚集索引层次多，不会造成数据重排。 非聚集索引类似在新华字典上通过偏旁部首来查询汉字，检索表也许是按照横、竖、撇来排列的，但是由于正文中是a~z的拼音顺序，所以就类似于逻辑地址于物理地址的不对应。同时适用的情况就在于分组，大数目的不同值，频繁更新的列中，这些情况即不适合聚集索引。 9. 乐观锁和悲观锁？数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。 乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 更多详情 10. 非关系型数据库和关系型数据库区别，优势比较？10.1 非关系型数据库的优势 性能 NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。 可扩展性 同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 10.2 关系型数据库的优势 复杂查询 可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持 使得对于安全性能很高的数据访问要求得以实现。 小结：对于这两类数据库，对方的优势就是自己的弱势，反之亦然。NOSQL数据库慢慢开始具备SQL数据库的一些复杂查询功能，比如MongoDB。对于事务的支持也可以用一些系统级的原子操作来实现例如乐观锁之类的方法来曲线救国，比如Redis set nx。 11. 数据库三范式，根据某个场景设计数据表？ 第一范式(确保每列保持原子性) 第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。 第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。 上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 第二范式(确保表中的每列都和主键相关) 第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键，如下表所示。 这样就产生一个问题：这个表中是以订单编号和商品编号作为联合主键。这样在该表中商品名称、单位、商品价格等信息不与该表的主键相关，而仅仅是与商品编号相关。所以在这里违反了第二范式的设计原则。 而如果把这个订单信息表进行拆分，把商品信息分离到另一个表中，把订单项目表也分离到另一个表中，就非常完美了。如下所示。 这样设计，在很大程度上减小了数据库的冗余。如果要获取订单的商品信息，使用商品编号到商品信息表中查询即可。 第三范式(确保每列都和主键列直接相关,而不是间接相关) 第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。 比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。如下面这两个表所示的设计就是一个满足第三范式的数据库表。 这样在查询订单信息的时候，就可以使用客户编号来引用客户信息表中的记录，也不必在订单信息表中多次输入客户信息的内容，减小了数据冗余。 12. 使用explain优化sql和索引？知乎详细回答 13.什么是内连接、外连接、交叉连接、笛卡尔积等？ 内连接(INNER JOIN) 等值连接 自然连接 不等连接 外连接(OUTER JOIN) 左外连接(LEFT OUTER JOIN或LEFT JOIN) 右外连接(RIGHT OUTER JOIN或RIGHT JOIN) 全外连接(FULL OUTER JOIN或FULL JOIN) 交叉连接(CROSS JOIN) 没有WHERE 子句，它返回连接表中所有数据行的笛卡尔积 14. mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决？14.1 MySQL有三种锁的级别表级锁： 开销小，加锁快；不会出现死锁； 锁定粒度大，发生锁冲突的概率最高,并发度最低。 行级锁： 开销大，加锁慢；会出现死锁； 锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 页面锁： 开销和加锁时间界于表锁和行锁之间；会出现死锁； 锁定粒度界于表锁和行锁之间，并发度一般 14.2 什么情况下会造成死锁所谓死锁: 是指两个或两个以上的进程在执行过程中。因争夺资源而造成的一种互相等待的现象,若无外力作用,它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁,这些永远在互相等竺的进程称为死锁进程。 表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。 死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。 14.3 死锁的解决办法 查出的线程杀死 kill 设置锁的超时时间 15. varchar和char的使用场景？ char的长度是不可变的，而varchar的长度是可变的。 定义一个char[10]和varchar[10]。如果存进去的是‘csdn’,那么char所占的长度依然为10，除了字符‘csdn’外，后面跟六个空格，varchar就立马把长度变为4了，取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的。 char的存取速度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找。char也为此付出的是空间的代价，因为其长度固定，所以难免会有多余的空格占位符占据空间，可谓是以空间换取时间效率。varchar是以空间效率为首位。 char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。 varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。 两者的存储数据都非unicode的字符数据。 16. mysql 高并发环境解决方案？ MySQL高并发环境解决方案：分库、分表、分布式、增加二级缓存。 需求分析： 互联网单位 每天大量数据读取，写入，并发性高。 现有解决方式： 水平分库分表，由单点分布到多点数据库中，从而降低单点数据库压力。 集群方案： 解决DB宕机带来的单点DB不能访问问题。 读写分离策略： 极大限度提高了应用中Read数据的速度和并发量。无法解决高写入压力。 17. 数据库崩溃时事务的恢复机制（REDO日志和UNDO日志）？更多详情]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive数据倾斜优化总结]]></title>
    <url>%2F2019%2F04%2F25%2FHive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1. 数据倾斜的原因1.1 操作 关键词 情形 后果 Join 其中一个表较小，但是key集中 分发到某一个或几个reduce上的数据远高于平均值 Join 大表与大表，但是分桶的判断字段0值或空值过多 这些空值都由一个reduce处理，非常慢 group by group by 维度过小，某值的数量过多 处理某值的reduce耗时 Count Distinct 某特殊值过多 处理此特殊值的reduce耗时 1.2 原因 key分布不均匀 业务数据本身的特性 建表时考虑不周 某些SQL语句本身就有数据倾斜 1.3 表现 任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大； 单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多； 最长时长远大于平均时长。 2. 数据倾斜的解决方案2.1 参数调节 set hive.map.aggr = true 在map中会做部分聚集操作，效率更高但需要更多的内存。 set hive.groupby.skewindata = true 数据倾斜的时候进行负载均衡，查询计划生成两个MR job，第一个job先进行key随机分配处理，随机分布到Reduce中，每个Reduce做部分聚合操作，先缩小数据量。第二个job再进行真正的group by key处理，根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Key被分布到同一个Reduce中），完成最终的聚合操作。 set hive.merge.mapfiles=true 当出现小文件过多，需要合并小文件 set hive.exec.reducers.bytes.per.reducer=1000000000 （单位是字节） 每个reduce能够处理的数据量大小，默认是1G。 hive.exec.reducers.max=999 最大可以开启的reduce个数，默认是999个。在只配了hive.exec.reducers.bytes.per.reducer以及hive.exec.reducers.max的情况下，实际的reduce个数会根据实际的数据总量/每个reduce处理的数据量来决定。 set mapred.reduce.tasks=-1 实际运行的reduce个数，默认是-1，可以认为指定，但是如果认为在此指定了，那么就不会通过实际的总数据量hive.exec.reducers.bytes.per.reducer来决定reduce的个数了。 2.2 SQL语句优化1.大小表Join 使用map join让小的维度表（1000条以下的记录条数） 先进内存，在map端完成reduce。如下：select /*+ mapjoin(a) */ a.c1, b.c1 ,b.c2from a join b where a.c1 = b.c1; 2.大表Join大表 把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。如下：select * from log a left outer join users b on case when a.user_id is null then concat('hive',rand()) else a.user_id end = b.user_id; 3.count distinct大量相同特殊值 count distinct时，将值为null的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。 执行如select a,count(distinct b) from t group by a; 类型的SQL时，会出现数据倾斜的问题 可替换成select a,sum(1) from (select a, b from t group by a,b) group by a; 4.group by维度过小 采用sum() group by的方式来替换count(distinct)完成计算。 5.不同数据类型关联产生数据倾斜 用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。 select * from users aleft outer join logs bon a.usr_id = cast(b.user_id as string) 6.小表不小不大，怎么用 map join 解决倾斜问题 使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子:select * from log aleft outer join users bon a.user_id = b.user_id; users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。解决方法：select /*+mapjoin(x)*/* from log a left outer join ( select /*+mapjoin(c)*/d.* from ( select distinct user_id from log ) c join users d on c.user_id = d.user_id ) x on a.user_id = b.user_id;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlload用法总结]]></title>
    <url>%2F2019%2F04%2F25%2Fsqlload%E7%94%A8%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[总体流程 创建bad、log、ctl、discard目录 在数据库中创建需要导入的表 编写控制文件，例如test.ctl ，放到ctl目录下（数据文件也在ctl目录下） 执行命令 sqlldr username/password@10.119.169.126:1521/dtlkdvapdb control=’ctl\test.ctl’ log=’log\test.log’ bad=’bad\test.log’ discard=’discard\test.log’ direct=true; 导入完成后需要检查log和bad日志，看是否存在问题导致数据并未入库 导入txt文件编写控制文件如下：OPTIONS(BINDSIZE=2097152,READSIZE=2097152,SKIP=1,ERRORS=-1,ROWS=50000)LOAD DATACHARACTERSET AL32UTF8INFILE 'C:\个人总结\sqlload导入数据\ctl\TFKBT.txt' "STR X'0a'"INSERT INTO TABLE TFKBTFIELDS TERMINATED BY X'01' TRAILING NULLCOLS (mandt,spras,fkber,fkbtx) 注意FIELDS TERMINATED BY X’01’中的01为每行数据之间的分隔符，不同的数据分隔符可能会不同 导入csv文件编写控制文件如下：options (skip = 1,rows = 5000)LOAD DATA CHARACTERSET AL32UTF8INFILE 'C:\个人总结\sqlload导入数据\ctl\数据.csv' appendINTO TABLE ODS_DEAL_DETAILfields terminated by ','Optionally enclosed by '"'(com_id_a,com_name_a,com_id_b,com_name_b,sbj_code,sbj_name)]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据架构随记]]></title>
    <url>%2F2019%2F04%2F22%2F%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E9%9A%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[更多详情请点击]]></content>
      <categories>
        <category>数据</category>
      </categories>
      <tags>
        <tag>数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop常用命令]]></title>
    <url>%2F2019%2F04%2F22%2Fhadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[hadoop fs -ls / 显示hdfs目录结构hadoop fs -du / 显示该目录中每个文件或目录的大小hadoop fs -du -s / 显示该目录总大小hadoop fs -mkdir /home 在hdfs指定目录内创建新目录hadoop fs -touchz /wahaha 创建一个空文件hadoop fs -rm /wahaha 删除一个文件hadoop fs -rmr /home 删除一个目录hadoop dfs -mv /hello /hello2 重命名hadoop dfs -cp /testdir/slaves /test 复制文件到指定目录hadoop dfs -cat /hello 查看文件hadoop fs -put hello / 上传文件到hdfshadoop fs -put hellodir/ / 上传目录到hdfshadoop dfs -get /hello localdir 将某个文件down至本地已有目录下hadoop dfs -getmerge /hellodir localdir 将指定目录下的所有内容merge成一个文件，下载到本地hadoop dfs -copyFromLocal hello / 类似put，但只能是从本地复制到hdfshadoop dfs -copyToLocal /home localdir 类似get，但只能把文件从hdfs下载到本地 hadoop job -list 列出将完成的jobhadoop job -kill jobid 关闭一个job]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle删除重复数据]]></title>
    <url>%2F2019%2F01%2F22%2FOracle%E5%88%A0%E9%99%A4%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1.查找表中多余的重复记录，重复记录是根据单个字段（id）来判断select * from 表 where id in (select id from 表 group by id having count(id) &gt; 1) 2.查找表中多余的重复记录（多个字段）select * from 表 a where (a.id,a.seq) in (select id,seq from 表 group by id,seq having count(*) &gt; 1) 3.删除表中多余的重复记录，重复记录是根据单个字段（id）来判断，只留有rowid最小的记录delete from 表 where id in (select id from 表 group by id having count(id) &gt; 1) and rowid not in (select min(rowid) from 表 group by id having count(*)&gt;1) 4.删除表中多余的重复记录（多个字段），只留有rowid最小的记录delete from 表 a where (a.id,a.seq) in (select id,seq from 表 group by id,seq having count(*) &gt; 1) and rowid not in (select min(rowid) from 表 group by id,seq having count(*)&gt;1) 5.查找表中多余的重复记录（多个字段），不包含rowid最小的记录select * from 表 a where (a.id,a.seq) in (select id,seq from 表 group by id,seq having count(*) &gt; 1) and rowid not in (select min(rowid) from 表 group by id,seq having count(*)&gt;1)]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle行列转换]]></title>
    <url>%2F2018%2F12%2F08%2FOracle%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[pivot 列转行 select * from (select name, nums from demo) pivot(sum(nums) for name in ('苹果' 苹果, '橘子', '葡萄', '芒果')); 注意： pivot（聚合函数 for 列名 in（类型）） ，其中 in(‘’) 中可以指定别名，in中还可以指定子查询，比如 select distinct code from customers unpivot 行转列 select id , name, jidu, xiaoshou from Fruit unpivot (xiaoshou for jidu in (q1, q2, q3, q4)); 注意： unpivot没有聚合函数，xiaoshou、jidu字段也是临时的变量 更多详细内容请点这里]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle树操作、递归查询]]></title>
    <url>%2F2018%2F12%2F04%2FOracle%E6%A0%91%E6%93%8D%E4%BD%9C%E3%80%81%E9%80%92%E5%BD%92%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[select … from tablenamestart with 条件1connect by 条件2where 条件3; 例：select * from tablestart with org_id = ‘HBHqfWGWPy’connect by prior org_id = parent_id; 简单说来是将一个树状结构存储在一张表里，比如一个表中存在两个字段:org_id，parent_id那么通过表示每一条记录的parent是谁，就可以形成一个树状结构。 用上述语法的查询可以取得这棵树的所有记录。 其中： 条件1是根结点的限定语句，当然可以放宽限定条件，以取得多个根结点，实际就是多棵树 条件2是连接条件，其中用prior表示上一条记录，比如connect by prior org_id = parent_id就是说上一条记录的org_id是本条记录的parent_id即本记录的父亲是上一条记录 条件3是过滤条件，用于对返回的所有记录进行过滤 在扫描树结构表时，需要依此访问树结构的每个节点，一个节点只能访问一次，其访问的步骤如下： 第一步：从根节点开始 第二步：访问该节点 第三步：判断该节点有无未被访问的子节点，若有，则转向它最左侧的未被访问的子节，并执行第二步，否则执行第四步 第四步：若该节点为根节点，则访问完毕，否则执行第五步 第五步：返回到该节点的父节点，并执行第三步骤 总之：扫描整个树结构的过程也即是中序遍历树的过程]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用操作]]></title>
    <url>%2F2018%2F08%2F23%2FLinux%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[文件的操作cpcp -a file1 file2 # 连同file1的所有特性把文件复制为file2cp file1 file2 dir1 # 将文件file1、file2复制到目录dir下 mv用于移动文件、目录、更名mv -f # 强制覆盖mv -i # 若目标文件存在，就询问是否进行覆盖操作 移动文件（多个） mv file1.txt file2.txt /home/test 移动目录 mv dir1/ /home/test 重命名文件或目录 mv file1.txt file2.txt # 文件mv dir1/ dir2/ # 目录 rm用于删除文件和目录rm -f # 强制删除rm -r # 递归删除，常用于目录删除（有提示）rm -i # 询问是否进行删除操作rm -rf dir # 强制删除目录dir中的所有文件和目录（不做提示） mkdir、touchmkdir dir1 # 在当前目录下创建目录touch file1 # 在当前目录下创建文件 tar用于对文件进行打包，默认不进行压缩。以下五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。-c: 建立压缩档案-x：解压-t：查看内容-r：向压缩归档文件末尾追加文件-u：更新原压缩包中的文件 下面的参数是根据需要在压缩或解压档案时可选的。-z：有gzip属性的-j：有bz2属性的-Z：有compress属性的-v：显示所有过程-O：将文件解开到标准输出 -f（必选参数）: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。tar -cf all.tar *.jpg # 将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。tar -rf all.tar *.gif # 将所有.gif的文件增加到all.tar的包里面去。-r是表示增加文件的意思。tar -uf all.tar logo.gif # 更新原来tar包all.tar中logo.gif文件，-u是表示更新文件的意思。tar -tf all.tar # 列出all.tar包中所有文件，-t是列出文件的意思tar -xf all.tar # 解出all.tar包中所有文件，-x是解开的意思 常用命令tar xvf FileName.tar # 解包tar cvf FileName.tar DirName # 打包 zip和unzip对文件打包、压缩或解压zip -r xxx.zip ./ # 压缩当前目录内文件为xxx.zip文件unzip filename.zip # 解压zip文件到当前目录 文件的编辑vi和vim插入模式：在此模式下可以输入字符，按ESC将回到命令模式。命令模式：可以移动光标、删除字符等。低行模式：可以保存文件、退出vi、设置vi、查找等功能(低行模式也可以看作是命令模式里的)。 打开文件、保存、关闭文件(vi命令模式下使用)vi filename # 打开filename文件:w # 保存文件 :w vpser.net # 保存至vpser.net文件 :q # 退出编辑器，如果文件已修改请使用下面的命令 :q! # 退出编辑器，且不保存 :wq # 退出编辑器，且保存文件 插入文本或行(vi命令模式下使用，执行下面命令后将进入插入模式，按ESC键可退出插入模式)a # 在当前光标位置的右边添加文本 i # 在当前光标位置的左边添加文本 A # 在当前行的末尾位置添加文本 I # 在当前行的开始处添加文本(非空字符的行首) O # 在当前行的上面新建一行 o # 在当前行的下面新建一行 R # 替换(覆盖)当前光标位置及后面的若干文本 J # 合并光标所在行及下一行为一行(依然在命令模式) 移动光标(vi命令模式下使用)上下左右方向键 h # 向左j # 向下k # 向上l # 向右空格键 # 向右Backspace # 向左Enter # 移动到下一行首- # 移动到上一行首 删除、恢复字符或行(vi命令模式下使用)x # 删除当前字符 nx # 删除从光标开始的n个字符 dd # 删除当前行 ndd # 向下删除当前行在内的n行 u # 撤销上一步操作 U # 撤销对当前行的所有操作 搜索(vi命令模式下使用)/vpser # 向光标下搜索vpser字符串 ?vpser # 向光标上搜索vpser字符串 n # 向下搜索前一个搜索动作 N # 向上搜索前一个搜索动作 跳至指定行的行首(vi命令模式下使用)n+ # 向下跳n行 n- # 向上跳n行 nG # 跳到行号为n的行 G # 跳至最后一行 设置行号(vi命令模式下使用):set nu # 显示行号 :set nonu # 取消显示行号 复制、粘贴(vi命令模式下使用)yy # 将当前行复制到缓存区，也可以用 &quot;ayy&quot; 复制，&quot;a&quot; 为缓冲区，a也可以替换为a到z的任意字母，可以完成多个复制任务。 nyy # 将当前行向下n行复制到缓冲区，也可以用 &quot;anyy&quot; 复制，&quot;a&quot; 为缓冲区，a也可以替换为a到z的任意字母，可以完成多个复制任务。 yw # 复制从光标开始到词尾的字符。 nyw # 复制从光标开始的n个单词。 y^ # 复制从光标到行首的内容。y$ # 复制从光标到行尾的内容。 p # 粘贴剪切板里的内容在光标后，如果使用了前面的自定义缓冲区，建议使用&quot;ap&quot; 进行粘贴。 P # 粘贴剪切板里的内容在光标前，如果使用了前面的自定义缓冲区，建议使用&quot;aP&quot; 进行粘贴。 替换(vi命令模式下使用):s/old/new # 用new替换行中首次出现的old :s/old/new/g # 用new替换行中所有的old :n,m s/old/new/g # 用new替换从n到m行里所有的old :%s/old/new/g # 用new替换当前文件里所有的old]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建博客常用操作]]></title>
    <url>%2F2018%2F08%2F22%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[写一篇文章输入创建文章命令，生成一个md文件(/blog/source/_posts/) hexo n &quot;hello&quot; == hexo new &quot;hello&quot; # 新建文章 用编辑器打开hello.md文件，编写完后保存 本地查看效果执行下面语句，执行完再登录localhost:4000查看效果（执行完不要按Ctrl+C，不然就停止了） hexo g == hexo generate # 生成网页hexo s == hexo server # 启动本地服务预览 部署hexo d == hexo deploy # 部署到github 部署后我们可以浏览器搜 username.github.io 查看自己的博客效果，比如我的monkeyip.github.io]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
