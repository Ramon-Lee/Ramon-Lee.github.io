<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用Debezium同步PostgreSQL数据至Kafka]]></title>
    <url>%2F2020%2F08%2F07%2F%E4%BD%BF%E7%94%A8Debezium%E5%90%8C%E6%AD%A5PostgreSQL%E6%95%B0%E6%8D%AE%E8%87%B3Kafka%2F</url>
    <content type="text"><![CDATA[Debezium连接器与Kafka Connect框架一起使用，以捕获数据库中的更改并生成更改事件。然后，Kafka Connect工作程序将为连接器配置的转换应用于连接器生成的每个消息，使用工作程序的转换器将每个消息键和值序列化为二进制形式，最后将每个消息写入正确的Kafka主题。 Debezium的PostgreSQL连接器包含两个不同的部分，它们可以一起工作，以便能够读取和处理服务器更改，必须在PostgreSQL服务器中安装和配置的逻辑解码输出插件，其中之一： decoderbufs（由Debezium社区维护，基于ProtoBuf） wal2json（由wal2json社区维护，基于JSON） pgoutput，PostgreSQL 10+中的标准逻辑解码插件（由Postgres社区维护，由Postgres自身用于逻辑复制）；该插件始终存在，这意味着不必安装任何其他库，并且Debezium连接器将直接将原始复制事件流解释为更改事件。 由于我虚拟机安装的是PostgreSQL 9.6.8版本，所以并不支持pgoutput插件，所以需要额外安装。Debezium官网有安装wal2json的教程，为了方便起见，这里安装wal2json插件。 官方文档： Debezium connector for PostgreSQL Kafka Connect wal2json Installationsudo yum install wal2json12cd /opt/modulegit clone https://github.com/eulerto/wal2json -b master --single-branchcd wal2json makemake install 编译成功的信息显示： Cloning into 'wal2json'...remote: Counting objects: 445, done.remote: Total 445 (delta 0), reused 0 (delta 0), pack-reused 445Receiving objects: 100% (445/445), 180.70 KiB | 0 bytes/s, done.Resolving deltas: 100% (317/317), done.Note: checking out 'd2b7fef021c46e0d429f2c1768de361069e58696'.You are in 'detached HEAD' state. You can look around, make experimentalchanges and commit them, and you can discard any commits you make in thisstate without impacting any branches by performing another checkout.If you want to create a new branch to retain commits you create, you maydo so (now or later) by using -b with the checkout command again. Example: git checkout -b new_branch_nameHEAD is now at d2b7fef... Improve stylegcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC -I. -I./ -I/usr/pgsql-9.6/include/server -I/usr/pgsql-9.6/include/internal -D_GNU_SOURCE -I/usr/include/libxml2 -I/usr/include -c -o wal2json.o wal2json.cgcc -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fexcess-precision=standard -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC -L/usr/pgsql-9.6/lib -Wl,--as-needed -L/usr/lib64 -Wl,--as-needed -Wl,-rpath,'/usr/pgsql-9.6/lib',--enable-new-dtags -shared -o wal2json.so wal2json.o/usr/bin/mkdir -p '/usr/pgsql-9.6/lib'/usr/bin/install -c -m 755 wal2json.so '/usr/pgsql-9.6/lib/' 如果在make时，检测到没有权限，则使用root账户执行命令chmod 777 -R /dic PostgreSQL Server ConfigurationSetting up libraries, WAL and replication parameters编辑$PGDATA目录中postgresql.conf vi $PGDATA/postgresql.conf#编辑内容如下：listen_addresses = '*'port = 5432 wal_level = logical max_wal_senders = 8wal_keep_segments = 4max_replication_slots = 4shared_preload_libraries = 'wal2json' Setting up replication permissions编辑$PGDATA目录中pg_hba.conf文件 vi $PGDATA/pg_hba.conf#编辑内容如下：# IPv4 local connections:host all all 0.0.0.0/0 md5# replication privilege.local replication postgres trusthost replication postgres 127.0.0.1/32 trusthost replication postgres ::1/128 trusthost replication postgres 192.168.142.102/32 trust 编辑完以上两文件，重启数据库服务： pg_ctl restart Database Test Environment Set-up--切换到postgres用户，进入到postgresql交互式命令行[postgres@hadoop102 monkey]$ psql--创建测试库和测试表postgres=# CREATE DATABASE test;postgres=# \c test;postgres=# CREATE TABLE test_table ( id char(10) NOT NULL, code char(10), PRIMARY KEY (id)); Decoding Output Plug-in Test 使用wal2json，为数据库test创建一个名叫test_slot的slot pg_recvlogical -d test --slot test_slot --create-slot -P wal2json 开始使用 test_slot 对数据库 test进行数据streaming变化的监测 pg_recvlogical -d test --slot test_slot --start -o pretty-print=1 -f - 对表test_table 做INSERT/UPDATE/DELETE 操作 test=# INSERT INTO test_table (id, code) VALUES('id1', 'code1');INSERT 0 1test=# update test_table set code='code2' where id='id1';UPDATE 1test=# delete from test_table where id='id1';DELETE 1 在监测窗口会接收到如下信息： Output for INSERT event &#123; "change": [ &#123; "kind": "insert", "schema": "public", "table": "test_table", "columnnames": ["id", "code"], "columntypes": ["character(10)", "character(10)"], "columnvalues": ["id1 ", "code1 "] &#125; ]&#125; Output for UPDATE event &#123; "change": [ &#123; "kind": "update", "schema": "public", "table": "test_table", "columnnames": ["id", "code"], "columntypes": ["character(10)", "character(10)"], "columnvalues": ["id1 ", "code2 "], "oldkeys": &#123; "keynames": ["id"], "keytypes": ["character(10)"], "keyvalues": ["id1 "] &#125; &#125; ]&#125; Output for DELETE event &#123; "change": [ &#123; "kind": "delete", "schema": "public", "table": "test_table", "oldkeys": &#123; "keynames": ["id"], "keytypes": ["character(10)"], "keyvalues": ["id1 "] &#125; &#125; ]&#125; 当测试完成，对数据库test进行监测的test_slot也可以被移除： pg_recvlogical -d test --slot test_slot --drop-slot 至此，wal2json插件算是安装成功并测试通过了。 Debezium PostgreSQL Connector相关配置Debezium PostgreSQL Connector 安装 connector’s plug-in archive 注意版本问题：目前的稳定版为1.2.0，我下载的是1.0.3 将下载好的debezium-connector-postgres-1.0.3.Final-plugin.tar.gz文件解压到kafka对应的connect目录下 [monkey@hadoop102 kafka]$ cd /opt/module/kafka[monkey@hadoop102 kafka]$ mkdir connect[monkey@hadoop102 kafka]$ cd /opt/software[monkey@hadoop102 software]$ tar -zxvf debezium-connector-postgres-1.0.3.Final-plugin.tar.gz -C /opt/module/kafka/connect Kafka Connect目前支持两种模式： standalone（单进程）和distributed。 由于我是单机，所以用standalone模式来测试。 编辑worker配置文件： [monkey@hadoop102 kafka]$ cd /opt/module/kafka/config[monkey@hadoop102 kafka]$ vi connect-standalone.properties# 编辑最后一行plugin.path，路径为Debezium connector的jar file所在目录，不用配置到最底层目录（注意可配置多个路径，单个路径也要以,结尾）plugin.path=/opt/module/kafka/connect, Connector 配置样例配置样例有两种形式：本地编辑properties文件和使用POST方式提交到Kafka Connect 服务 1、properties文件格式 name=student-connectorconnector.class=io.debezium.connector.postgresql.PostgresConnectortasks.max=1database.hostname=192.168.142.102database.port=5432database.user=postgresdatabase.password=postgresdatabase.dbname=testdatabase.server.name=infotable.whitelist=public.studentplugin.name=wal2jsonslot.name=my_slot 2、使用POST方式，提交JSON格式文件 &#123; "name": "student-connector", "config": &#123; "name": "student-connector", "connector.class": "io.debezium.connector.postgresql.PostgresConnector", "tasks.max": "1", "database.hostname": "192.168.142.102", "database.port": "5432", "database.user": "postgres", "database.password": "postgres", "database.dbname" : "test", "database.server.name": "info", "table.whitelist": "public.student", "plugin.name": "wal2json", "slot.name":"my_slot" &#125;&#125; 以上的slot.name为从插件和数据库实例进行流式更改而创建的Postgres逻辑解码插槽的名称。值必须符合Postgres复制插槽的命名规则，该规则指出：“每个复制插槽都有一个名称，该名称可以包含小写字母，数字和下划线字符”。不指定默认为debezium，如果需要添加多个connector，不指定的话，会报错：Caused by: org.postgresql.util.PSQLException: ERROR: replication slot &quot;debezium&quot; is active for PID 52197 standalone模式测试1、开启connector [monkey@hadoop102 kafka]$ bin/connect-standalone.sh config/connect-standalone.properties postgres-student.properties 2、操作PostgreSQL数据库test下的student表 test=# INSERT INTO student (id, name) VALUES('5', 'sam');INSERT 0 1test=# update student set name = 'ethan' where id = '5';UPDATE 1test=# delete from student where id='5';DELETE 1 3、启用kafka消费者，查看对应topic的消息接收情况 [monkey@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic info.public.student Output for INSERT event &#123; "schema":&#123; "type":"struct", "fields":[ &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"id" &#125;, &#123; "type":"string", "optional":true, "field":"name" &#125; ], "optional":true, "name":"info.public.student.Value", "field":"before" &#125;, &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"id" &#125;, &#123; "type":"string", "optional":true, "field":"name" &#125; ], "optional":true, "name":"info.public.student.Value", "field":"after" &#125;, &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"version" &#125;, &#123; "type":"string", "optional":false, "field":"connector" &#125;, &#123; "type":"string", "optional":false, "field":"name" &#125;, &#123; "type":"int64", "optional":false, "field":"ts_ms" &#125;, &#123; "type":"string", "optional":true, "name":"io.debezium.data.Enum", "version":1, "parameters":&#123; "allowed":"true,last,false" &#125;, "default":"false", "field":"snapshot" &#125;, &#123; "type":"string", "optional":false, "field":"db" &#125;, &#123; "type":"string", "optional":false, "field":"schema" &#125;, &#123; "type":"string", "optional":false, "field":"table" &#125;, &#123; "type":"int64", "optional":true, "field":"txId" &#125;, &#123; "type":"int64", "optional":true, "field":"lsn" &#125;, &#123; "type":"int64", "optional":true, "field":"xmin" &#125; ], "optional":false, "name":"io.debezium.connector.postgresql.Source", "field":"source" &#125;, &#123; "type":"string", "optional":false, "field":"op" &#125;, &#123; "type":"int64", "optional":true, "field":"ts_ms" &#125; ], "optional":false, "name":"info.public.student.Envelope" &#125;, "payload":&#123; "before":null, "after":&#123; "id":"5 ", "name":"sam " &#125;, "source":&#123; "version":"1.0.3.Final", "connector":"postgresql", "name":"info", "ts_ms":1583920562395, "snapshot":"false", "db":"test", "schema":"public", "table":"student", "txId":1760, "lsn":23397480, "xmin":null &#125;, "op":"c", "ts_ms":1583920562442 &#125;&#125; Output for UPDATE event &#123; "schema":&#123; "type":"struct", "fields":[ &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"id" &#125;, &#123; "type":"string", "optional":true, "field":"name" &#125; ], "optional":true, "name":"info.public.student.Value", "field":"before" &#125;, &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"id" &#125;, &#123; "type":"string", "optional":true, "field":"name" &#125; ], "optional":true, "name":"info.public.student.Value", "field":"after" &#125;, &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"version" &#125;, &#123; "type":"string", "optional":false, "field":"connector" &#125;, &#123; "type":"string", "optional":false, "field":"name" &#125;, &#123; "type":"int64", "optional":false, "field":"ts_ms" &#125;, &#123; "type":"string", "optional":true, "name":"io.debezium.data.Enum", "version":1, "parameters":&#123; "allowed":"true,last,false" &#125;, "default":"false", "field":"snapshot" &#125;, &#123; "type":"string", "optional":false, "field":"db" &#125;, &#123; "type":"string", "optional":false, "field":"schema" &#125;, &#123; "type":"string", "optional":false, "field":"table" &#125;, &#123; "type":"int64", "optional":true, "field":"txId" &#125;, &#123; "type":"int64", "optional":true, "field":"lsn" &#125;, &#123; "type":"int64", "optional":true, "field":"xmin" &#125; ], "optional":false, "name":"io.debezium.connector.postgresql.Source", "field":"source" &#125;, &#123; "type":"string", "optional":false, "field":"op" &#125;, &#123; "type":"int64", "optional":true, "field":"ts_ms" &#125; ], "optional":false, "name":"info.public.student.Envelope" &#125;, "payload":&#123; "before":&#123; "id":"5 ", "name":null &#125;, "after":&#123; "id":"5 ", "name":"ethan " &#125;, "source":&#123; "version":"1.0.3.Final", "connector":"postgresql", "name":"info", "ts_ms":1583920898322, "snapshot":"false", "db":"test", "schema":"public", "table":"student", "txId":1761, "lsn":23398864, "xmin":null &#125;, "op":"u", "ts_ms":1583920898326 &#125;&#125; Output for DEETE event &#123; "schema":&#123; "type":"struct", "fields":[ &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"id" &#125;, &#123; "type":"string", "optional":true, "field":"name" &#125; ], "optional":true, "name":"info.public.student.Value", "field":"before" &#125;, &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"id" &#125;, &#123; "type":"string", "optional":true, "field":"name" &#125; ], "optional":true, "name":"info.public.student.Value", "field":"after" &#125;, &#123; "type":"struct", "fields":[ &#123; "type":"string", "optional":false, "field":"version" &#125;, &#123; "type":"string", "optional":false, "field":"connector" &#125;, &#123; "type":"string", "optional":false, "field":"name" &#125;, &#123; "type":"int64", "optional":false, "field":"ts_ms" &#125;, &#123; "type":"string", "optional":true, "name":"io.debezium.data.Enum", "version":1, "parameters":&#123; "allowed":"true,last,false" &#125;, "default":"false", "field":"snapshot" &#125;, &#123; "type":"string", "optional":false, "field":"db" &#125;, &#123; "type":"string", "optional":false, "field":"schema" &#125;, &#123; "type":"string", "optional":false, "field":"table" &#125;, &#123; "type":"int64", "optional":true, "field":"txId" &#125;, &#123; "type":"int64", "optional":true, "field":"lsn" &#125;, &#123; "type":"int64", "optional":true, "field":"xmin" &#125; ], "optional":false, "name":"io.debezium.connector.postgresql.Source", "field":"source" &#125;, &#123; "type":"string", "optional":false, "field":"op" &#125;, &#123; "type":"int64", "optional":true, "field":"ts_ms" &#125; ], "optional":false, "name":"info.public.student.Envelope" &#125;, "payload":&#123; "before":&#123; "id":"5 ", "name":null &#125;, "after":null, "source":&#123; "version":"1.0.3.Final", "connector":"postgresql", "name":"info", "ts_ms":1583921079909, "snapshot":"false", "db":"test", "schema":"public", "table":"student", "txId":1762, "lsn":23399936, "xmin":null &#125;, "op":"d", "ts_ms":1583921079912 &#125;&#125; 主要有用的信息在： &#123; "payload":&#123; "before":&#123; "id":"5 ", "name":null &#125;, "after":&#123; "id":"5 ", "name":"ethan " &#125;, "source":&#123; "version":"1.0.3.Final", "connector":"postgresql", "name":"info", "ts_ms":1583920898322, "snapshot":"false", "db":"test", "schema":"public", "table":"student", "txId":1761, "lsn":23398864, "xmin":null &#125;, "op":"u", "ts_ms":1583920898326 &#125;&#125; distributed模式测试[monkey@hadoop102 kafka]$ bin/connect-distributed.sh config/connect-distributed.properties 得到的结果和standalone模式是一致的。]]></content>
      <categories>
        <category>数据同步</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>Debezium</tag>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume消费Kafka，启动后无法正常关闭Flume进程的问题]]></title>
    <url>%2F2020%2F07%2F31%2FFlume%E6%B6%88%E8%B4%B9Kafka%EF%BC%8C%E5%90%AF%E5%8A%A8%E5%90%8E%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E5%85%B3%E9%97%ADFlume%E8%BF%9B%E7%A8%8B%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在学习Flume时，有遇到Flume来消费Kafka数据的场景，所以自己学习着做了一些配置。Flume的配置文件如下： ## 组件a1.sources=r1 r2a1.channels=c1 c2a1.sinks=k1 k2## source1a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.r1.batchSize = 5000a1.sources.r1.batchDurationMillis = 2000a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.sources.r1.kafka.topics=topic_start## source2a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.r2.batchSize = 5000a1.sources.r2.batchDurationMillis = 2000a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092a1.sources.r2.kafka.topics=topic_event## channel1a1.channels.c1.type = filea1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/a1.channels.c1.maxFileSize = 2146435071a1.channels.c1.capacity = 1000000a1.channels.c1.keep-alive = 6## channel2a1.channels.c2.type = filea1.channels.c2.checkpointDir = /opt/module/flume/checkpoint/behavior2a1.channels.c2.dataDirs = /opt/module/flume/data/behavior2/a1.channels.c2.maxFileSize = 2146435071a1.channels.c2.capacity = 1000000 a1.channels.c2.keep-alive = 6## sink1a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_start/%Y-%m-%da1.sinks.k1.hdfs.filePrefix = logstart-##sink2a1.sinks.k2.type = hdfsa1.sinks.k2.hdfs.path = /origin_data/gmall/log/topic_event/%Y-%m-%da1.sinks.k2.hdfs.filePrefix = logevent-## 不要产生大量小文件a1.sinks.k1.hdfs.rollInterval = 3600a1.sinks.k1.hdfs.rollSize = 134217728a1.sinks.k1.hdfs.rollCount = 0a1.sinks.k2.hdfs.rollInterval = 10a1.sinks.k2.hdfs.rollSize = 134217728a1.sinks.k2.hdfs.rollCount = 0## 控制输出文件是原生文件a1.sinks.k1.hdfs.fileType = CompressedStreama1.sinks.k2.hdfs.fileType = CompressedStreama1.sinks.k1.hdfs.codeC = snappya1.sinks.k2.hdfs.codeC = snappy## 拼装a1.sources.r1.channels = c1a1.sinks.k1.channel= c1a1.sources.r2.channels = c2a1.sinks.k2.channel= c2 其实在以上的a1.sinks.k1.hdfs.codeC = snappy这一配置处，之前配置的是lzop，但是在查看Flume启动日志时发现，我目前安装的hadoop还不支持lzop这种压缩方式，所以sink貌似是没创建成功，被移除了，所以后面又改成了snappy。 20/07/31 18:30:45 ERROR node.AbstractConfigurationProvider: Sink k1 has been removed due to an error during configurationjava.lang.IllegalArgumentException: Unsupported compression codec lzop. Please choose from: [None, BZip2Codec, DefaultCodec, DeflateCodec, GzipCodec, Lz4Codec, SnappyCodec] 在Flume启动后，jps确实有了Application的进程。 查看Flume启动日志，也没有什么报错信息。但是诡异的事情发生了，Flume进程无法正常关闭，也就是使用kill 进程号，关闭不了，而使用kill -9 进程号却可以。 这二者的区别我也知道，kill -9是直接把指令发送给系统，让系统对进程强制关闭，但是这种方式可能会导致一些文件和数据的丢失，能不能尽量不用。kill是把指令发送给进程本身，让进程主动关闭。 我这里使用kill关闭不了Flume进程，能想到的就是进程接受不到指令，所以无法做出相应的关闭操作。 至于说Flume进程究竟发生了什么问题，还不得而知，有待今后的学习过程中寻找解决方案吧。]]></content>
      <categories>
        <category>数据采集</category>
      </categories>
      <tags>
        <tag>Flume</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见排序算法]]></title>
    <url>%2F2020%2F04%2F23%2F%E5%B8%B8%E8%A7%81%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[冒泡排序工作原理 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 复杂度 最坏时间复杂度 O(N^2) 最优时间复杂度 O(N) 平均时间复杂度 O(N^2) 额外空间复杂度 O(1) 稳定性 稳定 实现# 冒泡排序def bubble_sort(alist): n = len(alist) exchange = False # 首先得到每个循环需要比较的次数，第一次从0位置开始需要比较 len(alist) - 1 次 for i in range(n-1, 0, -1): # 相邻两个位置不断比较，如果左边的数大于右边就交换位置 for j in range(0, i): if alist[j] &gt; alist[j+1]: alist[j], alist[j+1] = alist[j+1], alist[j] exchange = True # 如果发现整个排序过程中没有交换，提前结束 if not exchange: break return alist 选择排序工作原理首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。步骤如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 复杂度 最坏时间复杂度 O(N^2) 最优时间复杂度 O(N^2) 平均时间复杂度 O(N^2) 额外空间复杂度 O(1) 稳定性 稳定 实现# 选择排序def selection_sort(alist): n = len(alist) for i in range(n-1): # 寻找[i,n]区间里的最小值 min_index = i for j in range(i+1, n): if alist[j] &lt; alist[min_index]: min_index = j alist[i], alist[min_index] = alist[min_index], alist[i] return alist 插入排序工作原理通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 复杂度 最坏时间复杂度 O(N^2) 最优时间复杂度 O(N) 平均时间复杂度 O(N^2) 额外空间复杂度 O(1) 稳定性 稳定 实现# 插入排序def insertion_sort(alist): # 从索引为 1 的值开始从后向前扫描 for i in range(1, len(alist)): current_value = alist[i] position = i # 如果前一个数大于当前值则将前一个数向右移动一位，直到找到前一个数小于当前值得位置，将该位置的值设为当前值 while position &gt; 0 and alist[position - 1] &gt; current_value: alist[position] = alist[position - 1] position -= 1 alist[position] = current_value return alist 快速排序工作原理快速排序使用分治法（Divide and conquer）策略来把一个序列（list）分为两个子序列（sub-lists）。步骤如下： 从数列中挑出一个元素，称为“基准”（pivot）； 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任何一边）。在这个分割结束之后，该基准就处于数列的中间位置。这个称为分割（partition）操作； 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归到最底部时，数列的大小是零或一，也就是已经排序好了。这个算法一定会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 复杂度 最坏时间复杂度 O(N^2) 最优时间复杂度 O(NlogN) 平均时间复杂度 O(NlogN) 额外空间复杂度 O(logN) 稳定性 不稳定 实现# 快速排序def __quickSort(alist, l, r): #当数列的大小比较小的时候，数列近乎有序的概率较大 # if (r - l &lt;= 15): # insertionSortHelp(alist, l, r) # return if l &gt;= r: return p = partition(alist, l, r) __quickSort(alist, l, p-1) __quickSort(alist, p+1, r)# 在alist[l...r]中寻找j,使得alist[l...j] &lt;= alist[l], alist[j+1...r] &gt;alist[l]def partition(alist, l, r): pos = randint(l, r) alist[pos], alist[l] = alist[l], alist[pos] v = alist[l] # v = alist[l] j = l i = l + 1 while i &lt;= r: if alist[i] &lt;= v: alist[j+1],alist[i] = alist[i],alist[j+1] j += 1 i += 1 alist[l], alist[j] = alist[j], alist[l] return j]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器输入URL后发生了什么？]]></title>
    <url>%2F2019%2F12%2F11%2F%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BE%93%E5%85%A5URL%E5%90%8E%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[注意：本文的步骤是建立在，请求的是一个简单的 HTTP 请求，没有 HTTPS、HTTP2、最简单的 DNS、没有代理、并且服务器没有任何问题的基础上，尽管这是不切实际的。 大致流程 URL 解析 DNS 查询 TCP 连接 处理请求 接受响应 渲染页面 一、URL解析URL（Universal Resource Locator）：统一资源定位符。俗称网页地址或者网址。 URL用来表示某个资源的地址。（通过俗称就能看出来） URL主要由以下几个部分组成： a.传输协议 b.服务器 c.域名 d.端口 e.虚拟目录 f.文件名 g.锚 h.参数 也就是说，通常一个URL是像下面这样 连起来就是：http://www.aspxfans.com:8080/news/index.asp?boardID=5&amp;ID=24618&amp;page=1#name 上面的链接有几个要注意的地方：“:” 和“/”的使用，80端口默认不显示，“?” 到“#”之间跟着参数，多个参数使用“&amp;”连接，“#”后面跟着锚。 二、DNS解析DNS解析（域名解析），DNS实际上是一个域名和IP对应的数据库。 IP地址往都难以记住，但机器间互相只认IP地址，于是人们发明了域名，让域名与IP地址之间一一对应，它们之间的转换工作称为域名解析，域名解析需要由专门的域名解析服务器来完成，整个过程是自动进行的。 可以在浏览器中输入IP地址浏览网站，也可以输入域名查询网站，虽然得出的内容是一样的但是调用的过程不一样，输入IP地址是直接从主机上调用内容，输入域名是通过域名解析服务器指向对应的主机的IP地址，再从主机调用网站的内容。 DNS解析基本步骤： 1.浏览器缓存 浏览器会缓存之前拿到的DNS 2-30分钟时间，浏览器会先检查是否在缓存中，没有则调用系统库函数进行查询。 2.操作系统缓存 检查hosts文件，这个文件保存了一些以前访问过的网站的域名和IP的数据。它就像是一个本地的数据库。如果找到就可以直接获取目标主机的IP地址了。 3.路由器缓存 路由器有自己的DNS缓存，可能就包括了正在查询的内容。 4.ISP DNS 缓存 ISP DNS 就是在客户端电脑上设置的首选 DNS 服务器，它们在大多数情况下都会有缓存。 根域名服务器查询 在前面所有步骤没有缓存的情况下，本地 DNS 服务器会将请求转发到互联网上的根域，下面这个图很好的诠释了整个流程： 递归查询：从根域名服务器到顶级域名服务器再到极限域名服务器依次搜索哦对应目标域名的IP。 三、TCP 连接TCP/IP 分为四层，在发送数据时，每层都要对数据进行封装： 1. 应用层：发送 HTTP 请求在前面的步骤我们已经得到服务器的 IP 地址，浏览器会开始构造一个 HTTP 报文，其中包括： 请求报头（Request Header）：请求方法、目标地址、遵循的协议等等 请求主体（其他参数） 其中需要注意的点：浏览器只能发送 GET、POST 方法，而打开网页使用的是 GET 方法 2. 传输层：TCP 传输报文传输层会发起一条到达服务器的 TCP 连接，为了方便传输，会对数据进行分割（以报文段为单位），并标记编号，方便服务器接受时能够准确地还原报文信息。 在建立连接前，会先进行 TCP 三次握手。 3. 网络层：IP协议查询Mac地址将数据段打包，并加入源及目标的IP地址，并且负责寻找传输路线。 判断目标地址是否与当前地址处于同一网络中，是的话直接根据 Mac 地址发送，否则使用路由表查找下一跳地址，以及使用 ARP 协议查询它的 Mac 地址。 注意：在 OSI 参考模型中 ARP 协议位于链路层，但在 TCP/IP 中，它位于网络层。 4. 链路层：以太网协议以太网协议 根据以太网协议将数据分为以“帧”为单位的数据包，每一帧分为两个部分： 标头：数据包的发送者、接受者、数据类型 数据：数据包具体内容 Mac 地址 以太网规定了连入网络的所有设备都必须具备“网卡”接口，数据包都是从一块网卡传递到另一块网卡，网卡的地址就是 Mac 地址。每一个 Mac 地址都是独一无二的，具备了一对一的能力。 广播 发送数据的方法很原始，直接把数据通过 ARP 协议，向本网络的所有机器发送，接收方根据标头信息与自身 Mac 地址比较，一致就接受，否则丢弃。 注意：接收方回应是单播 四、服务器处理请求大致流程 HTTPD 最常见的 HTTPD 有 Linux 上常用的 Apache 和 Nginx，以及 Windows 上的 IIS。它会监听得到的请求，然后开启一个子进程去处理这个请求。 处理请求 接受 TCP 报文后，会对连接进行处理，对HTTP协议进行解析（请求方法、域名、路径等），并且进行一些验证： 验证是否配置虚拟主机 验证虚拟主机是否接受此方法 验证该用户可以使用该方法（根据 IP 地址、身份信息等） 重定向 假如服务器配置了 HTTP 重定向，就会返回一个 301永久重定向响应，浏览器就会根据响应，重新发送 HTTP 请求（重新执行上面的过程）。 URL 重写 然后会查看 URL 重写规则，如果请求的文件是真实存在的，比如图片、html、css、js文件等，则会直接把这个文件返回。 否则服务器会按照规则把请求重写到 一个 REST 风格的 URL 上。然后根据动态语言的脚本，来决定调用什么类型的动态文件解释器来处理这个请求。 以 PHP 语言的 MVC 框架举例，它首先会初始化一些环境的参数，根据 URL 由上到下地去匹配路由，然后让路由所定义的方法去处理请求。 五、浏览器接受响应浏览器接收到来自服务器的响应资源后，会对资源进行分析。 首先查看 Response header，根据不同状态码做不同的事（比如上面提到的重定向）。如果响应资源进行了压缩（比如 gzip），还需要进行解压。 然后，对响应资源做缓存。接下来，根据响应资源里的 MIME 类型去解析响应内容（比如 HTML、Image各有不同的解析方式）。 六、浏览器渲染页面客户端拿到服务器端传输来的文件，找到HTML和MIME文件，通过MIME文件，浏览器知道要用页面渲染引擎来处理HTML文件。 a.浏览器会解析html源码，然后创建一个 DOM树。 在DOM树中，每一个HTML标签都有一个对应的节点，并且每一个文本也都会有一个对应的文本节点。 b.浏览器解析CSS代码，计算出最终的样式数据，形成css对象模型CSSOM。 首先会忽略非法的CSS代码，之后按照浏览器默认设置——用户设置——外链样式——内联样式——HTML中的style样式顺序进行渲染。 c.利用DOM和CSSOM构建一个渲染树（rendering tree）。 渲染树和DOM树有点像，但是是有区别的。 DOM树完全和html标签一一对应，但是渲染树会忽略掉不需要渲染的元素，比如head、display:none的元素等。 而且一大段文本中的每一个行在渲染树中都是独立的一个节点。渲染树中的每一个节点都存储有对应的css属性。 d.浏览器就根据渲染树直接把页面绘制到屏幕上。 基本流程图]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>DNS解析</tag>
        <tag>TCP连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络模型详解]]></title>
    <url>%2F2019%2F12%2F11%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一、什么是七层模型？什么是四层模型OSI模型（Open System Interconnection Reference Model，缩写为OSI）,全名“开放式系统互联通信参考模型”，是一个试图使各种计算机在全世界范围内互联为网络的标准框架。1983年，国际标准组织（ISO）发布了著名的ISO/IEC 7498标准，它定义了网络互联的7层框架，也就是开放式系统互联参考模型。 1. 为什么需要协议？什么是协议（protocol）？通俗的来讲，协议是一种双方都明白或者必须遵守的事先约定，比如说长城上放狼烟，是因为人们已经预先设定好狼烟这个物理信号代表了“敌人入侵”这一抽象信号。这样一个“狼烟=敌人入侵”就是一个简单的协议。协议可以更复杂，比如摩尔斯码(Morse Code)，使用短信号和长信号的组合，来代表不同的英文字母。 同样，计算机之间的通信也要遵循不同层次的协议，来实现计算机的通信。早期的计算机网络，都是由各厂商自己规定一套协议，IBM，Apple，和MicroSoft都有自己的网络协议，比如MicroSoft的两台电脑用网线连起来，互相说话能听懂。但是MicroSoft和Apple的电脑连接起来说话就听不懂了，想想你和我微信聊天，我是MicroSoft电脑，你是Apple电脑，你发送的消息到我这里显示不了或者解析成另一个意思，这样通讯就不能进行了（通过上面的图我们可以看到，表示层就是消除不同设备之间固有数据格式差异的）。 为了把全世界的所有不同类型的计算机都连接起来，就必须规定一套全球通用的协议，为了实现这个目标，互联网协议簇（Internet Protocol Suite）就成为了通用协议标准。互联网协议包含了上百种协议，但是最重要的两个协议是TCP和IP协议，而我们通常把基于TCP和IP协议的所有协议统称为”TCP/IP协议（蔟）”。 2. OSI七层模型是干什么的？互联网的实现，分成好几层，每一层都有自己的功能，就像建筑物一样，每一层都靠下一层支持。我们在上图中已经大致标出了每一层的功能。OSI模型就是这样的一个分层，它是一个由国际标准化组织提出的概念模型,试图提供一个使各种不同的计算机和网络在世界范围内实现互联的标准框架。它将计算机网络体系结构划分为七层，每层都可以提供抽象良好的接口。 3. TCP/IP四层（参考）模型TCP/IP和OSI模型组并不能精确的匹配，但是我们可以尽可能的参考OSI模型并在其中找到TCP/IP的对应位置。如上图所示，我们已经标出了TCP/IP对应的四层位置所在。通常人们认为OSI模型最上面三层（应用层、表示层、会话层）在TCP/IP中是一个应用层。由于TCP/IP有一个相对比较弱的会话层，由TCP和RTP下的打开和关闭连接组成，并在TCP/UDP下的各种应用提供不同的端口号，这些功能被单个的应用程序添加。 4. TCP/IP（参考）模型与OSI七层模型有什么异同？前面我们说过，TCP/IP协议是互联网协议（簇）的统称，他是互联网标准通信的基础，它提供点对点的链接机制，将数据应该如何封装、定址、传输、路由以及在目的地如何接收，都加以标准化。而OSI模型是开放式系统互联通信参考模型——笔者的理解是： OSI是一个完整的、完善的宏观模型，他包括了硬件层（物理层），当然也包含了很多上面途中没有列出的协议(比如DNS解析协议等)；而TCP/IP（参考）模型，更加侧重的是互联网通信核心（也是就是围绕TCP/IP协议展开的一系列通信协议）的分层，因此它不包括物理层，以及其他一些不想干的协议；其次，之所以说他是参考模型，是因为他本身也是OSI模型中的一部分，因此参考OSI模型对其分层。 二、自底向上的网络分层1. 物理层电脑要组网，第一件事要干什么？当然是先把电脑连起来，可以用光缆、电缆、双绞线、无线电波（WiFi）等方式。物理层的作用就是通过物理手段把电脑连接起来，它主要规定了网络的一些电气特性，作用是负责传送0和1的电信号。 这里说一下，通过物理手段将设备连接起来组网，物理手段就是光缆、电缆、双绞线、无线电波（WiFi）等，比如中美之间的网络通信是通过海底光缆；两个不同的局域网（电信的网络和移动的网络）通讯，嗯，稍微麻烦点，我的电信手机先连上电信的服务器，你的移动手机连移动服务器，他们两个ISP（Internet Service Provider 互联网服务提供商）之间是通过物理手段链接的，这样我们就能够间接的实现通讯了。下图是2015年全球互联网跨国通信光缆的连接情况： 可以看到，图中各个密密麻麻的线就是各个国家的链接情况，所以互联网可以说就是用物理设备将各个“局域网”相连组成的更大的“局域网“，更大局域网层层相连，最终就组成了”互联网“，比如小的互联网就是你家和我家的WiFi,一个省的每户家庭的WiFi组成这个省的局域网，各个省的局域网组成中国的局域网，各个国家之间的局域网通过物理手段互联就组成了横跨世界的“互联网”（当然天朝还有一堵墙）。 2. 数据链路层2.1 定义物理层就是传输电路的0和1信号的，但是单纯的0和1没有意义，必须规定解读方式：多少个0和1算一组？每个信号有什么意义？——这就是链路层的意义，它在物理层的上方，确定了0和1的分组方式。 2.2 以太网协议早些时候，各个公司都有自己的电信号分组方式，后来出现了“以太网”这种协议逐渐占据了主导的地位。“以太网”规定，一组电信号构成一个数据包，叫做“帧（Frame）”；每一帧分成两个部分：标头（Head）和数据（Data）。 因此，数据链路层的数据包就叫“以太网数据包”，他由“标头”和“数据”两部分组成——其中，“标头”包含数据包的一些说明项，比如发送者、接受者、数据类型等等。 2.3 MAC地址上面我们提到，以太网数据包的“标头”包含了发送者和接受者的信息，那么，发送者和接受者是如何标识的呢？ 以太网规定，连入网络的所有设备，都必须具有“网卡”接口。数据包必须是从一块网卡，传送到另一块网卡，网卡的地址，就是数据包的发送地址和接受地址，也叫MAC地址。 每块网卡出厂的时候，都有全世界独一无二的MAC地址，长度是48位的二进制，通常用12个十六进制数表示。 前6个十六进制是厂商编号，后6个是该厂商的网卡流水号，有了MAC地址，就可以定位网卡和数据包的路径了。 2.4 广播定义地址只是第一步，后面还有更多步骤——首先，一块网卡怎么指定另一块网卡的MAC地址？回答是有一种ARP协议，可以解决这个问题。这个留到后面介绍，这里只需要知道，以太网数据包必须知道接收方的MAC地址，然后才能发送。 其次，就算有了MAC地址，系统怎样才能把数据包准确送到接收方？回答是以太网采用了一种很”原始”的广播式的方式，它不是把数据包准确送到接收方，而是向本网络（局域网）内所有计算机发送，让每台计算机自己判断，是否为接收方。 一台计算机向本局域网内的所有电脑均发送相同的数据包，其他计算机收到这个数据包之后，会读取这个数据包的“标头”，找到其中接收方（目标方）的MAC地址，然后与自身的MAC地址进行比对，如果两者相同，说明就是要发给自己的，然后接受这个包并做出进一步的处理，否则丢弃这个包。这种发送方式就叫“广播”，主要通过分组交换机或者网络交换机进行。 3. 网络层根据上面的讲解，理论上依靠MAC地址和广播技术，上海的网卡发出的数据包就可以找到洛杉矶网卡了——但是如果全世界的计算机都这么干，那么每一台计算机发出的数据包都同步广播到全世界其他电脑，再一一比对判断，这样显然是低效、不现实的。 因此，上面我们强调，广播是在发送者所在的局域网内广播的，不同也就是说，如果两台计算机没有在同一个子网（局域网）内，是无法通过广播直接传过去的。前面我们说过，互联网是由一个个子网组成的更大的子网，一级一级组网，最终构成的互联网。 因此我们必须找到一种方法，区分哪些MAC地址属于同一个子网。如果是同一个子网就采用广播的形式，如果不是，则采用“路由”的方式（后面会讲）发送——这就导致了网络层的出现，他的作用是引入一套新的地址，使我们能够区分哪些计算机属于同一个子网，这个套机制就叫做“网络地址”，也就是“IP地址”。 3.1 IP协议规定网络地址的协议，叫IP协议。他定义的地址，就叫做“IP地址”。IP地址目前有IPV4(Internet Protocol version 4，IPv4)和IPV6(Internet Protocol version 4，IPv6)两版，又称“互联网通信协议第四/六版”。2011年，IANA IPv4 pool地址完全用尽时，IPv6仍处在部署的初期，因此IPV4地址也是目前最为广泛的IP地址——这个版本规定，网络地址由32个二进制位组成,习惯上，我们分成四段十进制数表示IPV4地址，从0.0.0.0到255.255.255.255。 互联网上的每一台计算机，都会被分配到一个IP地址，这个地址由两部分组成，前一部分代表网络，后一部分代表主机（又称终端系统，end system）。比如，IP地址172.16.254.1，这是一个32位的地址，假定它的网络部分是前24位（172.16.254），那么主机部分就是后8位（最后的那个1）。处于同一个子网络的电脑，它们IP地址的网络部分必定是相同的，也就是说172.16.254.2应该与172.16.254.1处在同一个子网络，而后面的“2”与“1”则是同一子网内两台不同电脑（主机）的编号。 3.2 子网掩码问题在于单单从IP地址，我们无法判断网络部分。还是以172.16.254.1为例，它的网络部分，到底是前24位，还是前16位，甚至前28位，从IP地址上是看不出来的。那么，怎样才能从IP地址，判断两台计算机是否属于同一个子网络呢？这就要用到另一个参数”子网掩码“（subnet mask）。 所谓”子网掩码”，就是表示子网络特征的一个参数。它在形式上等同于IP地址，也是一个32位二进制数字，它的网络部分全部为1，主机部分全部为0。比如，IP地址172.16.254.1，如果已知网络部分是前24位，主机部分是后8位，那么子网络掩码就是11111111.11111111.11111111.00000000，写成十进制就是255.255.255.0。 知道”子网掩码”，我们就能判断，任意两个IP地址是否处在同一个子网络。方法是将两个IP地址与子网掩码分别进行AND运算（两个数位都为1，运算结果为1，否则为0），然后比较结果是否相同，如果是的话，就表明它们在同一个子网络中，否则就不是。 3.3 路由/路由器/网关/交换机上面我们已经确定了两台计算机是否遭同一个子网中，如果在，则采用广播+MAC寻址的的方式发送数据包，如果不是，则要采用“路由”的方式了，那么什么是“路由”呢？ 路由（routing）就是通过互联的网络把信息从源地址传输到目的地址的活动。路由引导分组转送，经过一些中间的节点后，到它们最后的目的地。 从“路由”的定义中可以看到，“路由”是一种活动，一种动作，一种行为，作用是将信息从原地址传输到目的地址，比较特殊的是，原地址和目标地址是在两个不同的子网中的。那么如何传输呢？路由定义一条路径，经过因特网发送包到另一网络上的地址，但路由不定义完全路径，只定义从主机到可以将包转发到目的地的网关（子网）间的路径段（或从一个子网到另一个子网）。 路由器（Router）简单理解就是实现路由功能的机器。路由器连接两个或多个网络并提供路由功能。 前面我们说过，网卡是计算机的一个硬件，它在接收到网路信息之后，将信息交给计算机。当计算机需要发送信息的时候，也要通过网卡发送。一台计算机可以有不只一个网卡，比如笔记本就有一个以太网卡和一个WiFi网卡。计算机在接收或者发送信息的时候，要先决定想要通过哪个网卡。路由器(router)可以通俗理解为一台配备有多个网卡的专用电脑，它让网卡接入到不同的网络中。 网关（Gateway）是路由器的一种，通常我们把网络层使用的路由器称为网关，路由器可以在网络接口级或物理级路由；网关是在网络层上路由（个人感觉应该是一种概念，即在网络层连接两个子网的概念，并不存在实体，真正实现路由功能还是得靠路由器）。 说的再通俗一点，路由器上面有MAC地址和MAC地址对应的IP，而网关由于是网络层的概念因此只有IP地址。在今天很多局域网采用都是路由来接入网络，因此现在通常指的网关就是路由器的IP。 另外，需要强调一点，虽然路由器上面有MAC地址和IP地址，但它并不能通过MAC地址工作，必须通过IP寻址。因此它是工作在网络层的设备。 网络交换机（Network switch）是一个扩大网络的器材，能为子网中提供更多的连接端口，以便连接更多的电脑。交换机与路由器的区别： 1）工作层次不同 交换机主要工作在数据链路层（第二层） 路由器工作在网络层（第三层） 2）转发依据不同 交换机转发所依据的对象是：MAC地址。（物理地址） 路由转发所依据的对象是：IP地址。（网络地址） 3）主要功能不同 交换机主要用于组建局域网，连接同属于一个(广播域)子网的所有设备，负责子网内部通信（广播）。 路由主要功能是将由交换机组好的局域网相互连接起来，或者将他们接入Internet。 交换机能做的，路由都能做。 交换机不能分割广播域（子网），路由可以。 路由还可以提供防火墙的功能。 路由配置比交换机复杂。 这里我们还是需要说明一点： 交换机虽然主要依靠MAC地址查找工作在数据链路层，但是他也可以有IP地址，这样就可以进行远程登录等操作了。 LAN，全称Local Area Network，中文名叫做局域网；WAN，全称Wide Area Network，中文名叫做广域网。WAN是一种跨越大的、地域性的计算机网络的集合。通常跨越省、市，甚至一个国家。广域网包括大大小小不同的子网，子网可以是局域网，也可以是小型的广域网；WLAN，全称Wireless LAN, 无线局域网，通俗点讲就是WiFi。 家用的路由器，一般包括了交换机和路由器，因此他有两个接口——WAN端口用于连接至Internet；LAN端口用于连接至局域网设备。 举个栗子 上面说了这么多了，我们来举个例子——比如下图中位于中间位置的路由器有两个接口IP，地址分别为199.165.145.17和199.165.146.3。它们分别接入到两个网络：199.165.145和199.165.146。 显然，199.165.145和199.165.146是两个不同的子网，他们通过中间的路由器连接节，这个路由器有两个网卡——199.165.145.17和199.165.146.3。 现在考虑一种情况，我们从主机145.15生成发送到146.21的IP包: 第一步： 先写好数据包的标头，即写清楚发送者的IP地址（199.165.145.15）和接受者的IP地址（199.165.146.21），145.15会参照自己的路由表（routing table），里面有两行记录（当然实际的路由表肯定远远超过两条记录）： 第一行表示，如果IP目的地是199.165.145.0这个网络中的主机，那么说明是在同一个子网中，只需要用自己在eth0上的网卡（MAC地址）通过交换机直接传送，不需要前往router(Gateway 0.0.0.0 = “本地送信”)。 第二行表示所有不符合第一行的IP目的地，则应该送往送往Gateway 199.165.145.17这个主机，也就是中间router接入在eth0的网卡IP地址。 我们的IP包目的地为199.165.146.21，不符合第一行，所以按照第二行，发送到中间的router。主机145.15会在数据包的头部写上199.165.145.17对应的MAC地址，这样，就在199.165.145这个局域网中通过交换机（通过广播MAC地址）广播到199.165.145.17对应的主机（路由器）。 第二步： 中间的router在收到IP包之后，提取目的地IP地址，然后对照自己的routing table： 从前两行我们看到，由于router横跨eth0和eth1两个网络，它可以直接通过eth0和eth1上的网卡直接传送IP包。第三行表示，如果是前面两行之外的IP地址，则需要通过eth1，送往199.165.146.8(右边的router接口IP)。我们的目的地符合第二行，所以将IP放入一个新的帧中，在帧的头部写上199.165.146.21的MAC地址，通过199.165.146网中的交换机广播发往主机146.21。 IP包可以进一步接力，到达更远的主机。IP包从主机出发，根据沿途路由器的routing table指导，在router间接力。IP包最终到达某个router，这个router与目标主机位于一个局域网中，可以直接建立数据链路层的（广播）通信。最后，IP包被送到目标主机。这样一个过程叫做routing（我们就叫IP包接力好了，路由这个词实在是混合了太多的意思）。 整个过程中，IP包不断被主机和路由封装入帧(信封)并拆开，然后借助连接层，在局域网的各个网卡之间传送帧。整个过程中，我们的IP包的内容保持完整，没有发生变化。最终的效果是一个IP包从一个主机传送到另一个主机。利用IP包，我们不需要去操心底层（比如数据链路层）发生了什么。 3.4 ARP协议在上面的过程中，我们实际上假设了，每一台主机和路由都能了解局域网内的IP地址和MAC地址的对应关系，这是实现IP包封装(encapsulation)到帧的基本条件。IP地址与MAC地址的对应是通过ARP协议传播到局域网的每个主机和路由。每一台主机或路由中都有一个ARP cache，用以存储局域网内IP地址和MAC地址如何对应。 ARP协议（ARP介于数据链路层和网络层之间，ARP包需要包裹在一个帧中）的工作方式如下： 主机发出一个ARP包，该ARP包中包含有自己的IP地址和MAC地址。通过ARP包，主机以广播的形式询问局域网上所有的主机和路由：我是IP地址xxxx，我的MAC地址是xxxx，有人知道199.165.146.4的MAC地址吗？拥有该IP地址的主机会回复发出请求的主机：哦，我知道，这个IP地址属于我的一个NIC（网卡），它的MAC地址是xxxxxx。由于发送ARP请求的主机采取的是广播形式，并附带有自己的IP地址和MAC地址，其他的主机和路由会同时检查自己的ARP cache，如果不符合，则更新自己的ARP cache。 这样，经过几次ARP请求之后，ARP cache会达到稳定。如果局域网上设备发生变动，ARP重复上面过程。ARP协议只用于IPv4。IPv6使用Neighbor Discovery Protocol来替代ARP的功能。 4. 传输层4.1 端口号有了MAC地址和IP地址，我们已经可以在互联网上的任意两台电脑之间建立通信了。接下来的问题是，同一台主机上许多程序（进程）都需要用到网络，比如你一边浏览网页一边聊天。当一个数据包从网上发送过来的时候，我们需要一个参数来区分，他到底是提供哪个进程使用的——这个参数就叫做“端口号”，他其实就是每一个使用网卡的程序的编号。每个数据包发送到主机特定的端口，所以不同的程序就能取到自己想要的数据包。 端口是0到65535之间的一个整数，正好16个二进制。0~1023的端口被系统占用，用户只能使用大于1023的端口。不管是浏览网页还是聊天，应用程序都会随机选用一个端口，然后与服务器建立相应的端口关系。这里需要补充一点，HTTP协议默认使用80端口，8080是用来访问代理服务的。 “传输层”的功能，就是建立“端口到端口”之间的通信。相比之下，“网络层”的功能是建立“主机到主机”的通信。只要确定主机和端口号，我们就能实现程序之间的交流。 4.2 Socket上面我们已经说了，传输层是建立“端口到端口”之间的通信，更具体一点，也就是程序和程序之间的通信，或者“进程间通信”。嗯，挺唬人的一个概念。 进程间通信分为两种——一种是主机内部（或终端内部）进程间通信，这个由终端或主机上的操作系统决定，比如在Android系统上面进程间通信就是AIDL；另一种是跨主机进程间通信或者网络进程间通信，也叫“socket通信”。我们可以先笼统的理解——Unix系统把主机+端口，叫做”套接字”（socket），当然，这样说是有失偏颇的。 那么，Socket是什么？ 从编程语言的角度，socket是一个无符号整型变量，用来标识一个通信进程。两个进程通信，总要知道这几个信息：双方的ip地址和端口号，通信所采用的协议栈。socket就是和这些东西绑定的，实现socket可以使用unix提供的接口，也可以使用wIndows提供的winSock。 socket本质是编程接口(API)，对TCP/IP的封装。TCP/IP只是一个协议栈，必须要具体实现，同时还要提供对外的操作接口（API），这就是Socket接口。通过Socket,我们才能使用TCP/IP协议，因此有了一系列我们知道的函数接口——connect、accept、send、read、write等。 JDK的java.net包下有两个类：Socket和ServerSocket，在Client和Server建立连接成功后，两端都会产生一个Socket实例，操作这个实例，完成所需的会话，而程序员就通过这些API进行网络编程。 Socket连接过程分为三个步骤：服务器监听，客户端请求，连接确认。 4.3 UDP/TCP协议UDP和TCP协议都是传输层的协议，他们的主要作用就是在应用层的数据包标头加上端口号（或者在IP协议的数据包中插入端口号）。 UDP协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。 TCP协议可以近似认为是有确认机制的UDP协议。每发出一个数据包都要求确认。如果有一个数据包遗失，就收不到确认，发出方就知道有必要重发这个数据包了。 TCP协议主要的确认机制是”三次握手，四次挥手“,由于这个协议非常复杂，我们会另起一篇文章详细讲解。 5. 应用层应用程序收到”传输层”的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。“应用层”的作用，就是规定应用程序的数据格式。举例来说，TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了”应用层”。这是最高的一层，直接面对用户。 这里的应用层是文章开头的OSI七层模型的最上面三层的综合，因为是直接面向用户，因此它的主要作用是“消除设备固有数据格式和网络标准数据格式直接的差异”，因为在网络流中，数据的格式是标准化的，但是具体到不同的设备，不同的操作系统上，他的要求数据呈现格式是不同的，因此需要转化成统一的、用户能够感知的声音、图片、文字等信息，这就是应用层做的事情。 5.1 用户的上网设置你买了一台新电脑，插上网线，开机，这时电脑能够上网吗？通常你必须做一些设置。有时，管理员（或者ISP）会告诉你下面四个参数，你把它们填入操作系统，计算机就能连上网了： 本机的IP地址 子网掩码 网关的IP地址 DNS的IP地址 下图是Windows系统的设置窗口： 根据上面的讲解，我们应该知道这四个参数的必要性，只有设置了这些我们才能上网。 5.2 DNS解析网域名称系统（英文：Domain Name System，缩写：DNS），端口53，是互联网的一项服务。它作为将域名和IP地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。 什么意思？我们上面已经说了，网络中的数据包，是通过“端口号+IP地址+MAC地址”来识别目的地址的，也就是说定位一个主机的时候，我们是定位的是他的IP地址，然后我们通过浏览器访问的时候呢？比如我们要访问谷歌，就在浏览器中输入他的”域名“：www.google.com，却不是谷歌服务器的IP地址——这个时候，DNS协议就起作用了： 我们输入www.google.com并按下回车的时候，本机服务器先是请求DNS服务器，DNS服务器根据我们发送的域名，根据DNS协议，解析成该域名对应的IP地址并返回给本机，这样，我们就可以进行下面几层的地址封装了。 三、自顶向下的数据包结构现在我们从一个用户的角度来自顶向下的过一遍，一个网络数据包的过程。首先我们设置了本机参数： 本机的IP地址：192.168.1.100 子网掩码：255.255.255.0 网关的IP地址：192.168.1.1 DNS的IP地址：8.8.8.8 然后打开浏览器访问谷歌的网址：www.google.com，按下回车。这意味着，浏览器要向Google发送一个网页请求的数据包。 第一步： 主机会像DNS服务器发送请求，已知DNS服务器为8.8.8.8，于是我们向这个地址发送一个DNS数据包（53端口）；然后，DNS服务器做出响应，告诉我们Google的IP地址是172.194.72.105。于是，我们知道了对方的IP地址。 第二步： 接下来，我们要判断，这个IP地址是不是在同一个子网络，这就要用到子网掩码。已知子网掩码是255.255.255.0，本机用它对自己的IP地址192.168.1.100，做一个二进制的AND运算（两个数位都为1，结果为1，否则为0），计算结果为192.168.1.0；然后对Google的IP地址172.194.72.105也做一个AND运算，计算结果为172.194.72.0。这两个结果不相等，所以结论是，Google与本机不在同一个子网络。 第三步： 因此，我们要向Google发送数据包，必须通过网关192.168.1.1转发，也就是说，接收方的MAC地址将是网关的MAC地址。至此发送的各种必要参数已经基本确定了，数据包也可以发送了。 1. 应用层数据包首先，我们可以知道应用层HTTP协议的报文结构： 这里是请求简书的一个GET请求报文，请求谷歌的报文结构也是相同的，只是域名等内容不同罢了。我们假定这个部分的长度为4960字节，此时的数据包结构如下： 就是一个单纯的数据包，没有头部，数据部分就是上面的报文。 2. 传输层数据包（TCP/UDP数据包）TCP/UDP数据包需要设置端口，接收方（Google）的HTTP端口默认是80，发送方（本机）的端口是一个随机生成的1024-65535之间的整数，假定为51775。TCP数据包的标头长度为20字节，加上嵌入HTTP的数据包，总长度变为4980字节。 可以看到，TCP/UDP数据包就是在应用层数据包前面加上端口号的等必要的寻址信息作为头部。 3. 网络层数据包（IP数据包）下面就到了网络层，TCP数据包再嵌入IP数据包。IP数据包需要设置双方的IP地址，这是已知的，发送方是192.168.1.100（本机），接收方是172.194.72.105（Google）。IP数据包的标头长度为20字节，加上嵌入的TCP数据包，总长度变为5000字节。 4. 数据链路层数据包（以太网数据包）最后，IP数据包嵌入以太网数据包。以太网数据包需要设置双方的MAC地址，发送方为本机的网卡MAC地址，接收方为网关192.168.1.1的MAC地址（通过ARP协议得到）。 以太网数据包的数据部分，最大长度为1500字节，而现在的IP数据包长度为5000字节。因此，IP数据包必须分割成四个包。因为每个包都有自己的IP标头（20字节），所以四个包的IP数据包的长度分别为1500、1500、1500、560。 分割成四个数据包，分割只能分割数据部分，每个数据包都要具有相同的标头，不然找不到目的地址： 服务端响应： 经过多个网关的转发，Google的服务器172.194.72.105，收到了这四个以太网数据包。根据IP标头的序号，Google将四个包拼起来，取出完整的TCP数据包，然后读出里面的”HTTP请求”，接着做出”HTTP响应”，再用TCP协议发回来。本机收到HTTP响应以后，就可以将网页显示出来，完成一次网络通信。 这个例子就到此为止，虽然经过了简化，但它大致上反映了互联网协议的整个通信过程。 我们再总结一下整个过程中数据包的结构包装变化：]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>OSI七层模型</tag>
        <tag>TCP/IP四层模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手和四次挥手详解]]></title>
    <url>%2F2019%2F12%2F11%2FTCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. 三次握手三次握手（Three-way Handshake）其实就是指建立一个TCP连接时，需要客户端和服务器总共发送3个包。进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。实质上其实就是连接服务器指定端口，建立TCP连接，并同步连接双方的序列号和确认号，交换TCP窗口大小信息。 刚开始客户端处于 Closed 的状态，服务端处于 Listen 状态。 进行三次握手： 第一次握手：客户端给服务端发一个 SYN 报文，并指明客户端的初始化序列号 ISN©。此时客户端处于 SYN_SEND 状态。首部的同步位SYN=1，初始序号seq=x，SYN=1的报文段不能携带数据，但要消耗掉一个序号。 第二次握手：服务器收到客户端的 SYN 报文之后，会以自己的 SYN 报文作为应答，并且也是指定了自己的初始化序列号 ISN(s)。同时会把客户端的 ISN + 1 作为ACK 的值，表示自己已经收到了客户端的 SYN，此时服务器处于SYN_REVD 的状态。在确认报文段中SYN=1，ACK=1，确认号ack=x+1，初始序号seq=y。 第三次握手：客户端收到 SYN 报文之后，会发送一个 ACK 报文，当然，也是一样把服务器的 ISN + 1 作为 ACK报文 的值，表示已经收到了服务端的 SYN 报文，此时客户端处于 ESTABLISHED 状态。服务器收到 ACK 报文之后，也处于 ESTABLISHED 状态，此时，双方已建立起了连接。确认报文段ACK=1，确认号ack=y+1，序号seq=x+1（初始为seq=x，第二个报文段所以要+1），ACK报文段可以携带数据，不携带数据则不消耗序号。 发送第一个SYN的一端将执行主动打开（active open），接收这个SYN并发回下一个SYN的另一端执行被动打开（passive open）。 在socket编程中，客户端执行connect()时，将触发三次握手。 TCP报文段的头部结构： 序列号seq：占4个字节，表示一次tcp通信过程（从建立连接到断开）过程中某一次传输方向上的字节流的每个字节的编号。假定主机A和B进行tcp通信，A传送给B一个tcp报文段中，序号值被系统初始化为某一个随机值ISN，那么在该传输方向上（从A到B），后续的所有tcp报文断中的序号值都会被设定为ISN加上该报文段所携带数据的第一个字节在整个字节流中的偏移。例如某个TCP报文段传送的数据是字节流中的第1025~2048字节，那么该报文段的序号值就是ISN+1025。 确认号ack：占4个字节，期待收到对方下一个报文段的第一个数据字节的序号；序列号表示报文段携带数据的第一个字节的编号；而确认号指的是期望接收到下一个字节的编号；因此当前报文段最后一个字节的编号+1即为确认号。 确认ACK：占1位，仅当ACK=1时，确认号字段才有效。ACK=0时，确认号无效 同步SYN：连接建立时用于同步序号。当SYN=1，ACK=0时表示：这是一个连接请求报文段。若同意连接，则在响应报文段中使得SYN=1，ACK=1。因此，SYN=1表示这是一个连接请求，或连接接受报文。SYN这个标志位只有在TCP建立连接时才会被置1，握手完成后SYN标志位被置0。 终止FIN：用来释放一个连接。FIN=1表示：此报文段的发送方的数据已经发送完毕，并要求释放运输连接 PS：ACK、SYN和FIN这些大写的单词表示标志位，其值要么是1，要么是0；ack、seq小写的单词表示序号。 6位标志位： URG：紧急指针是否有效 ACK：表示确认号是否有效，携带ack标志的报文段也称确认报文段 PSH：提示接收端应用程序应该立即从tcp接受缓冲区中读走数据，为后续接收的数据让出空间 RST：表示要求对方重建连接。带RST标志的tcp报文段也叫复位报文段 SYN：表示建立一个连接，携带SYN的tcp报文段为同步报文段 FIN标志：表示告知对方本端要关闭连接了。 1.1 为什么需要三次握手，两次不行吗？弄清这个问题，我们需要先弄明白三次握手的目的是什么，能不能只用两次握手来达到同样的目的。 第一次握手：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。 第二次握手：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。 第三次握手：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。 因此，需要三次握手才能确认双方的接收与发送能力是否正常。 试想如果是用两次握手，则会出现下面这种情况： 如客户端发出连接请求，但因连接请求报文丢失而未收到确认，于是客户端再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接，客户端共发出了两个连接请求报文段，其中第一个丢失，第二个到达了服务端，但是第一个丢失的报文段只是在某些网络结点长时间滞留了，延误到连接释放以后的某个时间才到达服务端，此时服务端误认为客户端又发出一次新的连接请求，于是就向客户端发出确认报文段，同意建立连接，不采用三次握手，只要服务端发出确认，就建立新的连接了，此时客户端忽略服务端发来的确认，也不发送数据，则服务端一致等待客户端发送数据，浪费资源。 1.2 什么是半连接队列？服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。 当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。 这里在补充一点关于SYN-ACK 重传次数的问题：服务器发送完SYN-ACK包，如果未收到客户确认包，服务器进行首次重传，等待一段时间仍未收到客户确认包，进行第二次重传。如果重传次数超过系统规定的最大重传次数，系统将该连接信息从半连接队列中删除。 注意，每次重传等待的时间不一定相同，一般会是指数增长，例如间隔时间为 1s，2s，4s，8s… 1.3 ISN(Initial Sequence Number)是固定的吗？当一端为建立连接而发送它的SYN时，它为连接选择一个初始序号。ISN随时间而变化，因此每个连接都将具有不同的ISN。ISN可以看作是一个32比特的计数器，每4ms加1 。这样选择序号的目的在于防止在网络中被延迟的分组在以后又被传送，而导致某个连接的一方对它做错误的解释。 三次握手的其中一个重要功能是客户端和服务端交换 ISN(Initial Sequence Number)，以便让对方知道接下来接收数据的时候如何按序列号组装数据。如果 ISN 是固定的，攻击者很容易猜出后续的确认号，因此 ISN 是动态生成的。 不过有人就有疑问了：通过抓包就可以计算出来TCP连接的ISN，那固定与不固定ISN有什么区别呢？但我们知道，抓包只能发生在同一网络中，随机ISN能避免非同一网络的攻击。 1.4 三次握手过程中可以携带数据吗？其实第三次握手的时候，是可以携带数据的。但是，第一次、第二次握手不可以携带数据。 为什么这样呢?大家可以想一个问题，假如第一次握手可以携带数据的话，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 SYN 报文中放入大量的数据。因为攻击者根本就不理服务器的接收、发送能力是否正常，然后疯狂着重复发 SYN 报文的话，这会让服务器花费很多时间、内存空间来接收这些报文。 也就是说，第一次握手不可以放数据，其中一个简单的原因就是会让服务器更加容易受到攻击了。而对于第三次的话，此时客户端已经处于 ESTABLISHED 状态。对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了，所以能携带数据也没啥毛病。 1.5 SYN攻击是什么？服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的，所以服务器容易受到SYN洪泛攻击。SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server则回复确认包，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络拥塞甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。 检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击。在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击。 netstat -n -p --tcp | grep SYN_RECV 常见的防御 SYN 攻击的方法有如下几种： 缩短超时（SYN Timeout）时间 增加最大半连接数 过滤网关防护 SYN cookies技术 2. 四次挥手建立一个连接需要三次握手，而终止一个连接要经过四次挥手（也有将四次挥手叫做四次握手的）。这由TCP的半关闭（half-close）造成的。所谓的半关闭，其实就是TCP提供了连接的一端在结束它的发送后还能接收来自另一端数据的能力。 TCP 的连接的拆除需要发送四个包，因此称为四次挥手(Four-way handshake)，客户端或服务器均可主动发起挥手动作。 刚开始双方都处于 ESTABLISHED 状态，假如是客户端先发起关闭请求。四次挥手的过程如下： 第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于 FIN_WAIT1 状态。即发出连接释放报文段（FIN=1，序号seq=u），并停止再发送数据，主动关闭TCP连接，进入FIN_WAIT1（终止等待1）状态，等待服务端的确认。 第二次挥手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 +1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT 状态。即服务端收到连接释放报文段后即发出确认报文段（ACK=1，确认号ack=u+1，序号seq=v），服务端进入CLOSE_WAIT（关闭等待）状态，此时的TCP处于半关闭状态，客户端到服务端的连接释放。客户端收到服务端的确认后，进入FIN_WAIT2（终止等待2）状态，等待服务端发出的连接释放报文段。 第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态。即服务端没有要向客户端发出的数据，服务端发出连接释放报文段（FIN=1，ACK=1，序号seq=w，确认号ack=u+1），服务端进入LAST_ACK（最后确认）状态，等待客户端的确认。 第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 +1 作为自己 ACK 报文的序列号值，此时客户端处于 TIME_WAIT 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态，服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。 即客户端收到服务端的连接释放报文段后，对此发出确认报文段（ACK=1，seq=u+1，ack=w+1），客户端进入TIME_WAIT（时间等待）状态。此时TCP未释放掉，需要经过时间等待计时器设置的时间2MSL后，客户端才进入CLOSED状态。 收到一个FIN只意味着在这一方向上没有数据流动。客户端执行主动关闭并进入TIME_WAIT是正常的，服务端通常执行被动关闭，不会进入TIME_WAIT状态。 在socket编程中，任何一方执行close()操作即可产生挥手操作。 2.1 挥手为什么需要四次？因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉客户端，“你发的FIN报文我收到了”。只有等到我服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次挥手。 2.2 2MSL等待状态TIME_WAIT状态也成为2MSL等待状态。每个具体TCP实现必须选择一个报文段最大生存时间MSL（Maximum Segment Lifetime），它是任何报文段被丢弃前在网络内的最长时间。这个时间是有限的，因为TCP报文段以IP数据报在网络内传输，而IP数据报则有限制其生存时间的TTL字段。 2MSL即两倍的MSL，TCP的TIME_WAIT状态也称为2MSL等待状态，当TCP的一端发起主动关闭，在发出最后一个ACK包后，即第3次握手完成后发送了第四次握手的ACK包后就进入了TIME_WAIT状态，必须在此状态上停留两倍的MSL时间，等待2MSL时间主要目的是怕最后一个ACK包对方没收到，那么对方在超时后将重发第三次握手的FIN包，主动关闭端接到重发的FIN包后可以再发一个ACK应答包。 这种2MSL等待的另一个结果是这个TCP连接在2MSL等待期间，定义这个连接的插口（客户的IP地址和端口号，服务器的IP地址和端口号）不能再被使用。这个连接只能在2MSL结束后才能再被使用。 2.3 四次挥手释放连接时，等待2MSL的意义？MSL是Maximum Segment Lifetime的英文缩写，可译为“最长报文段寿命”，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。 为了保证客户端发送的最后一个ACK报文段能够到达服务器。因为这个ACK有可能丢失，从而导致处在LAST-ACK状态的服务器收不到对FIN-ACK的确认报文。服务器会超时重传这个FIN-ACK，接着客户端再重传一次确认，重新启动时间等待计时器。最后客户端和服务器都能正常的关闭。假设客户端不等待2MSL，而是在发送完ACK之后直接释放关闭，一但这个ACK丢失的话，服务器就无法正常的进入关闭连接状态。 两个理由： 保证客户端发送的最后一个ACK报文段能够到达服务端。 这个ACK报文段有可能丢失，使得处于LAST-ACK状态的服务端收不到对已发送的FIN+ACK报文段的确认，服务端超时重传FIN+ACK报文段，而客户端能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重新启动2MSL计时器，最后客户端和服务端都进入到CLOSED状态，若客户端在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到服务端重传的FIN+ACK报文段，所以不会再发送一次确认报文段，则服务端无法正常进入到CLOSED状态。 防止“已失效的连接请求报文段”出现在本连接中。 客户端在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。 2.4 为什么TIME_WAIT状态需要经过2MSL才能返回到CLOSE状态？理论上，四个报文都发送完毕，就可以直接进入CLOSE状态了，但是可能网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>三次握手</tag>
        <tag>四次挥手</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL核心知识点——索引]]></title>
    <url>%2F2019%2F12%2F02%2FMySQL%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E2%80%94%E2%80%94%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[什么是索引？为什么要建立索引？ 认识索引是什么东西非常关键，一个非常恰当的比喻就是书的目录页与书的正文内容之间的关系，为了方便查找书中的内容，通过对内容建立索引形成目录。索引主要用于快速找出在某个列中有特定值的行，倘若不使用索引，MySQL必须从第一条记录开始读完整个表，直到找出相关的行，表越大，查询数据所花费的时间就越多。如果表中查询的列有一个索引，MySQL能够快速到达一个位置去搜索数据，而不必查找所有数据，那么将会节省很大一部分时间。同时，索引它也是一个文件，它是要占据物理空间的。 比如对于MyISAM存储引擎来说: .frm后缀的文件存储的是表结构。 .myd后缀的文件存储的是表数据。 .myi后缀的文件存储的就是索引文件。 对于InnoDB存储引擎来说: .frm后缀的文件存储的是表结构。 .ibd后缀的文件存放索引文件和数据(需要开启innodb_file_per_table参数) 因此，当你对一张表建立索引时，索引文件的大小也会改变，当你数据表中的数据因为增删改变化时，索引文件也会变化的，只不过MySQL会自动维护索引，这个过程不需要你介入，这也是为什么不恰当的索引会影响MySQL性能的原因。 总结： 索引是按照特定的数据结构把数据表中的数据放在索引文件中，以便于快速查找； 索引存在于磁盘中，会占据物理空间。 索引的优缺点索引的优点： 通过创建唯一索引，可以保证每一行数据的唯一性 可以大大提高查询速度 可以加速表与表的连接 可以显著的减少查询中分组和排序的时间 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能 索引的缺点： 创建索引需要时间，后期创建的索引，创建开销时间与表数据量呈正相关 创建索引时，需要对表加锁，在锁表的同时，可能会影响到其他的数据操作 索引需要磁盘的空间进行存储，如果针对单表创建了大量的索引，可能比数据文件更快达到大小上限 当对表中的数据进行CRUD的时，也会触发索引的维护，而维护索引需要时间，可能会降低数据操作的性能 索引设计的原则不应该： 索引不是越多越好。索引太多，维护索引需要时间，同时索引也需要占用磁盘资源 频繁更新的数据，不宜建索引。数据频繁更新，触发索引频频维护，降低写速度 数据量小的表没必要建立索引。数据量过小，建索引等于多此一举，还增加了操作复杂度 应该： 重复率小的列建议生成索引。因为重复数据少，索引树查询更有效率 数据具有唯一性，建议生成唯一性索引。在数据库的层面，保证数据正确性 频繁group by、order by的列建议生成索引。可以大幅提高分组和排序效率 经常用于查询条件的字段建议生成索引。通过索引查询，速度更快 索引相关SQL查看表中的索引show index from &#123;table_name&#125; 添加索引# 创建表CREATE TABLE &#123;table_name&#125;( ... INDEX &#123;index_name&#125;(&#123;column_name&#125;) );# 修改表alter table &#123;table_name&#125; add unique index &#123;index_name&#125;(&#123;column_name&#125;); # 唯一索引alter table &#123;table_name&#125; add index &#123;index_name&#125;(&#123;column_name&#125;); # 普通索引create index &#123;index_name&#125; on &#123;table_name&#125;(&#123;column_name&#125;); # 普通索引 删除索引drop index &#123;index_name&#125; on &#123;table_name&#125;;alter table &#123;table_name&#125; drop index &#123;index_name&#125; 查看索引命中情况explain select * from &#123;table_name&#125; where &#123;column_name&#125; = xxx; 索引的类型从逻辑角度1. 普通索引普通索引是最基本的索引，它没有任何限制。它有以下几种创建方式：1）直接创建索引CREATE INDEX index_name ON table(column);2）修改表结构的方式添加索引ALTER TABLE table_name ADD INDEX index_name ON (column);3）创建表的时候同时创建索引CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 NOT NULL , `content` text CHARACTER SET utf8 NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (title));4）删除索引DROP INDEX index_name ON table; 2. 唯一索引与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式：1）创建唯一索引CREATE UNIQUE INDEX indexName ON table(column);2）修改表结构ALTER TABLE table_name ADD UNIQUE indexName ON (column);3）创建表的时候直接指定CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 NOT NULL , `content` text CHARACTER SET utf8 NULL , `time` int(10) NULL DEFAULT NULL , UNIQUE indexName (title)); 3. 主键索引是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引：CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) NOT NULL , PRIMARY KEY (`id`)); 4. 组合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀匹配原则。ALTER TABLE `table` ADD INDEX name_city_age (name,city,age); 最左前缀匹配原则：在MySQL建立组合索引时会遵循最左前缀匹配的原则，即最左优先，在检索数据时从组合索引的最左边开始匹配。 5. 全文索引主要用来查找文本中的关键字，而不是直接与索引中的值相比较。fulltext索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的where语句的参数匹配。fulltext索引配合match against操作使用，而不是一般的where语句加like。它可以在create table，alter table ，create index使用，不过目前只有char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，先将数据放入一个没有全局索引的表中，然后再用CREATE index创建fulltext索引，要比先为一张表建立fulltext然后再将数据写入的速度快很多。1）创建表的时候添加全文索引CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 NOT NULL , `content` text CHARACTER SET utf8 NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), FULLTEXT (content));2）修改表结构添加全文索引ALTER TABLE article ADD FULLTEXT index_content(content);3）直接创建索引CREATE FULLTEXT INDEX index_content ON article(content); 从物理存储角度 聚集索引（clustered index）非聚集索引（non-clustered index） 简单概况一下： 聚集索引就是以主键创建的索引 非聚集索引就是除了主键以外的索引。 非聚集索引也叫做二级索引，不用纠结那么多名词，将其等价就行了。 非聚集索引在建立的时候也未必是单列的，可以多个列来创建索引。 本质区别：表记录的排列顺序和索引的排列顺序是否一致。 从数据结构角度1. B-Tree和B+TreeB+树索引是B+树在数据库中的一种实现，是最常见也是数据库中使用最为频繁的一种索引。B+树中的B代表平衡（balance），而不是二叉（binary），因为B+树是从最早的平衡二叉树演化而来的。在讲B+树之前必须先了解二叉查找树、平衡二叉树（AVLTree）和平衡多路查找树（B-Tree），B+树即由这些树逐步优化而来。 二叉查找树二叉树具有以下性质：左子树的键值小于根的键值，右子树的键值大于根的键值。如下图所示，就是一颗二叉查找树： 二叉查找树可以任意地构造，同样是2,3,5,6,7,8这六个数字，也可以按照下图的方式来构造： 但是这棵二叉树的查询效率就低了。因此若想二叉树的查询效率尽可能高，需要这棵二叉树是平衡的，从而引出新的定义——平衡二叉树，或称AVL树。 平衡二叉树（AVL Tree）平衡二叉树（AVL树）在符合二叉查找树的条件下，还满足任何节点的两个子树的高度最大差为1。下面的两张图片，左边是AVL树，它的任何节点的两个子树的高度差&lt;=1；右边的不是AVL树，其根节点的左子树高度为3，而右子树高度为1； 如果在AVL树中进行插入或删除节点，可能导致AVL树失去平衡，这种失去平衡的二叉树可以概括为四种姿态：LL（左左）、RR（右右）、LR（左右）、RL（右左）。它们的示意图如下： 这四种失去平衡的姿态都有各自的定义：LL：LeftLeft，也称“左左”。插入或删除一个节点后，根节点的左孩子（Left Child）的左孩子（Left Child）还有非空节点，导致根节点的左子树高度比右子树高度高2，AVL树失去平衡。 RR：RightRight，也称“右右”。插入或删除一个节点后，根节点的右孩子（Right Child）的右孩子（Right Child）还有非空节点，导致根节点的右子树高度比左子树高度高2，AVL树失去平衡。 LR：LeftRight，也称“左右”。插入或删除一个节点后，根节点的左孩子（Left Child）的右孩子（Right Child）还有非空节点，导致根节点的左子树高度比右子树高度高2，AVL树失去平衡。 RL：RightLeft，也称“右左”。插入或删除一个节点后，根节点的右孩子（Right Child）的左孩子（Left Child）还有非空节点，导致根节点的右子树高度比左子树高度高2，AVL树失去平衡。 AVL树失去平衡之后，可以通过旋转使其恢复平衡。下面分别介绍四种失去平衡的情况下对应的旋转方法。 LL的旋转。LL失去平衡的情况下，可以通过一次旋转让AVL树恢复平衡。步骤如下： 将根节点的左孩子作为新根节点。 将新根节点的右孩子作为原根节点的左孩子。 将原根节点作为新根节点的右孩子。 LL旋转示意图如下： RR的旋转：RR失去平衡的情况下，旋转方法与LL旋转对称，步骤如下： 将根节点的右孩子作为新根节点。 将新根节点的左孩子作为原根节点的右孩子。 将原根节点作为新根节点的左孩子。 RR旋转示意图如下： LR的旋转：LR失去平衡的情况下，需要进行两次旋转，步骤如下： 围绕根节点的左孩子进行RR旋转。 围绕根节点进行LL旋转。 LR的旋转示意图如下： RL的旋转：RL失去平衡的情况下也需要进行两次旋转，旋转方法与LR旋转对称，步骤如下： 围绕根节点的右孩子进行LL旋转。 围绕根节点进行RR旋转。 RL的旋转示意图如下： 平衡多路查找树（B-Tree）B-Tree是为磁盘等外存储设备设计的一种平衡查找树。因此在讲B-Tree之前先了解下磁盘的相关知识。 系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来，而不是需要什么取什么。 InnoDB存储引擎中有页（Page）的概念，页是其磁盘管理的最小单位。InnoDB存储引擎中默认每个页的大小为16KB，可通过参数innodb_page_size将页的大小设置为4K、8K、16K，在MySQL中可通过如下命令查看页的大小：show variables like 'innodb_page_size'; 而系统一个磁盘块的存储空间往往没有这么大，因此InnoDB每次申请磁盘空间时都会是若干地址连续磁盘块来达到页的大小16KB。InnoDB在把磁盘数据读入到内存时会以页为基本单位，在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘I/O次数，提高查询效率。 B-Tree结构的数据可以让系统高效的找到数据所在的磁盘块。为了描述B-Tree，首先定义一条记录为一个二元组[key, data]，key为记录的键值，对应表中的主键值，data为一行记录中除主键外的数据。对于不同的记录，key值互不相同。 一棵m阶的B-Tree有如下特性： 每个节点最多有m个孩子。 除了根节点和叶子节点外，其它每个节点至少有Ceil(m/2)个孩子。（Ceil(m/2)为小数向上取整，如果是个3阶B-Tree，那每个节点至少有2个孩子，和下图磁盘块3和磁盘块8不符？此处没有理解） 若根节点不是叶子节点，则至少有2个孩子 所有叶子节点都在同一层，且不包含其它关键字信息 每个非终端节点包含n个关键字信息（P0,P1,…Pn, k1,…kn） 关键字的个数n满足：ceil(m/2)-1 &lt;= n &lt;= m-1 ki(i=1,…n)为关键字，且关键字升序排序。 Pi(i=1,…n)为指向子树根节点的指针。P(i-1)指向的子树的所有节点关键字均小于ki，但都大于k(i-1) B-Tree中的每个节点根据实际情况可以包含大量的关键字信息和分支，如下图所示为一个3阶的B-Tree： 每个节点占用一个盘块的磁盘空间（实际上，每个节点为一个页，包含若干地址连续的磁盘块），一个节点上有两个升序排序的关键字和三个指向子树根节点的指针，指针存储的是子节点所在磁盘块的地址。两个关键词划分成的三个范围域对应三个指针指向的子树的数据的范围域。以根节点为例，关键字为17和35，P1指针指向的子树的数据范围为小于17，P2指针指向的子树的数据范围为17~35，P3指针指向的子树的数据范围为大于35。 模拟查找关键字29的过程： 根据根节点找到磁盘块1，读入内存。【磁盘I/O操作第1次】 比较关键字29在区间（17,35），找到磁盘块1的指针P2。 根据P2指针找到磁盘块3，读入内存。【磁盘I/O操作第2次】 比较关键字29在区间（26,30），找到磁盘块3的指针P2。 根据P2指针找到磁盘块8，读入内存。【磁盘I/O操作第3次】 在磁盘块8中的关键字列表中找到关键字29。 分析上面过程，发现需要3次磁盘I/O操作，和3次内存查找操作。由于内存中的关键字是一个有序表结构，可以利用二分法查找提高效率。而3次磁盘I/O操作是影响整个B-Tree查找效率的决定因素。B-Tree相对于AVLTree缩减了节点个数，使每次磁盘I/O取到内存的数据都发挥了作用，从而提高了查询效率。 B+TreeB+Tree是在B-Tree基础上的一种优化，使其更适合实现外存储索引结构，InnoDB存储引擎就是用B+Tree实现其索引结构。 从上一节中的B-Tree结构图中可以看到每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。 与B-Tree相比，B+Tree有以下不同点： 非叶子节点只存储key键值信息。 所有叶子节点之间都有一个链指针。 数据记录都存放在叶子节点中。 每个节点的指针上限为2d而不是2d+1。 将上一节中的B-Tree优化，由于B+Tree的非叶子节点只存储键值信息，假设每个磁盘块能存储4个键值及指针信息，则变成B+Tree后其结构如下图所示： 通常在B+Tree上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。因此可以对B+Tree进行两种查找运算：一种是对于主键的范围查找和分页查找，另一种是从根节点开始，进行随机查找。 可能上面例子中只有22条数据记录，看不出B+Tree的优点，下面做一个推算： InnoDB存储引擎中页的大小为16KB，一般表的主键类型为INT（占用4个字节）或BIGINT（占用8个字节），指针类型也一般为4或8个字节，也就是说一个页（B+Tree中的一个节点）中大概存储16KB/(8B+8B)=1K个键值（因为是估值，为方便计算，这里的K取值为10^3）。也就是说一个深度为3的B+Tree索引可以维护10^3 * 10^3 * 10^3 = 10亿条记录。 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree的高度一般都在2~4层。mysql的InnoDB存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要1~3次磁盘I/O操作。 数据库中的B+Tree索引可以分为聚集索引（clustered index）和非聚集索引（non-clustered index）。上面的B+Tree示例图在数据库中的实现即为聚集索引，聚集索引的B+Tree中的叶子节点存放的是整张表的行记录数据。非聚集索引与聚集索引的区别在于非聚集索引的叶子节点并不包含行记录的全部数据，而是存储相应行数据的聚集索引键，即主键。当通过非聚集索引来查询数据时，InnoDB存储引擎会遍历非聚集索引找到主键，然后再通过主键在聚集索引中找到完整的行记录数据。 这一点，在MyISAM和InnoDB的主键之间表现是不同的。MyISAM使用的是非聚集索引，最好的表现在MyISAM的存储文件分为索引文件(.MYI)和数据文件(.MYD)，而InnoDB使用的是聚集索引，索引和数据在一个文件里。 2. hash索引 一句话概括：基于哈希表实现，对于每一行数据，存储引擎都会对所有的索引列计算出一个哈希码，将所有的哈希码存储在哈希表中作为key，同时在哈希表中保存指向每个数据行指针，作为value存储在哈希表中。 假设有下表：CREATE TABLE student( first_name varchar(20) not null, last_name varchar(20) not null, age tinyint(3) not null, created_at timestamp not null, key using hash(first_name)) engine=memory; 如果我们要执行select last_name from student where first_name=&#39;zhang&#39;;，MySQL会先计算zhang的哈希值，然后用该值寻找对应的记录指针，最后再去比较first_name是否等于zhang。因为哈希索引只存储对于的哈希值和行指针，所以哈希索引的结构很紧凑，查询速度非常快。但是也有一些缺点。 因为哈希索引只有哈希值与指针，所以每次查询必须回表去读取数据行。 因为哈希索引不是按照索引值顺序存储的，所以哈希索引也不能用于排序。 哈希索引不支持部分索引列查询，比如将student表的索引改为hash(first_name,last_name)，那么查询必须用到first_name,last_name才会使用到索引。 哈希索引只支持等值比较，所以&lt;,&gt;等范围查询是不会使用到索引的。 哈希索引也会存在哈希冲突，当出现冲突的时候，查询效率就很降低很多。 主流存储引擎不支持该类型，比如MyISAM和InnoDB。哈希索引只有Memory引擎支持。（但InnoDB有另一种实现方法：自适应哈希索引。InnoDB存储引擎会监控对表上索引的查找，如果观察到建立哈希索引可以带来速度的提升，则建立哈希索引。） 3. 空间数据索引(R-Tree)空间索引可用于地理数据存储，它需要GIS相关函数的支持，由于MySQL的GIS支持并不完善，所以该索引方式在MySQL中很少有人使用。 4. 全文索引（FULLTEXT）全文索引主要用于海量数据的搜索，比如淘宝或者京东对商品的搜索，你不可能使用like进行模糊匹配吧，MySQL从5.6开始支持InnoDB引擎的全文索引，功能没有专业的搜索引擎比如Sphinx或Solr丰富，如果你的需求比较简单，可以尝试一下MySQL的全文索引，否则建议使用专业的搜索引擎。 总结:1.B+Tree索引使用最广泛，主流引擎都支持。2.哈希索引性能高，适用于特殊场合。3.R-Tree不常用。4.全文索引适用于海量数据的关键字模糊搜索。 索引和存储引擎之间的关系上面讲述了索引有不同的类型，存储引擎也有不同的类型，那么索引和存储引擎之间有什么关系呢？首先你需要知道，在MySQL中，索引是在存储引擎中实现的，并不是所有的存储引擎都支持所有的索引类型，比如哈希索引，MyISAM和InnoDB是不支持的；同样，即使对于同一类型的索引，不同的存储引擎实现的方式也可能是不同的，比如MyISAM和InnoDB对B+Tree索引，具体的实现是有差别的。 总结:1.不同的存储引擎可能支持不同的索引类型；2.不同的存储引擎对同一中索引类型可能有不同的实现方式。 聚集索引，非聚集索引，覆盖索引的原理想要理解索引原理必须清楚一种数据结构「平衡树」(非二叉)，也就是b tree或者 b+ tree，重要的事情说三遍：“平衡树，平衡树，平衡树”。当然，有的数据库也使用哈希桶作用索引的数据结构，然而，主流的RDBMS都是把平衡树当做数据表默认的索引数据结构。 我们平时建表的时候都会为表加上主键，在某些关系数据库中，如果建表时不指定主键，数据库会拒绝建表的语句执行。事实上，一个加了主键的表，并不能被称之为「表」。一个没加主键的表，它的数据无序的放置在磁盘存储器上，一行一行的排列的很整齐，跟我认知中的「表」很接近。如果给表上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是上面说的「平衡树」结构，换句话说，就是整个表就变成了一个索引。没错，再说一遍，整个表变成了一个索引，也就是所谓的「聚集索引」。 这就是为什么一个表只能有一个主键，一个表只能有一个「聚集索引」，因为主键的作用就是把「表」的数据格式转换成「索引（平衡树）」的格式放置。 上图就是带有主键的表（聚集索引）的结构图。其中树的所有结点（底部除外）的数据都是由主键字段中的数据构成，也就是通常我们指定主键的id字段。最下面部分是真正表中的数据。假如我们执行一个SQL语句： select * from table where id = 1256; 首先根据索引定位到1256这个值所在的叶结点，然后再通过叶结点取到id等于1256的数据行。这里不讲解平衡树的运行细节，但是从上图能看出，树一共有三层，从根节点至叶节点只需要经过三次查找就能得到结果。如下图： 假如一张表有一亿条数据，需要查找其中某一条数据，按照常规逻辑，一条一条的去匹配的话，最坏的情况下需要匹配一亿次才能得到结果，用大O标记法就是O(n)最坏时间复杂度，这是无法接受的，而且这一亿条数据显然不能一次性读入内存供程序使用，因此，这一亿次匹配在不经缓存优化的情况下就是一亿次IO开销，以现在磁盘的IO能力和CPU的运算能力，有可能需要几个月才能得出结果。 如果把这张表转换成平衡树结构（一棵非常茂盛和节点非常多的树），假设这棵树有10层，那么只需要10次IO开销就能查找到所需要的数据，速度以指数级别提升，用大O标记法就是O(log n)，n是记录总树，底数是树的分叉数，结果就是树的层次数。换言之，查找次数是以树的分叉数为底，记录总数的对数，用公式来表示就是： 用程序来表示就是Math.Log(100000000,10)，100000000是记录数，10是树的分叉数（真实环境下分叉数远不止10），结果就是查找次数，这里的结果从亿降到了个位数。因此，利用索引会使数据库查询有惊人的性能提升。 然而，事物都是有两面的，索引能让数据库查询数据的速度上升，而使写入数据的速度下降，原因很简单的，因为平衡树这个结构必须一直维持在一个正确的状态，增删改数据都会改变平衡树各节点中的索引数据内容，破坏树结构，因此，在每次数据改变时，DBMS必须去重新梳理树（索引）的结构以确保它的正确，这会带来不小的性能开销，也就是为什么索引会给查询以外的操作带来副作用的原因。 聚集索引一般是表中的主键索引，如果表中没有显示指定主键，则会选择表中的第一个不允许为NULL的唯一索引，如果还是没有的话，就采用Innodb存储引擎为每行数据内置的6字节ROWID作为聚集索引。 讲完聚集索引，接下来聊一下非聚集索引，也就是我们平时经常提起和使用的常规索引。 非聚集索引和聚集索引一样，同样是采用平衡树作为索引的数据结构。索引树结构中各节点的值来自于表中的索引字段，假如给user表的name字段加上索引，那么索引就是由name字段中的值构成，在数据改变时，DBMS需要一直维护索引结构的正确性。如果给表中多个字段加上索引，那么就会出现多个独立的索引结构，每个索引（非聚集索引）互相之间不存在关联。如下图： 每次给字段建一个新索引，字段中的数据就会被复制一份出来，用于生成索引。因此，给表添加索引，会增加表的体积，占用磁盘存储空间。 非聚集索引和聚集索引的区别在于，通过聚集索引可以查到需要查找的数据，而通过非聚集索引可以查到记录对应的主键值，再使用主键的值通过聚集索引查找到需要的数据，如下图： 不管以任何方式查询表，最终都会利用主键通过聚集索引来定位到数据，聚集索引（主键）是通往真实数据所在的唯一路径。 然而，有一种例外可以不使用聚集索引就能查询出所需要的数据，这种非主流的方法称之为「覆盖索引」查询，也就是平时所说的复合索引或者多字段索引查询。文章上面的内容已经指出，当为字段建立索引以后，字段中的内容会被同步到索引之中，如果为一个索引指定两个字段，那么这个两个字段的内容都会被同步至索引之中。 先看下面这个SQL语句//建立索引create index index_birthday on user_info(birthday);//查询生日在1991年11月1日出生用户的用户名select user_name from user_info where birthday = '1991-11-1'; 这句SQL语句的执行过程如下: 首先，通过非聚集索引index_birthday查找birthday等于1991-11-1的所有记录的主键ID值 然后，通过得到的主键ID值执行聚集索引查找，找到主键ID值对就的真实数据（数据行）存储的位置 最后，从得到的真实数据中取得user_name字段的值返回，也就是取得最终的结果 我们把birthday字段上的索引改成双字段的覆盖索引：create index index_birthday_and_user_name on user_info(birthday, user_name); 这句SQL语句的执行过程就会变为: 通过非聚集索引index_birthday_and_user_name查找birthday等于1991-11-1的叶节点的内容，然而，叶节点中除了有user_name表主键ID的值以外，user_name字段的值也在里面，因此不需要通过主键ID值的查找数据行的真实所在，直接取得叶节点中user_name的值返回即可。通过这种覆盖索引直接查找的方式，可以省略不使用覆盖索引查找的后面两个步骤，大大的提高了查询性能，如下图： 覆盖索引可以完美的解决二级索引回表查询问题。但是前提是一定得注意查询时候索引的最左侧匹配原则。 使用聚集索引的查询效率要比非聚集索引的效率要高，但是如果需要频繁去改变聚集索引的值，写入性能并不高，因为需要移动对应数据的物理位置。 聚集索引两点关键信息：根据主键值创建了 B+ 树结构，每个叶子节点包含了整行数据。 MySQL中每个表都有一个聚簇索引（clustered index ），除此之外的表上的每个非聚簇索引都是二级索引，又叫辅助索引（secondary indexes）。 总的来说，在MySQL中MyIsAm使用的是B+tree索引结构，叶节点的data仅仅存放的是指向数据记录的一个地址，在MyIsAm中主键索引和辅助索引没有任何区别，只是主键索引要求key是唯一的，而辅助索引的key可以重复。 在InnoDB中使用的也是B+tree索引结构，但是在实现方式来说和MyIsAm完全不同，MyIsAm的数据文件和索引文件以及表定义文件都是分开的，MyIsAm中索引文件只是存储一个指向具体数据的一个指针。但是在innodb中，Btree可以分为两种：主键索引和二级索引（也叫辅助索引）。 innodb中主键索引一定是聚集索引，表数据文件本身就是一个B+tree结构，这个树的叶节点保存了每一条记录的完整数据，这个叶节点的key就是数据的主键，innodb中二级索引保存的是索引列值以及指向主键的指针，所以我们使用覆盖索引优化其实说白了就是对MySQL的innodb索引加速的。 MyISAM引擎中leaf node存储的内容：主键索引仅仅存储行指针；二级索引存储的也仅仅是行指针。InnoDB引擎中leaf node存储的内容：主键索引：聚集索引存储完整的数据（整行数据）；二级索引：存储索引列值+主键信息。 后记这篇文章，目的主要是对MySQL的索引有个概念上的认识，以及了解索引的类型，索引和存储引擎之间的关系。 因为我本人对MySQL的使用属于菜鸟级别，更没有太多数据库调优的经验，在这里大谈数据库索引调优有点大言不惭，就当是我个人的一篇学习笔记了。 其实数据库索引调优是一项技术活，不能仅仅靠理论，因为实际情况千变万化，而且MySQL本身存在很复杂的机制，如查询优化策略和各种引擎的实现差异等都会使情况变得更加复杂。但同时这些理论是索引调优的基础，只有在明白理论的基础上，才能对调优策略进行合理推断并了解其背后的机制，然后结合实践中不断的实验和摸索，从而真正达到高效使用MySQL索引的目的。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL核心知识点——锁机制]]></title>
    <url>%2F2019%2F11%2F25%2FMySQL%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E2%80%94%E2%80%94%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一、锁的分类 1、按锁的粒度划分，可分为表级锁、行级锁、页级锁2、按锁级别划分，可分为共享锁、排他锁3、按使用方式划分，可分为乐观锁、悲观锁 1.1 按粒度划分的锁1.1.1 表级锁（偏向于读） 优缺点：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低 支持引擎：MyISAM、MEMORY、InNoDB 表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁） 1.1.2 行级锁 优缺点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 支持引擎：InnoDB 行级锁定分为行共享读锁（共享锁）与行独占写锁（排他锁） 行锁的锁定颗粒度在MySQL中是最细的，应用于InnoDB存储引擎，只针对操作的当前行进行加锁。并发情况下，产生锁等待的概率较低，支持较大的并发数，但开销大，加锁慢，而且会出现死锁。 在InnoDB中使用行锁有一个前提条件：检索数据时需要通过索引。因为InnoDB是通过给索引的索引项加锁来实现行锁的。 在不通过索引条件查询的时候，InnoDB会使用表锁，这在并发较大时，可能导致大量的锁冲突。此外，行锁是针对索引加锁，存在这种情况，虽然是访问的不同记录，但使用的是同一索引项，也可能会出现锁冲突。提示：不一定使用了索引检索就一定会使用行锁，也有可能使用表锁。因为MySQL会比较不同执行计划的代价，当全表扫描比索引效率更高时，InnoDB就使用表锁。因此需要结合SQL的执行计划去分析锁冲突。 行锁会产生死锁，因为在行锁中，锁是逐步获得的，主要分为两步：锁住主键索引，锁住非主键索引。如：当两个事务同时执行时，一个锁住了主键索引，在等待其他索引；另一个锁住了非主键索引，在等待主键索引。这样便会发生死锁。InnoDB一般都可以检测到这种死锁，并使一个事务释放锁回退，另一个获取锁完成事务。 1.1.3 页级锁对于行级锁与表级锁的折中，开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 表锁定与行锁定实际操作数据库 1.2 按锁的级别划分1.2.1 共享锁（读锁）一个事务获取了一个数据行的共享锁，本事务可以对该数据进行修改，其他事务都能访问到数据，但是只能读不能修改，且不允许对数据行加排他锁。 用法：SELECT … LOCK IN SHARE MODE;前边必须使用begin 1.2.2 排他锁（写锁）一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行读取和修改。可以直接通过select …from…查询数据，因为普通查询没有任何锁机制。 用法：SELECT … FOR UPDATE;前边必须使用begin 由于InnoDB预设是Row-Level Lock，所以只有「明确」的指定主键，MySQL才会执行Row lock (只锁住被选取的资料例) ，否则MySQL将会执行Table Lock (将整个资料表单给锁住)。 假设一张表t，有id和name两个字段主键为idselect * from t where id = 1 for update; /*明确指定主键，并且有此笔记录，row lock*/select * from t where id = 11 for update; /*明确指定主键，若查无此笔记录，无lock*/select * from t where name ='qweq' for update; /*无主键，table lock*/select * from t where id &lt;&gt; 2 for update; /*主键不明确，table lock*/select * from t where id like 3 for update; /*主键不明确，table lock*/ 表级锁分为：读锁和写锁（lock table 表名称 read(write),表名称2 read(write)...）行级锁分为：共享锁和排他锁 二、MyISAM存储引擎的锁2.1 支持表锁（偏向于读） MyISAM在执行SQL语句时，会自动为SELECT语句加上共享锁，为UPDATE/DELETE/INSERT操作加上排他锁 MyISAM读写、写写之间是串行的，读读之间是并行的 由于表锁的锁定粒度大，读写又是串行的，因此如果更新操作较多，ＭyISAM表可能会出现严重的锁等待 2.2 并发锁在存储引擎中有一个系统变量concurrent_insert，专门控制其并发插入的行为： concurrent_insert=0时，不允许并发插入功能。 concurrent_insert=1时，允许对没有空洞（即表的中间没有被删除的行）的表使用并发插入，新数据位于数据文件结尾（缺省）。 concurrent_insert=2时，不管表有没有空洞，都允许在数据文件结尾并发插入。 只需在加表锁命令中加入“local”选项，即：lock table tbl_name local read，在满足MyISAM表并发插入条件的情况下，其他用户就可以在表尾并发插入记录，但更新操作会被阻塞，而且加锁的用户无法访问到其他用户并发插入的记录。 在Mysql 5.5.2及以下版本concurrent_insert参数使用数值型默认为1，从5.5.3版本开始concurrent_insert参数用枚举值默认为AUTO。 2.3 锁调度前面讲过，MyISAM存储引擎的读锁和写锁是互斥的，读写操作是串行的。那么，一个进程请求某个 MyISAM表的读锁，同时另一个进程也请求同一表的写锁，MySQL如何处理呢？答案是写进程先获得锁。不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插到读锁请求之前！这是因为MySQL认为写请求一般比读请求要重要。这也正是MyISAM表不太适合于有大量更新操作和查询操作应用的原因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。这种情况有时可能会变得非常糟糕！幸好我们可以通过一些设置来调节MyISAM 的调度行为。 设置MyISAM调度行为：a、通过指定启动参数low-priority-updates，使MyISAM引擎默认给予读请求以优先的权利。b、通过执行命令SET LOW_PRIORITY_UPDATES=1，使该连接发出的更新请求优先级降低。c、通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性，降低该语句的优先级。d、系统参数max_write_lock_count设置一个合适的值；当一个表的读锁达到这个值后，MySQL便暂时将写请求的优先级降低，给读进程一定获得锁的机会。 MySQL中UPDATE的LOW_PRIORITY解决并发问题 三、InnoDB存储引擎的锁与InnoDB与MyISAM的最大不同有两点: 支持事务 采用行锁 3.1 支持事务1. 事务ACID（原子性、一致性、隔离性和持久性）2. 事务的并发处理导致的问题 更新丢失：读取数据后，被其他事务覆盖数据 脏读：读取数据后，更新数据的事务回滚了，也就是读取的数据不正确 不可重复读：由于其他事务的插手，在同一事务中两次相同的查询数据是不同的（由于修改导致） 幻读：由于事务1有了新增或删除操作，事务2读取不到事务1操作的数据行，因此再操作对应的行就会被阻塞。（特别注意：幻读并不是两次查询的结果不同） 3. 事务隔离级别更新数据丢失不仅仅是数据库事务控制器解决，主要由应用解决。本来是为了实现事务的并发，以下操作对于并发的副作用越来越小，但付出的代价越来越大。 读未提交的数据（Read uncommitted）：可能有脏读、不可重复读、幻读的问题 读提交的数据（Read committed），没有脏读的问题，可能有不可重复读、幻读的问题 可重复读（Repeatable read）：没有脏读、不可重复读的问题，可能有幻读的问题（InnoDB通过多版本并发控制MVCC解决了幻读问题） 可串行化（Serializable）：没有脏读、不可重复读、幻读的问题 3.2 行锁 行级锁分为： 记录锁（Record lock）：对索引项加锁，即锁定一条记录。 间隙锁（Gap lock）：对索引项之间的“间隙”、对第一条记录前的间隙或最后一条记录后的间隙加锁，即锁定一个范围的记录，不包含记录本身 Next-key Lock：锁定一个范围的记录并包含记录本身（上面两者的结合）。 3.2.1 共享锁与排他锁 共享锁：事务对数据添加了读锁，本事务可以对该数据进行修改，其他事务也只能加读锁，期间不能修改，直到事务释放读锁；若其他事务也获得读锁，则本事务和其他事务对该数据的修改都会被阻塞 排他锁：若事务T对数据加排他锁，本事务可以读也可以写数据，其他事务不能对数据加任何锁，直到事务T释放锁 3.2.2 意向共享锁与意向排他锁 意向共享锁（IS）：事务打算给数据行加共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁 意向排他锁（IX）：事务打算给数据行加排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁 注意：普通select操作不会添加任何锁 注意：申请意向锁的动作是数据库完成的，就是说，事务A申请一行的行锁的时候，数据库会自动先开始申请表的意向锁，不需要我们程序员使用代码来申请。 3.2.3 行级锁（Record lock）导致的死锁为什么会产生死锁？ 1、产生死锁原理：在MySQL中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。在UPDATE、DELETE操作时，MySQL不仅锁定WHERE条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的next-key locking。（如UPDATE USER SET NAME=’HELLO’ WHERE ID &gt; 5会锁定所有主键大于等于5的记录，在该语句完成前，不能对主键等于5的记录进行操作加锁） 2、死锁导致原因：当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。 3、如何避免死锁：用SHOW ENGINE INNODB STATUS;命令来确定最后一个死锁产生的原因和改进措施 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 在程序以批量方式处理数据的时候，如果事先对数据排序，保证每个线程按固定的顺序来处理记录，也可以大大降低出现死锁的可能。 记录一次MySQL死锁排查过程 3.2.4 行级锁的间隙锁（Next-Key lock）1、什么时候会出现间隙锁？用法：select * from 表名 where 字段名 &gt; 参数 for update;使用范围条件而不是相等条件检索数据，InnoDB除了给索引记录加锁，还会给不存在的记录（间隙）加锁，其他事务不能操作当前事务锁定的索引与间隙。 2、目的 防止幻读，避免其他事务插入数据 满足其恢复和复制的需要，MySQL的恢复机制是通过BINLOG记录来执行IUD操作来同步Slave的，这就要求：在一个事务未提交前，其他并发事务不能插入满足其锁定条件的任何记录，为了恢复，不能插入其他事务。 3、危害因为Query执行过程中通过范围查找的话，它会锁定整个范围内所有的索引键值，即使这个键值并不存在。间隙锁有一个比较致命的弱点，就是当锁定一个范围键值之后，即使某些不存在的键值也会被锁定，而造成在锁定期间无法插入锁定键值范围内的任何数据，在某些场景下这可能会对性能造成很大的危害。 间隙锁实例博客 3.2.5 什么时候使用表锁？绝大部分情况使用行锁，但在个别特殊事务中，也可以考虑使用表锁 1. 事务需要更新大部分数据，表又较大 若使用默认的行锁，不仅该事务执行效率低（因为需要对较多行加锁，加锁是需要耗时的）；而且可能造成其他事务长时间锁等待和锁冲突；这种情况下可以考虑使用表锁来提高该事务的执行速度。 2. 事务涉及多个表，较复杂，很可能引起死锁，造成大量事务回滚 这种情况也可以考虑一次性锁定事务涉及的表，从而避免死锁、减少数据库因事务回滚带来的开销；当然，应用中这两种事务不能太多，否则，就应该考虑使用ＭyISAＭ 四、乐观锁与悲观锁4.1 悲观锁 行锁、表锁、读锁、写锁都是在操作之前先上锁 悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。 流程：（1）在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。（2）其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 优点：悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。 缺点： 在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会； 在只读型事务处理中由于不会产生冲突，也没必要使用锁，这样做只能增加系统负载；还有会降低了并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数据。 4.2 乐观锁乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。如果系统并发量非常大，悲观锁会带来非常大的性能问题，选择使用乐观锁，现在大部分应用属于乐观锁。 版本控制机制 每一行数据多一个字段version，每次更新数据对应版本号+1。原理：读出数据，将版本号一同读出，之后更新，版本号+1，提交数据版本号大于数据库当前版本号，则予以更新，否则认为是过期数据，重新读取数据。 使用时间戳实现 每一行数据多一个字段time。原理：读出数据，将时间戳一同读出，之后更新，提交数据时间戳等于数据库当前时间戳，则予以更新，否则认为是过期数据，重新读取数据。 深入理解乐观锁与悲观锁 五、案例分析5.1 表锁案例分析5.1.1 加读锁lock table 表名 read; 当前session和其他session都可以查询该表记录； 当前session不能查询其他没有锁定的表，其他session可以查询或更新未锁定的表； 当前session插入或更新锁定的表会报错，其他session插入或更新锁定的表会一直等待获得锁。 直到释放锁unlock tables;之后，其他session才会获得锁，插入或更新操作完成。 5.1.2 加写锁lock table 表名 write; 当前session对锁定表的查询+插入+更新操作都可执行，其他session对锁定表的查询被阻塞，需要等待锁被释放。 直到释放锁unlock tables;之后，其他session才会获得锁，查询返回。 简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。（阻塞指的是阻塞其他session） 5.1.3 表锁分析查看哪些表被加锁了show open tables; 如何分析表锁定 可以通过检查table_lock_waited和table_locks_immediate状态变量来分析系统上的表锁定show status like 'table%'; 这里有两个状态变量记录MySQL内部表级锁定的情况，两个变量说明如下： Table_locks_immediate：产生表级锁定的次数，表示可以立即获取锁的查询次数，每立即获取锁值加1 Table_locks_waited：出现表级锁定争用而发生等待的次数（不能立即获取锁的次数，每等待一次锁值加1），此值高则说明存在着较严重的表级锁争用情况。 5.2 行锁案例分析/*step1：session1更新但不提交*/set autocommit=0;update user set salary=220 where id=1;/*step2：session2被阻塞，只能等待*/set autocommit=0;update user set salary=220 where id=1;..../*step3：session1提交更新*/commit;/*step4：session2解除阻塞，更新正常进行*/ 5.2.1 行锁分析通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况。 show status like 'innodb_row_lock%'; 对各个状态量的说明如下： Innodb_row_lock_current_waits: 当前正在等待锁定的数量 Innodb_row_lock_time: 从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg: 每次等待所花平均时间 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花时间 Innodb_row_lock_waits:系统启动后到现在总共等待的次数 对于这5个状态变量，比较重要的主要是： Innodb_row_lock_time_avg （等待平均时长） Innodb_row_lock_waits （等待总次数） Innodb_row_lock_time（等待总时长） 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。 5.2.2 优化建议 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁 合理设计索引，尽量缩小锁的范围 尽可能减少检索条件，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可能低级别事务隔离]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>锁机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL核心知识点——事务]]></title>
    <url>%2F2019%2F11%2F25%2FMySQL%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E7%82%B9%E2%80%94%E2%80%94%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[一、什么是事务？事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 二、事物的四大特性(ACID) 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 三、并发事务带来哪些问题？在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对统一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复度和幻读区别： 不可重复读的重点是修改，幻读的重点在于新增或者删除。 幻读错误的理解：说幻读是 事务A 执行两次 select 操作得到不同的数据集，即 select 1 得到 10 条记录，select 2 得到 11 条记录。这其实并不是幻读，这是不可重复读的一种，只会在 R-U R-C 级别下出现，而在 mysql 默认的 RR 隔离级别是不会出现的。 幻读的比较白话的理解： 幻读，并不是说两次读取获取的结果集不同，幻读侧重的方面是某一次的 select 操作得到的结果所表征的数据状态无法支撑后续的业务操作。更为具体一些：select 某记录是否存在，不存在，准备插入此记录，但执行 insert 时发现此记录已存在，无法插入，此时就发生了幻读。 不可重复读侧重表达 读-读，幻读则是说 读-写，用写来证实读的是鬼影。 关于幻读更加详细的说明 四、事务隔离级别有哪些？MySQL的默认隔离级别是？ READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 /*查看系统、当前会话隔离级别*/select @@global.transaction_isolation,@@transaction_isolation;/*或者如下方式*/show global variables like '%isolation%'; show session variables like '%isolation%';/*设置当前会话隔离级别*/set session transaction_isolation ='read-uncommitted'; /*设置系统隔离级别*/set global transaction_isolation ='read-uncommitted'; 这里有个注意点，关于幻读，在数据库规范里，RR 级别会导致幻读，但是，由于 Mysql 的优化，MySql 的 RR 级别不会导致幻读：在使用默认的 select 时，MySql 使用 MVCC 机制保证不会幻读；你也可以使用锁，在使用锁时，例如 for update（X 锁），lock in share mode（S 锁），MySql 会使用 Next-Key Lock 来保证不会发生幻读。前者称为快照读，后者称为当前读。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读）并不会有任何性能损失。 InnoDB 存储引擎在分布式事务的情况下一般会用到SERIALIZABLE(可串行化)隔离级别。 MVCC 介绍：全称多版本并发控制。 innoDB 每个聚集索引都有 4 个隐藏字段，分别是主键（RowID），最近更改的事务 ID（MVCC 核心），Undo Log 的指针（隔离核心），索引删除标记（当删除时，不会立即删除，而是打标记，然后异步删除）； 本质上，MVCC 就是用 Undo Log 链表实现。 MVCC 的实现方式：事务以排它锁的方式修改原始数据，把修改前的数据存放于 Undo Log，通过回滚指针与数据关联，如果修改成功，什么都不做，如果修改失败，则恢复 Undo Log 中的数据。 多说一句，通常我们认为 MVCC 是类似乐观锁的方式，即使用版本号，而实际上，innoDB 不是这么实现的。当然，这不影响我们使用 MySql。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习笔记]]></title>
    <url>%2F2019%2F11%2F25%2FMySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[/* 启动MySQL */net start mysql/* 连接与断开服务器 */mysql -h 地址 -P 端口 -u 用户名 -p 密码/* 跳过权限验证登录MySQL */mysqld --skip-grant-tables-- 修改root密码密码加密函数password()update mysql.user set password=password('root');SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 数据库操作-- 查看当前数据库 select database();-- 显示当前时间、用户名、数据库版本 select now(), user(), version();-- 创建库 create database[ if not exists] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 show databases[ like 'pattern']-- 查看当前库信息 show create database 数据库名-- 修改库的选项信息 alter database 库名 选项信息-- 删除库 drop database[ if exists] 数据库名 -- 同时删除该数据库相关的目录及其目录内容 表的操作-- 创建表 create [temporary] table[ if not exists] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 temporary 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT 'string']-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，.ibd表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 数据文件目录 DATA DIRECTORY = '目录' -- 索引文件目录 INDEX DIRECTORY = '目录' -- 表注释 COMMENT = 'string' -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE 'pattern'] SHOW TABLES FROM 表名-- 查看表结构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE 'PATTERN'] SHOW TABLE STATUS [FROM db_name] [LIKE 'pattern']\G-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构 ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段名 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作-- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件] 字符集编码-- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致 SHOW VARIABLES LIKE 'character_set_%' -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码 SET 变量名 = 变量值 set character_set_client = gbk; set character_set_results = gbk; set character_set_connection = gbk; SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE 'pattern']/SHOW CHARSET [LIKE 'pattern'] 查看所有字符集 SHOW COLLATION [LIKE 'pattern'] 查看所有校对集 charset 字符集编码 设置字符集编码 collate 校对集编码 设置校对集编码 数据类型（列类型）1. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数'123'，补填后为'00123' - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155 datetime “YYYY-MM-DD hh:mm:ss”timestamp “YY-MM-DD hh:mm:ss” “YYYYMMDDhhmmss” “YYMMDDhhmmss” YYYYMMDDhhmmss YYMMDDhhmmssdate “YYYY-MM-DD” “YY-MM-DD” “YYYYMMDD” “YYMMDD” YYYYMMDD YYMMDDtime “hh:mm:ss” “hhmmss” hhmmssyear “YYYY” “YY” YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...)create table tab ( gender set('男', '女', '无') );insert into tab values ('男, 女'); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。 选择类型-- PHP角度1. 功能满足2. 存储空间尽量小，处理效率更高3. 考虑兼容问题-- IP存储 ----------1. 只需存储，可用字符串2. 如果需计算，查找等，可存储为4个字节的无符号int，即unsigned 1) PHP函数转换 ip2long可转换为整型，但会出现携带符号问题。需格式化为无符号的整型。 利用sprintf函数格式化字符串 sprintf("%u", ip2long('192.168.3.134')); 然后用long2ip将整型转回IP字符串 2) MySQL函数转换(无符号整型，UNSIGNED) INET_ATON('127.0.0.1') 将IP转为整型 INET_NTOA(2130706433) 将整型转为IP``` ## 列属性（列约束）```sql1. 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. unique 唯一索引（唯一约束） 使得某字段的值也不能重复。 3. null 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, 'val'); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null 4. default 默认值属性 当前字段的默认值。 insert into tab values (default, 'val'); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp );5. auto_increment 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. comment 注释 例：create table tab ( id int ) comment '注释内容';7. foreign key 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范-- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表-- 1NF, 第一范式 字段不能再分，就满足第一范式。-- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。-- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。 selectselect [all|distinct] select_expr from -&gt; where -&gt; group by [合计函数] -&gt; having -&gt; order by -&gt; limita. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. from 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 select * from tb1 as tt, tb2 as bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 select * from tb1, tb2;c. where 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. group by 子句, 分组子句 group by 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 group by 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. having 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. order by 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. limit 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. distinct, all 选项 distinct 去除重复记录 默认为 all, 全部记录 UNION 将多个select查询的结果组合成一个结果集合。SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。 子查询 - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。 连接查询(join) 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发起连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id; 导入导出select * into outfile 文件地址 [控制格式] from 表名; -- 导出表数据load data [local] infile 文件地址 [replace|ignore] into table 表名 [控制格式]; -- 导入数据 生成的数据默认的分隔符是制表符 local未指定，则数据文件必须在服务器上 replace 和 ignore 关键词控制对现有的唯一键记录的重复的处理-- 控制格式fields 控制字段格式默认：fields terminated by '\t' enclosed by '' escaped by '\\' terminated by 'string' -- 终止 enclosed by 'char' -- 包裹 escaped by 'char' -- 转义 -- 示例： SELECT a,b,a+b INTO OUTFILE '/tmp/result.text' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"' LINES TERMINATED BY '\n' FROM test_table;lines 控制行格式默认：lines terminated by '\n' terminated by 'string' -- 终止 insertselect语句获得的数据可以用insert插入。可以省略对列的指定，要求 values () 括号内，提供给了按照列顺序出现的所有字段的值。 或者使用set语法。 insert into tbl_name set field=value,...；可以一次性使用多个值，采用(), (), ();的形式。 insert into tbl_name values (), (), ();可以在列值指定时，使用表达式。 insert into tbl_name values (field_value, 10+10, now());可以使用一个特殊值 default，表示该列使用默认值。 insert into tbl_name values (field_value, default);可以通过一个查询的结果，作为需要插入的值。 insert into tbl_name select ...;可以指定在插入的值出现主键（或唯一索引）冲突时，更新其他非主键列的信息。 insert into tbl_name values/set/select on duplicate key update 字段=值, …; deleteDELETE FROM tbl_name [WHERE where_definition] [ORDER BY ...] [LIMIT row_count]按照条件删除指定删除的最多记录数。Limit可以通过排序条件删除。order by + limit支持多表删除，使用类似连接语法。delete from 需要删除数据多表1，表2 using 表连接操作 条件。 truncateTRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区 备份与还原备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 -B 库名 &gt; 文件名(D:/a.sql)可以-w携带备份条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件 视图什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name -- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务(transaction)事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 锁表表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES 触发器 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1[, str2,...])-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert SQL编程--// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。 -- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date('yyyy-mm-dd hh:ii:ss'); -- 获取日期部分time('yyyy-mm-dd hh:ii:ss'); -- 获取时间部分date_format('yyyy-mm-dd hh:ii:ss', '%d %y %a %d %m %b %j'); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由"参数名"和"参数类型"组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE 'partten' SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。 存储过程存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END 用户和权限管理用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 'user_name'@'192.168.1.1' - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD('密码') -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD('密码') -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] 'password'] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限 表维护-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 杂项1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \'5. CMD命令行内的语句结束符可以为 ";", "\G", "\g"，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\c]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-Tornado框架总结]]></title>
    <url>%2F2019%2F09%2F19%2FPython-Tornado%E6%A1%86%E6%9E%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Tornado框架常用的包和模块# 异步HTTP服务器from tornado.ioloop import IOLoopfrom tornado import gen,web# 多进程服务器from tornado.httpserver import HTTPServer# 访问数据库from tornado_mysql import pools# 访问网路from tornado.httpclient import AsyncHTTPClient# 单元测试from tornado.testing import gen_test, AsyncTestCaseimport unittest# TCPServerfrom tornado import ioloop,gen,iostreamfrom tornado.tcpserver import TCPServer# TCPClientfrom tornado import ioloop,gen,iostreamfrom tornado.tcpclient import TCPClient# socket模块（套接字）import socket socket模块# 获取tcp/ip套接字sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM)# 服务端套接字函数sock.bind() #绑定(主机,端口号)到套接字sock.listen() #开始TCP监听sock.accept() #被动接受TCP客户的连接,(阻塞式)等待连接的到来# 客户端套接字函数sock.connect((hostname,port)) #主动初始化TCP服务器连接# 公共用途的套接字函数sock.recv(bufsize[,flag]) #接受套接字的数据。数据以字符串形式返回，bufsize指定最多可以接收的数量sock.send() #发送TCP数据(send在待发送数据量大于己端缓存区剩余空间时,数据丢失,不会发完)sock.close() #关闭套接字# 面向文件的套接字方法sock.fileno() #套接字的文件描述符 ioloop模块loop = ioloop.IOLoop.current() #创建一个IO循环的对象loop.start() #ioloop对象启动ioloop.IOLoop.current().run_sync(func) #run_sync 可以自动开启 ioloop 并在函数执行结束之后关闭 ioloop，此种调用在执行一次性操作时非常有用，比如要对数据库进行一次性修改 tornado.ioloop.IOLoop 提供了三个接口可以用于网络编程1.add_handler(fd, handler, events) add_handler用于添加socket到主循环中, 接受三个参数: fd 是socket的文件描述符；handler 是处理此socket的callback函数 ；events 是此socket注册的事件。 2.update_handler(fd, events) update_handler用于更新住循环中已存在的socket响应事件, 接受两个参数: fd 是socket对应的文件描述符；events 是注册的新事件。 3.remove_handler(fd) remove_handler用于移除主循环中已存在的socket。 tornado.ioloop.IOLoop的4种响应事件 事件 描述 tornado.ioloop.IOLoop.NONE 无事件 tornado.ioloop.IOLoop.READ 读事件 tornado.ioloop.IOLoop.WRITE 写事件 tornado.ioloop.IOLoop.ERROR 发生错误的事件]]></content>
      <categories>
        <category>Web后端</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tornado框架学习]]></title>
    <url>%2F2019%2F09%2F09%2FTornado%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[为什么用Tornado?异步编程原理服务器同时要对许多客户端提供服务，他的性能至关重要。而服务器端的处理流程，只要遇到了I/O操作，往往需要长时间的等待。 当然，我们可以用多线程/多进程达到类似的目的，但线程和进程都是系统控制的，消耗资源较多，而且何时运行，何时挂起不由程序本身做主，调度开销较大。我们希望将多任务流程的调度工作方法放到自己的代码里，精确的控制他的行踪，与线程相似，我们称这种任务流程为协程。协程实在一个线程之内，无需操作系统参与，由程序自身调度的执行单位。按照上述模式，在一个进程之内同时处理多个协程，充分利用CPU时间，就是我们需要的异步编程。 底层依赖epoll在Linux下，底层对一个耗时操作（如网络访问）的处理流程为：发起访问，将网络连接的文件描述符和期待事件注册到epoll里。当期待事件发生，epoll触发事件处理机制，通过回调函数通知tornado，tornado切换协程。 用生成器实现协程要实现上述异步的操作，必须判断两个点：1.等待开始—-协程走到这里，去epoll注册，将CPU的控制权让给别的协程。2.等待结束—-协程走到这里，接到epoll回调，开始请求CPU的控制权，执行后续操作。在Tornado 里，用一个yield语句来表示这两个点。程序开始等待时，向外yield一个Future对象，直到等待结束才回来执行yield的下一条语句。 Tornado异步HTTP服务器Tornado最大用途当然是HTTP服务器 异步HTTP服务器#http_0.pyfrom tornado.ioloop import IOLoopfrom tornado import gen,webclass ExampleHandler(web.RequestHandler): @gen.coroutine # 装饰器，作用就是把一个普通的函数变成一个返回Future对象的函数，即异步函数 def get(self): delay = self.get_argument('delay', 5) yield gen.sleep(int(delay)) # delay传入是几就延迟几秒，如果没有传入，默认值为5秒 self.write(&#123;"status":1, "msg":"success"&#125;) # 如果输出的是一个字典，tornado会自动把它变成JSON串 self.finish() # 正常结束的HTTP调用须以self.finish()结束。但如果是渲染模版或者跳转到其他网站，不应该添加self.finish() # @gen.coroutine # def post(self): # passapplication = web.Application([ (r"/example", ExampleHandler), #(r"/other", OtherHandler), ],autoreload = True) # 构造Application对象application.listen(8765) # 监听端口IOLoop.current().start() # 启动消息循环 运行以上代码，用浏览器访问http://localhost:8765/example?delay=1即可在延迟1秒之后得到返回结果{“status”: 1,“msg”: “success”}。delay传入是几就延迟几秒，如果没有传入，默认值为5。在这个例子里，我们用gen.sleep实现延迟。gen.sleep与time.sleep的用法相似，区别在于它是异步的，只阻塞当前协程，等待一段时间，但不影响同一进程内的其他协程的执行。可以认为gen.sleep是time.sleep的一个异步版本。在tornado开发中，我们经常需要使用某个内含等待的函数的异步版本，以免同步的等待阻塞住整个进程。这些异步的操作都返回Future对象。 检验他能否并发服务打开两个浏览器（不要使用同一个浏览器的两个标签页），各自在地址栏中输入http://localhost:8765/example?delay=10尽可能快的在两个浏览器里先后敲回车加载页面，你会发现，两个页面都是在你开始加载的10秒之后返回，两次加载的总用时是10秒稍多，远不到20秒，这说明我们的事例程序虽然只有一个进程，一个线程，却能在第一个请求完成之前开始处理第二个请求。 对代码的详细解说tornado与webapp一样，用一个继承于web.RequestHandler的类构造处理HTTP访问的handler，不同于Django，flask，bottle等用函数构造handler。在这个类里，我们重载get方法来处理GET请求，也可以重载post, put,delete等其他HTTP方法。你大概注意到了，重载的方法前都加了@gen.coroutine这样一个装饰器，他的作用就是把一个普通的函数变成一个返回Future对象的函数，即异步函数。异步函数一定要用yield调用，而且只有在另一个异步函数之内的调用才能起作用。用self.write输出执行结果。如果输出的是一个字典，tornado会自动把它变成JSON串。正常结束的HTTP调用须以self.finish()结束。但如果是渲染模版或者跳转到其他网站，不应该添加self.finish()。接下来要构造Application对象，然后监听端口，启动消息循环，服务器就能运行起来了。 创建Appliaction对象时可设置这些参数 autoreload:若设为True，在程序运行起来之后，每次编辑代码并保存时，可以自动重新运行 debug:若设制为true，会把运行出错信息打印在屏幕上。 cookie_secret:用于cookie加密的密钥，形如：“ofjf939.m%dw$#3fdn923hrfsp309-[2”。 static_path:静态文件路径。如果与当前文件同一路径，可设为：os.path.dirname(file). xsrf_cookies:xsrf检测 多进程服务器前面讨论过，一个典型的单进程，单线程。需访问数据库的异步Tornado服务器每秒可以响应500个请求，在较低配置服务器上实际的负载能力大概也是这个数字。如果需要更高的负载能力，且服务器有多个CPU，应开启多个服务进程。只要将代码改成如下： #http_0.pyfrom tornado.ioloop import IOLoopfrom tornado import gen,webfrom tornado.httpserver import HTTPServerclass ExampleHandler(web.RequestHandler): @gen.coroutine # 装饰器，作用就是把一个普通的函数变成一个返回Future对象的函数，即异步函数 def get(self): delay = self.get_argument('delay', 5) yield gen.sleep(int(delay)) self.write(&#123;"status":1, "msg":"success"&#125;) # 如果输出的是一个字典，tornado会自动把它变成JSON串 self.finish() # 正常结束的HTTP调用须以self.finish()结束。但如果是渲染模版或者跳转到其他网站，不应该添加self.finish() # @gen.coroutine # def post(self): # passapplication = web.Application([ (r"/example", ExampleHandler), #(r"/other", OtherHandler), ],autoreload = True) # 构造Application对象server = HTTPServer(application)server.bind(8765)server.start(4)IOLoop.current().start() # 启动消息循环 就能同时启动4个进程了。理论上讲，在一个线程里也能运行多个协程，可以作出「多进程 × 多线程 × 多协程」的模式。而实际上，协程可以完全代替线程，「多进程 × 多协程」已经能够充分利用服务器的硬件资源。 流式响应的HTTP服务器如果响应数据较大，为了节约内存，或者是各部分数据的返回要有一个时间差，我们需要将数据分成多次发送。 #http_stream.pyfrom tornado.ioloop import IOLoopfrom tornado import gen,webclass ExampleHandler(web.RequestHandler): @gen.coroutine def get(self): for _ in range(5): yield gen.sleep(1) self.write('zzzzzzzzzzzz&amp;lt;br&amp;gt;') # &amp;lt;表示&lt;，&amp;gt;表示&gt; self.flush() # 作用是将此前由self.write()写入缓冲区的内容发送出去 self.finish()application = web.Application([ (r"/example", ExampleHandler), ],autoreload = True)application.listen(8765)IOLoop.current().start() 演示及解说与前面的http_0.py相比，明显多了一行self.flush().他的作用是将此前由self.write()写入缓冲区的内容发送出去。在self.finish()结束这次响应之前，可以多次调用self.write和self.flush,逐步发送数据。运行这段代码，用浏览器访问http://localhost:8765/example可以看到页面每隔一秒打印一行输出。如果注释掉self.flush()在运行，就会等到5秒时候才将5行输出同时打印出来。 访问数据库#http_db.pyfrom tornado.ioloop import IOLoopfrom tornado import gen,webfrom tornado_mysql import poolsconnParam = &#123;'host':'localhost', 'port':3306, 'user':'root', 'passwd':'root', 'db':'test'&#125;class GetUserHandler(web.RequestHandler): POOL = pools.Pool( connParam, max_idle_connections=1, max_recycle_sec=3, ) @gen.coroutine def get(self): userid = self.get_argument('id') cursor = yield self.POOL.execute('select name from user where id = %s', userid) if cursor.rowcount &gt; 0: self.write( &#123; "status": 1, "name": cursor.fetchone()[0] &#125; ) else: self.write( &#123; "status": 0, "name": "" &#125; ) self.finish()application = web.Application([ (r"/getuser", GetUserHandler), ],autoreload = True)application.listen(8765)IOLoop.current().start() 以上代码从mysql库中读取数据。 演示首先，安装tornado-MySQL然后，在你的mysql中建立一个名为user的表，至少有两个字段：一个整数型的id,和一个字符串型的name。编辑http_db.py，将mysql的连接参数填入connParam中。运行http_db.py，用浏览器访问：http://localhost:8765/getuser?id=1如果你的user表中有id=1的数据，他的name字段是jim,你将看到返回值：{“status”: 1,“name”: “jim”}如果没有找到数据，你将看到返回值：{“status”: 0,“name”: “”} 需要ORM？很多人喜欢通过ORM访问数据库，而Tornado没有提供异步的ORM工具。这是访问mysql的例子，如果你要使用postgresql或其他数据库，也需要先安装这些数据库的tornado接口，就像tornado_mysql一样。 访问网路#http_req.pyfrom tornado.ioloop import IOLoopfrom tornado import gen,webfrom tornado.httpclient import AsyncHTTPClienturl = 'http://hq.sinajs.cn/list=sz000001'#url = 'https://baidu.com'class GetPageHandler(web.RequestHandler): @gen.coroutine def get(self): client = AsyncHTTPClient() response = yield client.fetch(url), method='GET') self.write(response.body.decode('gbk')) self.finish()application = web.Application([ (r"/getpage", GetPageHandler), ],autoreload = True)application.listen(8765)IOLoop.current().start()"""此程序有可能会报AsyncHTTPClient访问超时问题，经检验，为网络问题""" 演示运行http_req.py，用浏览器访问：http://localhost:8765/getpage,即可看到从http://hq.sinajs.cn/list=sz000001抓取的内容。 解说Tornado有一个AsyncHTTPClient用于访问其他网页，用法比较简单。他在fetch方法中等待远端网页返回内容，因此也要以yield调用。 tornado用户认证# http_auth.pyfrom tornado.ioloop import IOLoopfrom tornado import gen, webclass LoginHandler(web.RequestHandler): @gen.coroutine def get(self): self.set_secure_cookie('username', 'Jim') self.write('login ok.') self.finish()class LogoutHandler(web.RequestHandler): @gen.coroutine def get(self): self.clear_cookie('username') self.write('logout ok.') self.finish()class WhoHandler(web.RequestHandler): def get_current_user(self): if self.get_secure_cookie('username'): return str(self.get_secure_cookie('username'), encoding="utf-8") else: return 'unknown' @gen.coroutine def get(self): self.write('you are ' + self.current_user) self.finish()application = web.Application([ (r"/login", LoginHandler), (r"/logout", LogoutHandler), (r"/whoami", WhoHandler),], autoreload=True, cookie_secret="feljjfesrh48thfe2qrf3np2zl90bmw")application.listen(8765)IOLoop.current().start() 演示1.运行http_auth.py，用浏览器访问http://localhost:8765/whoami，可以看到输出：you are unknown,说明还没有登录2.访问http://localhost:8765/login，看到：login ok.登录成功。在访问http://localhost:8765/whoami，这是会看到： you are Jim，说明你已经以Jim身份登录了。3.访问http://localhost:8765/logout，看到：logout ok，退出登录，在访问http://localhost:8765/whoami，又变回了：you are unknown,说明你已经退出了登录。 解说你知道服务端通常用 cookie 保存用户的身份信息。login 时创建 cookie；logout 时清除 cookie；需要检查用户身份时，读取 cookie。在 tornado 里，创建 cookie 通常用 set_secure_cookie，这样创建的 cookie 是加密的。与之对应的读取加密 cookie 的方法是 get_secure_cookie。为了给 cookie 加密，要在创建 Application 时添加 cookie_secret 属性，这是加密的密钥，它的值用一串乱写的字符就行了。清除 cookie 用 clear_cookie。在 RequestHandler 里有一个 get_current_user 方法，它会在 get / post 之前调用，其返回值会赋予 current_user 属性。我们在 WhoHandler 里重载了 get_current_user，后面在 get 里就能直接使用 self.current_user 了。 tornado定时任务定时任务分两种，一种是每隔一定的时间周期性地执行，另一种是在某个钟点单次执行。 周期性定时任务#cron_0.pyfrom tornado import ioloop, gen@gen.coroutinedef Count(): print("1 second has gone.")if __name__ == '__main__': ioloop.PeriodicCallback(Count, 1000).start() ioloop.IOLoop.current().start() 演示及解说在启动消息循环之前，用PeriodicCallback设定每1000毫秒执行一次异步函数Count。直接运行，每过一秒会打印一行：1 second has gone. 单次定时任务#cron_1.pyfrom tornado import ioloop, genfrom time import time@gen.coroutinedef Ring(): print('it\'s time to get up')if __name__ == '__main__': loop = ioloop.IOLoop.current() loop.call_at(time() + 5, Ring) loop.start() 演示及解说在启动消息循环之前，用call_at设定5秒后执行一次异步函数Ring。如果想在明早9点执行，需要输入明早9点的unix时间戳。直接运行上述代码，5秒之后会打印一行：it’s time to get up.仅此一次。如果要在一个相对的时间（例如五秒钟后）而不是一个绝对时间（例如八点整）运行定时任务，用 call_later 会比 call_at 更简单一点。可以将 loop.call_at(time() + 5, Ring) 改为 loop.call_later(5,Ring) 执行效果是一样的。 单元测试#test.pyfrom tornado.testing import gen_test, AsyncTestCasefrom tornado.httpclient import AsyncHTTPClientimport unittestclass MyAsyncTest(AsyncTestCase): @gen_test def test_xx(self): client = AsyncHTTPClient(self.io_loop) path = 'http://localhost:8765/example?delay=2' responses = yield [client.fetch(path, method = 'GET') for _ in range(10)] for response in responses: print(response.body)if __name__ == '__main__': unittest.main() 演示1.先运行http_0.py，保证通过浏览器访问http://localhost:8765/example?delay=2能得到结果2.运行test.py，看到如下输出 可见，测试模块在2.027s之内完成了10次请求，每个请求都要延迟2秒返回，因此，这10个请求必定是并行执行的。 解说与普通的 unittest 用法相似，先定义好基于 AsyncTestCase 的测试类，在执行 unittest.main()时，测试类中所有 以 test 开头的方法都会执行。注意，测试类中的方法要以 gen_test 装饰。每次测试整体的用时不能超过 5 秒，超则报错。 tornado异步TCP连接tornado有TCPClient和TCPServer两个类，可用于实现tcp的客户端和服务端。事实上，这两个类都是对iostream的简单包装。 真正重要的是iostreamiostream是client与server之间的tcp通道，被动等待创建iostream的一方是server，主动找对方创建的一方是client。在iostream创建之后，client与server的操作再无分别，在任何时候都可以通过iostream.write向对方传送内容，或者通过iostream.read_xx接受对方传来的内容，或者以iostream.close关闭连接。 TCPServer#tcp_server.pyfrom tornado import ioloop,gen,iostreamfrom tornado.tcpserver import TCPServerclass MyTcpServer(TCPServer): @gen.coroutine def handle_stream(self,stream,address): try: while True: msg = yield stream.read_bytes(20, partial = True) print(msg, 'from', address) yield gen.sleep(0.005) # 延时5ms yield stream.write(msg[::-1]) if msg == 'over': stream.close() except iostream.StreamClosedError: passif __name__ == '__main__': server = MyTcpServer() server.listen(8760) server.start() ioloop.IOLoop.current().start() 解说创建一个继承于TCPServer的类的实例，监听端口，启动服务器，启动消息循环，服务器开始运行。这时，如果有client连接过来，tornado会创建一个iostream,然后调用handle_stream方法，调用时传入的两个参数是iostream和client的地址。我们示例的功能很简单，每收到一段 20 字符以内的内容，将之反序回传，如果收到 ‘over’，就断开连接。注意，断开连接不用 yield 调用。无论是谁断开连接，连接双方都会各自触发一个 StreamClosedError。 TCPClient#tcp_client.pyfrom tornado import ioloop,gen,iostreamfrom tornado.tcpclient import TCPClient@gen.coroutinedef Trans(): straem = yield TCPClient().connect('localhost', 8760) try: for msg in ('zzxxc', 'abcde', 'i feel lucky', 'over'): yield straem.write(msg.encode('utf-8')) back = yield straem.read_bytes(20, partial = True) print(back) except iostream.StreamClosedError: passif __name__ == '__main__': ioloop.IOLoop.current().run_sync(Trans) 解说使用TCPClient比TCPServer更简单，无需继承，只要用connect方法连接到server，就会返回iostream对象了。在本例中，我们像server发送一些字符串，他都会反序发回来，最后发个“over”，让sever断开连接。值得注意，这段代码与之前的几个例子有个根本的区别，之前都是服务器，被动等待行为发生，而这段代码是一运行就主动发起行为（连接），因此它的运行方式不同于以往，需要我们主动通过 ioloop 的 run_sync 来调用。以往那些实例中的异步处理方法实际是由 Tornado 调用的。在 run_sync 里，tornado 会先启动消息循环，执行目标函数，之后再结束消息循环。 演示在第一个终端端口运行tcp_server.py,在第二个终端端口运行tcp_client.py，即可看到他们之间的交互和断开的过程。 tornado的ioloop消息循环tornado的异步功能都是通过ioloop实现的。前面的每一段示例代码的最后一行都是启动ioloop: ioloop.IOLoop.current().start() 每个进程都有一个磨人的ioloop，虽然还可以有更多个，通常使用默认的就够了。在上面的这行代码里，我们通过current()获得当前的ioloop，让他start(),ioloop就会一直跑下去，让他run_sync，就会跑起来，执行目标函数，执行完就停止。一般编程中很少用到ioloop的其他功能，只要简简单单的处理好RequestHandler的get/post方法，调用tornado-Mysql的异步函数访问数据库，返回结果就可以了。可是，如果有些内含等待（CPU休息）的操作，找不到现成的异步库，只要深入到这个操作的底层，靠ioloop，可以把同步的操作变成异步。 把同步的操作变成异步内含等待的操作大概有几种情况：1.为了拖时间而等待对应前面用过的 gen.sleep，如果没有 gen.sleep 呢？可以很简单地通过也在前面用过的 call_at 或 call_later 来实现这个功能。2.等待网络返回这一部分是本章的重点。我们将不再使用 iostream，而是在同步代码的基础上，用 ioloop 自己实现一个与 iostream 同样异步、高效的 tcp client。 同步的代码#sync_client.pyimport socketfrom time import timesock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)sock.connect(('localhost', 8760))t0 = time()for _ in range(1000): sock.send(b'test message.') sock.recv(99)print('time cost', time() - t0)sock.close() 演示及解说这是一段十分普通的tcp client代码，当然它是同步的，连上服务器之后，发一条，收一条，重复一千次，看他耗时多少。服务器端就用我们前面用过的tcp_server.py，在运行sync_client.py,转瞬之间就结束了，输出：time cost 0.15320992469787598这么快，还需要异步吗？慢着，快是因为我们服务端的处理极其简单，又放在本地，实际的情况复杂得多。为了模拟真实场景，我们在服务端加一个只有 5ms 的时延。在 yield stream.write(msg[::-1]) 之前加上 yield gen.sleep( 0.005 ) 加上时延之后重新启动 tcp_server.py，再运行 sync_client.py，这回就慢多了。输出：time cost 6.7937400341033936由于时延，client 每次发出消息之后要等 5 ms 以上才能收到回复，时间浪费在 recv 里，一千次当然要五秒多。我们的目标就是通过异步编程优化等待的开销。 再看看异步的代码#async_client.pyimport socketfrom time import timefrom tornado import iolooploop = ioloop.IOLoop.current()socks = [socket.socket(socket.AF_INET, socket.SOCK_STREAM) for _ in range(50)][sock.connect(('localhost', 8760)) for sock in socks]SockD = &#123;sock.fileno(): sock for sock in socks&#125;t0 = time()n = 0def OnEvent(fd, event): if event == loop.WRITE: loop.update_handler(fd, loop.READ) elif event == loop.READ: sock = SockD[fd] sock.recv(99) global n n += 1 if n &gt;= 1000: print('time cost ', time() - t0) sock.close() loop.remove_handler(fd) loop.stop() return loop.update_handler(fd, loop.WRITE) sock.send(b'test message.')for fd, sock in SockD.items(): loop.add_handler(fd, OnEvent, loop.WRITE) sock.send(b'test message.')loop.start() 演示及解说启动加了时延的tcp_server.py，在运行aync_client.py，瞬间结束，输出：time cost 0.27816009521484375比同步的快四，五十倍。为什么这么快？因为现在的recv虽然写法与同步版本一样，调用的时机已经不同。同步的版本，send之后紧接着调用recv，却不知道数据多久才能返回，从调用recv到获得数据之间只能等待。而现在的异步版本，send完成时只是注册了一个读事件，直到真有数据到来时才调用recv，于是recv不用等待，时间就节省下来了。节省下来的时间给了别的协程。可以看到，我们创建了50个连接来完成这一千次收发，每个连接一个协程，send之后数据未来之际，别的协程可以发送自己的数据。 程序运行步骤1.创建50个socket对象并全部连接到服务器。2.为这些socket对象创立文件描述符的索引。后面，在回调函数里，我们要通过文件描述符获取socket对象3.对每个socket对象注册一个WRITE事件的回调函数，之后发送第一条消息。消息发送完成时，WRITE事件发生，触发i oloop执行刚刚注册的回调函数，传入文件描述符和触发回调的事件，这次的事件是WRITE。接下来我们要等待服务器端返回的消息，因此将注册等待的事件改为READ。4.对于每个等待READ事件的socket对象，一旦有数据来，又以事件READ触发回调，我们检查一下n的值，未满1000就发下一条数据。发之前要再把注册等待的事件改为WRITE。5.重复3，4两步，直到发满一千条，取消事件注册，结束。 我们用了 ioloop 的三个方法来实现消息注册与回调：add_handler：注册一个回调函数到一个文件描述符的指定事件上；update_handler：改变 add_handler 注册的事件，文件描述符和回调函数不变；remove_handler：取消事件注册。]]></content>
      <categories>
        <category>Web后端</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用算法总结]]></title>
    <url>%2F2019%2F06%2F14%2F%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[二分查找 给定一个数组（有序列表）和一个数字，要求查出该数字所在的索引位置。 def binary_search(list, item): # lowh和high用于跟踪要在其中查找的列表部分 low = 0 high = len(list) - 1 # 只要范围没有缩小到只包含一个元素，就检查中间的元素 while low &lt;= high: mid = (low + high) // 2 guess = list[mid] # 找到了元素 if guess == item: return mid # 猜的数字大了 if guess &gt; item: high = mid - 1 # 猜的数字小了 else: low = mid + 1 # 没有指定的元素 return None# 测试my_list = [1, 3, 5, 7, 9]print(binary_search(my_list, 3)) # =&gt; 1print(binary_search(my_list, -1)) # =&gt; None 总结： 二分查找是对数时间，时间复杂度为O(logn) 简单查找是线性时间，时间复杂度为O(n) O(logn)比O(n)快。需要搜索的元素越多，前者比后者就快越多 算法运行时间并不以秒为单位 算法运行时间是从其增速的角度度量的 选择排序 将数组元素按从小到大的顺序排列。 # 找出数组中的最小元素def findSmallest(arr): # 存储最小的值 smallest = arr[0] # 存储最小元素的索引 smallest_index = 0 for i in range(1, len(arr)): if arr[i] &lt; smallest: smallest_index = i smallest = arr[i] return smallest_index# 对数组进行排序def selectionSort(arr): newArr = [] for i in range(len(arr)): # 找出数组中最小元素的索引，并将其加入到新数组中 smallest_index = findSmallest(arr) newArr.append(arr.pop(smallest_index)) # 原数组arr不断的删除最小的元素 return newArrprint(selectionSort([5, 3, 6, 2, 10])) 选择排序，每检查一次数组，找出最小元素，运行时间都为O(n)，而这个操作需要执行n次，因此其时间复杂度为O(n2)（其中的常数1/2要省略） 总结： 需要存储多个元素时，可使用数组或链表 数组的元素都在一起，链表的元素是分开的，其中每个元素都存储了下一个元素的地址 数组的读取速度很快，链表的插入和删除速度很快 递归基线条件和递归条件def countdown(i): # 基线条件 if i &lt;= 0: return 0 # 递归条件 else: print(i) return countdown(i-1)countdown(5) 调用栈（call stack）def greet2(name): print("how are you, ", name, "?") def bye(): print("ok bye!") def greet(name): print("hello, ", name, "!") greet2(name) print("getting ready to say bye...") bye() greet("adit") 递归调用栈def fact(x): if x == 1: return 1 else: return x * fact(x-1) print(fact(5)) 说明： 每个fact函数调用都有自己的x变量，但是不能访问其他函数调用的变量x 最后一次被调用的函数先返回，然后接着返回之前的调用 总结： 递归指的是调用自己的函数 每个递归函数都有两个条件：基线条件和递归条件 栈有两种操作：压入和弹出 所有函数调用都进入调用栈 调用栈可能很长，这将占用大量的内存 快速排序分治算法（D&amp;C算法）# 循环求和def sum(arr): total = 0 for x in arr: total += x return totalprint(sum([1, 2, 3, 4])) # 递归求和def sum(list): if list == []: return 0 return list[0] + sum(list[1:])print(sum([1, 2, 3])) # 递归计算列表包含的元素数def count(list): if list == []: return 0 return 1 + count(list[1:])print(count([1, 2, 3])) # 递归找出列表中最大的数字def max_(lst): if len(lst) == 0: return 0 if len(lst) == 1: return lst[0] else: sub_max = max_(lst[1:]) return lst[0] if lst[0] &gt; sub_max else sub_max print(max_([1, 2, 5, 3])) 快速排序def quicksort(array): # 基线条件：为空或只包含一个元素的数组是“有序”的 if len(array) &lt; 2: return array else: # 递归条件 pivot = array[0] # 由所有小于基准值的元素组成的子数组 less = [i for i in array[1:] if i &lt;= pivot] # 由所有大于基准值的元素组成的子数组 greater = [i for i in array[1:] if i &gt; pivot] return quicksort(less) + [pivot] + quicksort(greater) print(quicksort([10, 5, 2, 3])) 总结： D&amp;C将问题逐步分解。使用D&amp;C处理列表时，基线条件很可能是空数组或只包含一个元素的数组 实现快速排序时，请随机地选择用作基准值的元素。快速排序的平均运行时间为O(n log n) 大O表示法中的常量有时候事关重大，这就是快速排序比合并排序快的原因所在 比较简单查找和二分查找时，常量几乎无关紧要，因为列表很长时，O(log n)的速度比O(n)快得多 散列表# 散列表book = &#123;"apple": 0.67, "milk": 1.49, "avocado": 1.49&#125;print(book)print(book["apple"]) # 散列表防止重复voted = &#123;&#125;def check_voter(name): if voted.get(name): print("kick them out!") else: voted[name] = True print("let them vote!") check_voter("tom")check_voter("mike")check_voter("mike") 总结： 散列表的查找、插入和删除速度都非常快 散列表适合用于模拟映射关系 散列表可用于缓存数据（例如，在Web服务器上） 散列表非常适合用于防止重复 广度优先搜索广度优先搜索解决了两类问题： 第一类问题：从节点A出发，有前往节点B的路径吗？ 第二类问题：从节点A出发，前往节点B的哪条路径最短？ from collections import dequedef person_is_seller(name): return name[-1] == 'm'graph = &#123;&#125;graph["you"] = ["alice", "bob", "claire"]graph["bob"] = ["anuj", "peggy"]graph["alice"] = ["peggy"]graph["claire"] = ["thom", "jonny"]graph["anuj"] = []graph["peggy"] = []graph["thom"] = []graph["jonny"] = []def search(name): # 创建一个队列 search_queue = deque() # 将你的邻居都加入到这个搜索队列中 search_queue += graph[name] # 这个数组用于记录检查过的人 searched = [] while search_queue: # 只要队列不为空，就取出其中的第一个人，并从队列中移除 person = search_queue.popleft() # 仅当这个人没检查过时才检查 if person not in searched: # 检查这个人是否是芒果销售商 if person_is_seller(person): print(person + " is a mango seller!") return True else: # 不是芒果销售商。将这个人的朋友都加入搜索队列 search_queue += graph[person] # 将这个人标记为检查过 searched.append(person) return Falsesearch("you") 总结： 广度优先搜索指出是否有从A到B的路径，如果有，广度优先搜索将找出最短路径 面临类似于寻找最短路径的问题时，可尝试使用图来建立模型，再使用广度优先搜索来解决问题 有向图中的边为箭头，箭头的方向指定了关系的方向，例如，rama→adit表示rama欠adit钱 无向图中的边不带箭头，其中的关系是双向的，例如，ross - rachel表示“ross与rachel约会，而rachel也与ross约会” 队列是先进先出（FIFO）的，栈是后进先出（LIFO）的 你需要按加入顺序检查搜索列表中的人，否则找到的就不是最短路径，因此搜索列表必须是队列 对于检查过的人，务必不要再去检查，否则可能导致无限循环 狄克斯特拉算法对比广度优先搜索，狄克斯特拉算法采用了“加权图”的概念。 # the graphgraph = &#123;&#125;graph["start"] = &#123;&#125;graph["start"]["a"] = 6graph["start"]["b"] = 2graph["a"] = &#123;&#125;graph["a"]["fin"] = 1graph["b"] = &#123;&#125;graph["b"]["a"] = 3graph["b"]["fin"] = 5graph["fin"] = &#123;&#125;# the costs tableinfinity = float("inf")costs = &#123;&#125;costs["a"] = 6costs["b"] = 2costs["fin"] = infinity# the parents tableparents = &#123;&#125;parents["a"] = "start"parents["b"] = "start"parents["fin"] = Noneprocessed = []def find_lowest_cost_node(costs): lowest_cost = float("inf") lowest_cost_node = None # 遍历所有的节点 for node in costs: cost = costs[node] # 如果当前节点的开销更低且未处理过 if cost &lt; lowest_cost and node not in processed: # 就将其视为开销最低的节点 lowest_cost = cost lowest_cost_node = node return lowest_cost_node# 在未处理的节点中找出开销最小的节点node = find_lowest_cost_node(costs)# 这个while循环在所有节点都被处理过后结束while node is not None: cost = costs[node] neighbors = graph[node] # 遍历当前节点的所有邻居 for n in neighbors.keys(): new_cost = cost + neighbors[n] # 如果经当前节点前往该邻居更近 if costs[n] &gt; new_cost: # 就更新该邻居的开销 costs[n] = new_cost # 同时将该邻居的父节点设置为当前节点 parents[n] = node # 将当前节点标记为处理过 processed.append(node) # 找出接下来要处理的节点，并循环 node = find_lowest_cost_node(costs) print("cost from the start to each node:")print(costs) 总结： 广度优先搜索用于在非加权图中查找最短路径 狄克斯特拉算法用于在加权图中查找最短路径 仅当权重为正时狄克斯特拉算法才管用 如果图中包含负权边，请使用贝尔曼福德算法 贪婪算法# 需要覆盖的州states_needed = set(["mt", "wa", "or", "id", "nv", "ut", "ca", "az"])# 广播台覆盖的州stations = &#123;&#125;stations["kone"] = set(["id", "nv", "ut"])stations["ktwo"] = set(["wa", "id", "mt"])stations["kthree"] = set(["or", "nv", "ca"])stations["kfour"] = set(["nv", "ut"])stations["kfive"] = set(["ca", "az"])final_stations = set()while states_needed: # 覆盖了最多的未覆盖州的广播台 best_station = None # 包含该广播台覆盖的所有未覆盖的州 states_covered= set() for station, states_for_station in stations.items(): # 计算交集，同时出现在states_needed和states_for_station中的州 covered = states_needed &amp; states_for_station # 该广播台覆盖的州比best_station多 if len(covered) &gt; len(states_covered): best_station = station states_covered = covered states_needed -= states_covered final_stations.add(best_station) print(final_stations) 总结： 贪婪算法寻找局部最优解，企图以这种方式获得全局最优解 对于NP完全问题，还没有找到快速解决方案 面临NP完全问题时，最佳的做法是使用近似算法 贪婪算法易于实现、运行速度快，是不错的近似算法 动态规划# 最长公共子串# 两个字母相同if word_a[i] == word_b[j]: # 将当前单元格的值设置为左上方单元格的值加1 cell[i][j] = cell[i-1][j-1] + 1# 两个字母不同else: # 值为0 cell[i][j] = 0 # 最长公共子序列# 两个字母相同if word_a[i] == word_b[j]: # 将当前单元格的值设置为左上方单元格的值加1 cell[i][j] = cell[i-1][j-1] + 1# 两个字母不同else: # 选择上方和左方邻居中较大的那个 cell[i][j] = max(cell[i-1][j], cell[i][j-1]) 总结： 需要在给定约束条件下优化某种指标时，动态规划很有用 问题可分解为离散子问题时，可使用动态规划来解决 每种动态规划解决方案都涉及网格 单元格中的值通常就是你要优化的值 每个单元格都是一个子问题，因此你需要考虑如何将问题分解为子问题 没有放之四海皆准的计算动态规划解决方案的公式 K最近邻算法总结： KNN用于分类和回归，需要考虑最近的邻居 分类就是编组 回归就是预测结果（如数字） 特征抽取意味着将物品（如水果或用户）转换为一系列可比较的数字 能否挑选合适的特征事关KNN算法的成败]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop知识点总结]]></title>
    <url>%2F2019%2F04%2F30%2FHadoop%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[HDFS 和YARN 的基本概念HDFS分布式文件系统，主/从架构 NameNode：负责管理元数据（文件名称，副本数量，文件位置，块大小）。HDFS 存储是以块存储 默认块大小 128MB，hadoop1 中默认的块大小是64MB。一个节点。 DataNode：主要存储真正的数据，多节点。 secondaryNamenode：辅助节点，用于合并两类文件。 Fsimage, edits：作为元数据的镜像和操作的日志记录。 在HDFS第一次使用的时候需要对其进行格式化，目的是生成fsimage 和 edits 文件。 YARN资源管理器：YARN为这些运行在操作系统上的任务分配资源，管理。 主节点（resourceManager）：负责全部集群中的资源管理，和任务分配 从节点（nodeManager）：负责每个机器上的资源管理 HDFS的优缺点HDFS优点 高容错性：数据自动保存多个副本，副本丢失后，会自动恢复。 适合批处理：移动计算而非数据、数据位置暴露给计算框架。 适合大数据处理：GB、TB、甚至PB级数据、百万规模以上的文件数量，1000以上节点规模。 流式文件访问：一次性写入，多次读取；保证数据一致性。 可构建在廉价机器上：通过多副本提高可靠性，提供了容错和恢复机制。 HDFS缺点 低延迟数据访问：比如毫秒级、低延迟与高吞吐率。 小文件存取：占用NameNode大量内存，寻道时间超过读取时间。 并发写入、文件随机修改：一个文件只能有一个写者，仅支持append HDFS的副本复制策略 HDFS读写数据流程HDFS读数据流程 客户端通过向namenode请求下载文件 ，namenode 收到请求之后查询元数据信息,找到datanode数据块的信息。 客户端挑选一台就近的datanode,进行请求数据。 datanode开始传输数据给客户端，是以packet 为单位进行读取。 客户端 接收packet 数据,先在本地缓存，最后写入到目标文件。 HDFS写数据流程 客户端向namenode请求上传文件，namenode检测该文件是否已存在，父目录是否存在，然后返回是否可以上传。 客户端请求上传第一个block，namenode返回三个节点（dn1,dn2,dn3）。 客户端向dn1请求上传数据，dn1收到请求后会调用dn2，dn2调用dn3，建立传输通道，dn1、dn2、dn3逐级应答。 客户端开始往dn1上传第一个block（先从磁盘读取放到一个本地内存缓存），以packet为单位。dn1收到一个block就会传给dn2，dn2传给dn3。dn1每传完一个packet会被放入一个应答队列等待应答。 当一个block传输完成之后，客户端再次向namenode请求上传第二、第三个block，重复上面的步骤（2-4步），直至文件上传完成。 NameNode与DataNode的工作机制NameNode工作机制 第一次启动:第一次启动都需要格式化nameNode ,创建fsimage,edits. 第一次启动只需要加载fsiamge。 如果不是第一次启动： 直接加载edits ,fsimage镜像文件 ，合并成一个新的fsimage 文件，再创建edits 文件记录新的操作行为。 启动的过程中，会存在30秒钟等待时间 ，这个等待的时间就是安全模式。 DataNode工作机制 一个数据块在datanode上是以文件形式存储在磁盘上的，包括了两个文件，一个数据本身，一个是元数据包 包括数据块的长度，,数据块的校验和,由于HDFS上的数据是不允许被重复上传的所以在上传之前会对上传的数据进行检查 ,时间戳。 DataNode启动后会向nameNode进行注册，通过后，会周期性的向namenode上报自己的datanode上的块信息。 心跳报告，每3秒钟向nameNode进行汇报,心跳的返回结果中带有NameNode 带给该datanode复制数据块，移动数据块的命令, 如果说超过了10分钟datanode没有响应 ，则就会认为这个datanode节点不可用,会选择其他的机器。 如果NameNode意外终止，secondaryNameNode的工作是什么？它是如何工作的？ 并非NameNode的热备； 辅助NameNode，分担其工作量； 定期合并fsimage和edits，推送给NameNode； 在紧急情况下，可辅助恢复NameNode。 HDFS安全模式在系统的正常操作期间，namenode会在内存中保留所有块位置的映射信息。在安全模式下，各个datanode会向namenode发送最新的块列表信息，namenode了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”，namenode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以namenode不会进入安全模式。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase知识点总结]]></title>
    <url>%2F2019%2F04%2F30%2FHBase%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[HBase的架构 HMaster： 负责HBase中table和region的管理，regionserver的负载均衡，region分布调整，region分裂以及分裂后的region分配，regionserver失效后的region迁移等。 Zookeeper： 存储root表的地址和master地址，regionserver主动向zookeeper注册，使得master可随时感知各regionserver的健康状态。避免master单点故障。 RegionServer： HRegion Server主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。 HRegion Server内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，Region中由多个Store组成。每个Store对应了Table中的一个Column Family的存储，即一个Store管理一个region上的一个列簇。每个Store包含一个MemStore和0到多个StoreFile。Store是HBase存储核心，由MemStore和StoreFiles组成。 MemStore： MemStore是Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。 HBase中的rowkey以及热点问题HBase热点现象：检索hbase的记录首先要通过rowkey来定义数据行，当大量的client访问hbase集群的一个或少数几个节点，造成少数region server的读/写请求过多，负载过大，最终导致单个主机负载过大，引来性能下降甚至region不可用。 热点产生原因：有大量连续编号的rowkey，导致大量记录集中在个别region。 避免热点方法： 加盐：在rowkey的前面增加随机数。此方式适用于将hbase作为海量存储数据而不频繁查询的业务场景。 哈希：Hash散列，将数据打乱。 反转：包括rowkey字段反转和时间戳反转。例如联通就是这样。时间戳加手机号20181123_13191***，引入一张索引表存储以上字段，hbase中的rowkey则是以上字段的反转。 rowkey设计原则： rowkey唯一原则 rowkey长度原则： 二进制数，可以是任意字符，最多给到64kb，建议10-100个字节，但是越短越好，最好不要超过16个字节。原因如下： 数据都是存在Hfile中按照key-value进行存储的，如果rowkey超过了100个字节，1000万条数据，100*1000万=10亿个字节，约为1G，极大浪费Hfile的存储资源。 memstore将缓存部分数据到内存，如果rowkey过大，内存的有效利用率就会降低，从而降低检索效率。 HBase的应用场景 需对数据进行随机读写操作； 大数据上高并发操作，比如每秒对PB级数据进行上千次操作； 读写访问均是非常简单的操作。 HBase的寻址过程（读写数据过程）client–zookeeper–root–meta–region客户端先通过zookeeper获取到root表的地址，通过root表获取.meta表的地址，.meta表上记录了具体的region的地址。]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive知识点总结]]></title>
    <url>%2F2019%2F04%2F30%2FHive%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[hive与mysql的区别 回答思路：hive背景（原理、本质）–&gt;两者操作、本质的差别–&gt;读写差别–&gt;其它差别。 Hive的诞生背景：学mysql的也想入门大数据，但又不会java，于是hive就诞生了。Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表。本质是将HQL语句转化为MR程序。 hive总体来说操作等方面和MySQL没有太大差别。但是本质却有差别，Hive注重联机分析的处理，mysql注重事务的处理。Hive注重的是分析，mysql注重的是处理。 MySQL在写的时候检查字段，hive在读的时候检查字段。所以在新增数据时，hive比较快，只需要直接load就行，而MySQL需要先检查字段。 hive与mysql的其它差别如下： 数据存储位置：Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。 执行：Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。 执行延迟：hive高延迟，mysql低延迟。 可扩展性：hive的可扩展性高，而数据库由于ACID语义限制，扩展性有限。 数据规模：hive可以支持很大规模的数据，数据库可以支持的数据规模较小。 hive内部表和外部表内部表和外部表的区别未被external修饰的是内部表（managed table），被external修饰的为外部表（external table）。 区别： 内部表数据由Hive自身管理，外部表数据由HDFS管理。 删除内部表会直接删除元数据（metadata）及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除。 对内部表的修改会将修改直接同步给元数据，而对外部表的表结构和分区进行修改，则需要修复（MSCK REPAIR TABLE table_name;） 内部表和外部表的转换alter table t_newuser set TBLPROPERTIES(‘EXTERNAL’=‘TRUE’); ###true一定要大写alter table t_newuser set TBLPROPERTIES(‘EXTERNAL’=‘false’); ###false大小写都没关系 hive分区的三种类型 静态分区：加载数据的时候指定分区的值。 动态分区：数据未知，根据分区的值确定创建分区。 混合分区：静态加动态。 hive的存储格式 Textfile未经过压缩的。 Sequencefile：hive为用户提供的二进制存储，本身就压缩，不能使用load方式加载数据。 Rcfile：hive提供的行列混合存储，hive在该格式下，会尽量将附近的行和列的块存储到一起，仍然是压缩格式，查询效率比较高。 Orc是rcfile的升级版本。 Parquet列式存储。 hive窗口函数 FIRST_VALUE：取分组内排序后，截止到当前行，第一个值 。 LAST_VALUE： 取分组内排序后，截止到当前行，最后一个值 。 LEAD(col,n,DEFAULT) ：用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL） 。 LAG(col,n,DEFAULT) ：与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）。 hive的4种排序方式 order by全局排序。全部数据划分到一个reduce上。与sql中的order by类似，不同的是，hive中的order by在严格模式下，必须跟limit。 sort by 每个mapreduce内部排序。 distributed by分区排序，与sql中的group by类似，常与sort by组合使用，distributed by控制map的输出在reduce中如何划分，sort by控制reduce中的数据如何排序。hive要求distributed by语句出现在sort by语句之前。 cluster by，当distributed by与sort by字段相同，可以用cluster by代替该组合，但cluster by 不能跟desc，asc。补充：可以这样书写select a.* from (select * from test cluster by id ) a order by a.id; hive调优方式 Fetch抓取：把不需要MR任务计算的查询语句设置成不执行MR任务。三个参数，none表示禁用Fetch，所有查询都执行MR任务；more在进行select/filter/limit查询时不会运行MR任务；minimal在select/limit的时候不会运行MR任务，但是filter会运行MR任务。hive.fetch.task.conversion 本地模式：让输入的数据量特别小的任务直接在本地节点上进行处理，而不提交到集群。本地模式通过判断文件的大小（默认128MB）和已输入文件的个数（默认4个）来判断是否在本地执行。set hive.exec.mode.local.auto=true; //开启本地 mrset hive.exec.mode.local.auto.inputbytes.max=50000000;set hive.exec.mode.local.auto.input.files.max=10; 表的优化：优化手段有join、行列过滤、分区、分桶、动态分区等等。 避免数据倾斜：通过合理设置map和reduce数、小文件合并等方式尽量保证负载均衡。或为了避免因为map或reduce任务卡死导致数据倾斜，通常也设置推测执行。 推测执行：为了避免因为程序的BUG/负载不均衡/资源分布不均等原因导致同一作业中某一任务运行速度过慢，设置推测执行，为该任务启动一个备份任务，同时执行，最先运行完成的计算结果作为最终结果。分为map端和reduce端的推测执行。set mapreduce.map.speculative=trueset mapreduce.reduce.speculative=true 并行执行：把没有依赖关系的MR任务设置为并行执行，提高多任务运行时的效率。set hive.exec.parallel=true ; // 开启任务并行执行set hive.exec.parallel.thread.number=8; //默认值为8个任务可以同时运行 严格模式：为了防止一些不正常的查询语句的执行。hive.mapred.mode= strict JVM重用：当有很多小文件的时候，每次运行MR任务都会开启一个JVM进程，JVM频繁的开启关闭消耗大量的性能，所以在处理小文件的时候，可以设置JVM重用，让一个JVM处理多个任务后再关闭。mapreduce.job.jvm.numtasks 压缩：通过压缩对项目进行优化。例如开启 map 输出阶段压缩可以减少 job 中 map 和 Reduce task 间数据传输量。 执行计划：Hive中提供的可以查看Hql语句的执行计划，在执行计划中会生成抽象语法树，在语法树中会显示HQL语句之间的依赖关系以及执行过程。通过这些执行的过程和依赖可以对HQL语句进行优化。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库知识点总结]]></title>
    <url>%2F2019%2F04%2F30%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1. 事务四大特性（ACID）原子性、一致性、隔离性、持久性？ 原子性（Atomicity） 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency） 事务开始前和结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability） 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 2. 事务的并发？事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别？从理论上来说, 事务应该彼此完全隔离, 以避免并发事务所导致的问题，然而, 那样会对性能产生极大的影响, 因为事务必须按顺序运行，在实际开发中, 为了提升性能, 事务会以较低的隔离级别运行， 事务的隔离级别可以通过隔离事务属性指定。 2.1 事务的并发问题 脏读 事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据. 不可重复读 事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。 幻读 幻读解决了不重复读，保证了同一个事务里，查询的结果都是事务开始时的状态（一致性）。 例如：事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作 这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。 而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有跟没有修改一样，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 2.2 事务的隔离级别*数据库隔离级别：是在在数据库操作中，为了有效保证并发读取数据的正确性提出的。* 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 READ UNCOMMITTED（读未提交数据） 允许事务读取未被其他事务提交的变更数据，会出现脏读、不可重复读和幻读问题。 READ COMMITTED（读已提交数据） 只允许事务读取已经被其他事务提交的变更数据，可避免脏读，仍会出现不可重复读和幻读问题。 REPEATABLE READ（可重复读） 确保事务可以多次从一个字段中读取相同的值，在此事务持续期间，禁止其他事务对此字段的更新，可以避免脏读和不可重复读，仍会出现幻读问题。 SERIALIZABLE（序列化） 确保事务可以从一个表中读取相同的行，在这个事务持续期间，禁止其他事务对该表执行插入、更新和删除操作，可避免所有并发问题，但性能非常低。 Oracle支持两种事务隔离级别：READ COMMITTED（默认事务隔离级别），SERIALIZABLE。 MySQL支持四种事务隔离级别，其中REPEATABLE READ为默认事务隔离级别。 3. MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别？ 3.1 InnoDB存储引擎InnoDB是事务型数据库的首选引擎，支持事务安全表（ACID），其它存储引擎都是非事务安全表，支持行锁定和外键，MySQL5.5以后默认使用InnoDB存储引擎。 InnoDB特点： 支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。 如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）。 3.2 MyISAM存储引擎MyISAM基于ISAM存储引擎，并对其进行扩展。它是在Web、数据仓储和其他应用环境下最常使用的存储引擎之一。MyISAM拥有较高的插入、查询速度，但不支持事务，不支持外键。 MyISAM特点： 插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择MyISAM能实现处理高效率。如果应用的完整性、并发性要求比较低，也可以使用 3.3 MEMORY存储引擎MEMORY存储引擎将表中的数据存储到内存中，为查询和引用其他表数据提供快速访问。 MEMORY特点： 所有的数据都在内存中，数据的处理速度快，但是安全性不高。如果需要很快的读写速度，对数据的安全性要求较低，可以选择MEMOEY。 它对表的大小有要求，不能建立太大的表。所以，这类数据库只使用在相对较小的数据库表。 4. MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景？4.1 事务处理上方面MyISAM：强调的是性能， 每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 4.2 锁级别MyISAM：只支持表级锁， 用户在操作MyISAM表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 InnoDB： 支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 5. 查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序？5.1 执行顺序 from:需要从哪个数据表检索数据 where:过滤表中数据的条件 group by:如何将上面过滤出的数据分组 having:对上面已经分组的数据进行过滤的条件 select:查看结果集中的哪个列，或列的计算结果 order by :按照什么样的顺序来查看返回的数据 5.2 语句解析顺序from后面的表关联，是自右向左解析 而where条件的解析顺序是自下而上的。 也就是说，在写SQL的时候，尽量把数据量小的表放在最右边来进行关联（用小表去匹配大表），而把能筛选出小量数据的条件放在where语句的最左边 （用小表去匹配大表） 6. 什么是临时表，临时表什么时候删除？MySQL 临时表在我们需要保存一些临时数据时是非常有用的。临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。 使用其他MySQL客户端程序连接MySQL数据库服务器来创建临时表，那么只有在关闭客户端程序时才会销毁临时表，当然也可以手动删除。 7. MySQL B+Tree索引和Hash索引的区别？CREATE TABLE act_info(id BIGINT NOT NULL AUTO_INCREMENT,act_id VARCHAR(50) NOT NULL COMMENT "活动id",act_name VARCHAR(50) NOT NULL COMMENT "活动名称",act_date datetime NOT NULL,PRIMARY KEY(id),KEY idx_actid_name(act_id,act_name) USING BTREE) ENGINE=INNODB DEFAULT CHARSET=UTF8 ROW_FORMAT=COMPACT COMMENT "活动记录表"; 7.1 B-TREE索引 B-TREE索引的特点 B-TREEB-TREE以B+树结构存储数据，大大加快了数据的查询速度 B-TREE索引在范围查找的SQL语句中更加适合（顺序存储） B-TREE索引使用场景 全值匹配的查询SQL，如 where act_id= ‘1111_act’ 联合索引汇中匹配到最左前缀查询，如联合索引 KEY idx_actid_name(act_id,act_name) USING BTREE，只要条件中使用到了联合索引的第一列，就会用到该索引，但如果查询使用到的是联合索引的第二列act_name，该SQL则便无法使用到该联合索引（注：覆盖索引除外） 匹配模糊查询的前匹配，如where act_name like ‘11_act%’ 匹配范围值的SQL查询，如where act_date &gt; ‘9865123547215’（not in和&lt;&gt;无法使用索引） 覆盖索引的SQL查询，就是说select出来的字段都建立了索引 7.2 HASH索引HASH的特点 Hash索引基于Hash表实现，只有查询条件精确匹配Hash索引中的所有列才会用到hash索引 存储引擎会为Hash索引中的每一列都计算hash码，Hash索引中存储的即hash码，所以每次读取都会进行两次查询 Hash索引无法用于排序 Hash不适用于区分度小的列上，如性别字段 8. 聚集索引和非聚集索引区别？聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。 8.1 聚集索引聚集索引表记录的排列顺序和索引的排列顺序一致，所以查询效率快，只要找到第一个索引值记录，其余就连续性的记录在物理也一样连续存放。聚集索引对应的缺点就是修改慢，因为为了保证表中记录的物理和索引顺序一致，在记录插入的时候，会对数据页重新排序。 聚集索引类似于新华字典中用拼音去查找汉字，拼音检索表于书记顺序都是按照a~z排列的，就像相同的逻辑顺序于物理顺序一样，当你需要查找a,ai两个读音的字，或是想一次寻找多个傻(sha)的同音字时，也许向后翻几页，或紧接着下一行就得到结果了。 8.2 非聚集索引非聚集索引制定了表中记录的逻辑顺序，但是记录的物理和索引不一定一致，两种索引都采用B+树结构，非聚集索引的叶子层并不和实际数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针方式。非聚集索引层次多，不会造成数据重排。 非聚集索引类似在新华字典上通过偏旁部首来查询汉字，检索表也许是按照横、竖、撇来排列的，但是由于正文中是a~z的拼音顺序，所以就类似于逻辑地址于物理地址的不对应。同时适用的情况就在于分组，大数目的不同值，频繁更新的列中，这些情况即不适合聚集索引。 9. 乐观锁和悲观锁？数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。 乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 更多详情 10. 非关系型数据库和关系型数据库区别，优势比较？10.1 非关系型数据库的优势 性能 NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。 可扩展性 同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 10.2 关系型数据库的优势 复杂查询 可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持 使得对于安全性能很高的数据访问要求得以实现。 小结：对于这两类数据库，对方的优势就是自己的弱势，反之亦然。NOSQL数据库慢慢开始具备SQL数据库的一些复杂查询功能，比如MongoDB。对于事务的支持也可以用一些系统级的原子操作来实现例如乐观锁之类的方法来曲线救国，比如Redis set nx。 11. 数据库三范式，根据某个场景设计数据表？ 第一范式(确保每列保持原子性) 第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。 第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。 上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 第二范式(确保表中的每列都和主键相关) 第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键，如下表所示。 这样就产生一个问题：这个表中是以订单编号和商品编号作为联合主键。这样在该表中商品名称、单位、商品价格等信息不与该表的主键相关，而仅仅是与商品编号相关。所以在这里违反了第二范式的设计原则。 而如果把这个订单信息表进行拆分，把商品信息分离到另一个表中，把订单项目表也分离到另一个表中，就非常完美了。如下所示。 这样设计，在很大程度上减小了数据库的冗余。如果要获取订单的商品信息，使用商品编号到商品信息表中查询即可。 第三范式(确保每列都和主键列直接相关,而不是间接相关) 第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。 比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。如下面这两个表所示的设计就是一个满足第三范式的数据库表。 这样在查询订单信息的时候，就可以使用客户编号来引用客户信息表中的记录，也不必在订单信息表中多次输入客户信息的内容，减小了数据冗余。 12. 使用explain优化sql和索引？知乎详细回答 13.什么是内连接、外连接、交叉连接、笛卡尔积等？ 内连接(INNER JOIN) 等值连接 自然连接 不等连接 外连接(OUTER JOIN) 左外连接(LEFT OUTER JOIN或LEFT JOIN) 右外连接(RIGHT OUTER JOIN或RIGHT JOIN) 全外连接(FULL OUTER JOIN或FULL JOIN) 交叉连接(CROSS JOIN) 没有WHERE 子句，它返回连接表中所有数据行的笛卡尔积 14. mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决？14.1 MySQL有三种锁的级别表级锁： 开销小，加锁快；不会出现死锁； 锁定粒度大，发生锁冲突的概率最高,并发度最低。 行级锁： 开销大，加锁慢；会出现死锁； 锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 页面锁： 开销和加锁时间界于表锁和行锁之间；会出现死锁； 锁定粒度界于表锁和行锁之间，并发度一般 14.2 什么情况下会造成死锁所谓死锁: 是指两个或两个以上的进程在执行过程中。因争夺资源而造成的一种互相等待的现象,若无外力作用,它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁,这些永远在互相等竺的进程称为死锁进程。 表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。 死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。 14.3 死锁的解决办法 查出的线程杀死 kill 设置锁的超时时间 15. varchar和char的使用场景？ char的长度是不可变的，而varchar的长度是可变的。 定义一个char[10]和varchar[10]。如果存进去的是‘csdn’,那么char所占的长度依然为10，除了字符‘csdn’外，后面跟六个空格，varchar就立马把长度变为4了，取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的。 char的存取速度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找。char也为此付出的是空间的代价，因为其长度固定，所以难免会有多余的空格占位符占据空间，可谓是以空间换取时间效率。varchar是以空间效率为首位。 char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。 varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。 两者的存储数据都非unicode的字符数据。 16. mysql 高并发环境解决方案？ MySQL高并发环境解决方案：分库、分表、分布式、增加二级缓存。 需求分析： 互联网单位 每天大量数据读取，写入，并发性高。 现有解决方式： 水平分库分表，由单点分布到多点数据库中，从而降低单点数据库压力。 集群方案： 解决DB宕机带来的单点DB不能访问问题。 读写分离策略： 极大限度提高了应用中Read数据的速度和并发量。无法解决高写入压力。 17. 数据库崩溃时事务的恢复机制（REDO日志和UNDO日志）？更多详情]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive数据倾斜优化总结]]></title>
    <url>%2F2019%2F04%2F25%2FHive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1. 数据倾斜的原因1.1 操作 关键词 情形 后果 Join 其中一个表较小，但是key集中 分发到某一个或几个reduce上的数据远高于平均值 Join 大表与大表，但是分桶的判断字段0值或空值过多 这些空值都由一个reduce处理，非常慢 group by group by 维度过小，某值的数量过多 处理某值的reduce耗时 Count Distinct 某特殊值过多 处理此特殊值的reduce耗时 1.2 原因 key分布不均匀 业务数据本身的特性 建表时考虑不周 某些SQL语句本身就有数据倾斜 1.3 表现 任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大； 单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多； 最长时长远大于平均时长。 2. 数据倾斜的解决方案2.1 参数调节 set hive.map.aggr = true 在map中会做部分聚集操作，效率更高但需要更多的内存。 set hive.groupby.skewindata = true 数据倾斜的时候进行负载均衡，查询计划生成两个MR job，第一个job先进行key随机分配处理，随机分布到Reduce中，每个Reduce做部分聚合操作，先缩小数据量。第二个job再进行真正的group by key处理，根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Key被分布到同一个Reduce中），完成最终的聚合操作。 set hive.merge.mapfiles=true 当出现小文件过多，需要合并小文件 set hive.exec.reducers.bytes.per.reducer=1000000000 （单位是字节） 每个reduce能够处理的数据量大小，默认是1G。 hive.exec.reducers.max=999 最大可以开启的reduce个数，默认是999个。在只配了hive.exec.reducers.bytes.per.reducer以及hive.exec.reducers.max的情况下，实际的reduce个数会根据实际的数据总量/每个reduce处理的数据量来决定。 set mapred.reduce.tasks=-1 实际运行的reduce个数，默认是-1，可以认为指定，但是如果认为在此指定了，那么就不会通过实际的总数据量hive.exec.reducers.bytes.per.reducer来决定reduce的个数了。 2.2 SQL语句优化1.大小表Join 使用map join让小的维度表（1000条以下的记录条数） 先进内存，在map端完成reduce。如下：select /*+ mapjoin(a) */ a.c1, b.c1 ,b.c2from a join b where a.c1 = b.c1; 2.大表Join大表 把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。如下：select * from log a left outer join users b on case when a.user_id is null then concat('hive',rand()) else a.user_id end = b.user_id; 3.count distinct大量相同特殊值 count distinct时，将值为null的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。 执行如select a,count(distinct b) from t group by a; 类型的SQL时，会出现数据倾斜的问题 可替换成select a,sum(1) from (select a, b from t group by a,b) group by a; 4.group by维度过小 采用sum() group by的方式来替换count(distinct)完成计算。 5.不同数据类型关联产生数据倾斜 用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。 select * from users aleft outer join logs bon a.usr_id = cast(b.user_id as string) 6.小表不小不大，怎么用 map join 解决倾斜问题 使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子:select * from log aleft outer join users bon a.user_id = b.user_id; users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。解决方法：select /*+mapjoin(x)*/* from log a left outer join ( select /*+mapjoin(c)*/d.* from ( select distinct user_id from log ) c join users d on c.user_id = d.user_id ) x on a.user_id = b.user_id;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlload用法总结]]></title>
    <url>%2F2019%2F04%2F25%2Fsqlload%E7%94%A8%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[总体流程 创建bad、log、ctl、discard目录 在数据库中创建需要导入的表 编写控制文件，例如test.ctl ，放到ctl目录下（数据文件也在ctl目录下） 执行命令 sqlldr username/password@10.119.169.126:1521/dtlkdvapdb control=’ctl\test.ctl’ log=’log\test.log’ bad=’bad\test.log’ discard=’discard\test.log’ direct=true; 导入完成后需要检查log和bad日志，看是否存在问题导致数据并未入库 导入txt文件编写控制文件如下：OPTIONS(BINDSIZE=2097152,READSIZE=2097152,SKIP=1,ERRORS=-1,ROWS=50000)LOAD DATACHARACTERSET AL32UTF8INFILE 'C:\个人总结\sqlload导入数据\ctl\TFKBT.txt' "STR X'0a'"INSERT INTO TABLE TFKBTFIELDS TERMINATED BY X'01' TRAILING NULLCOLS (mandt,spras,fkber,fkbtx) 注意FIELDS TERMINATED BY X’01’中的01为每行数据之间的分隔符，不同的数据分隔符可能会不同 导入csv文件编写控制文件如下：options (skip = 1,rows = 5000)LOAD DATA CHARACTERSET AL32UTF8INFILE 'C:\个人总结\sqlload导入数据\ctl\数据.csv' appendINTO TABLE ODS_DEAL_DETAILfields terminated by ','Optionally enclosed by '"'(com_id_a,com_name_a,com_id_b,com_name_b,sbj_code,sbj_name)]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据架构随记]]></title>
    <url>%2F2019%2F04%2F22%2F%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E9%9A%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[更多详情请点击]]></content>
      <categories>
        <category>数据</category>
      </categories>
      <tags>
        <tag>数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop常用命令]]></title>
    <url>%2F2019%2F04%2F22%2Fhadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[hadoop fs -ls / 显示hdfs目录结构hadoop fs -du / 显示该目录中每个文件或目录的大小hadoop fs -du -s / 显示该目录总大小hadoop fs -mkdir /home 在hdfs指定目录内创建新目录hadoop fs -touchz /wahaha 创建一个空文件hadoop fs -rm /wahaha 删除一个文件hadoop fs -rmr /home 删除一个目录hadoop dfs -mv /hello /hello2 重命名hadoop dfs -cp /testdir/slaves /test 复制文件到指定目录hadoop dfs -cat /hello 查看文件hadoop fs -put hello / 上传文件到hdfshadoop fs -put hellodir/ / 上传目录到hdfshadoop dfs -get /hello localdir 将某个文件down至本地已有目录下hadoop dfs -getmerge /hellodir localdir 将指定目录下的所有内容merge成一个文件，下载到本地hadoop dfs -copyFromLocal hello / 类似put，但只能是从本地复制到hdfshadoop dfs -copyToLocal /home localdir 类似get，但只能把文件从hdfs下载到本地 hadoop job -list 列出将完成的jobhadoop job -kill jobid 关闭一个job]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle删除重复数据]]></title>
    <url>%2F2019%2F01%2F22%2FOracle%E5%88%A0%E9%99%A4%E9%87%8D%E5%A4%8D%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1.查找表中多余的重复记录，重复记录是根据单个字段（id）来判断select * from 表 where id in (select id from 表 group by id having count(id) &gt; 1) 2.查找表中多余的重复记录（多个字段）select * from 表 a where (a.id,a.seq) in (select id,seq from 表 group by id,seq having count(*) &gt; 1) 3.删除表中多余的重复记录，重复记录是根据单个字段（id）来判断，只留有rowid最小的记录delete from 表 where id in (select id from 表 group by id having count(id) &gt; 1) and rowid not in (select min(rowid) from 表 group by id having count(*)&gt;1) 4.删除表中多余的重复记录（多个字段），只留有rowid最小的记录delete from 表 a where (a.id,a.seq) in (select id,seq from 表 group by id,seq having count(*) &gt; 1) and rowid not in (select min(rowid) from 表 group by id,seq having count(*)&gt;1) 5.查找表中多余的重复记录（多个字段），不包含rowid最小的记录select * from 表 a where (a.id,a.seq) in (select id,seq from 表 group by id,seq having count(*) &gt; 1) and rowid not in (select min(rowid) from 表 group by id,seq having count(*)&gt;1)]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle行列转换]]></title>
    <url>%2F2018%2F12%2F08%2FOracle%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[pivot 列转行 select * from (select name, nums from demo) pivot(sum(nums) for name in ('苹果' 苹果, '橘子', '葡萄', '芒果')); 注意： pivot（聚合函数 for 列名 in（类型）） ，其中 in(‘’) 中可以指定别名，in中还可以指定子查询，比如 select distinct code from customers unpivot 行转列 select id , name, jidu, xiaoshou from Fruit unpivot (xiaoshou for jidu in (q1, q2, q3, q4)); 注意： unpivot没有聚合函数，xiaoshou、jidu字段也是临时的变量 更多详细内容请点这里]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle树操作、递归查询]]></title>
    <url>%2F2018%2F12%2F04%2FOracle%E6%A0%91%E6%93%8D%E4%BD%9C%E3%80%81%E9%80%92%E5%BD%92%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[select … from tablenamestart with 条件1connect by 条件2where 条件3; 例：select * from tablestart with org_id = ‘HBHqfWGWPy’connect by prior org_id = parent_id; 简单说来是将一个树状结构存储在一张表里，比如一个表中存在两个字段:org_id，parent_id那么通过表示每一条记录的parent是谁，就可以形成一个树状结构。 用上述语法的查询可以取得这棵树的所有记录。 其中： 条件1是根结点的限定语句，当然可以放宽限定条件，以取得多个根结点，实际就是多棵树 条件2是连接条件，其中用prior表示上一条记录，比如connect by prior org_id = parent_id就是说上一条记录的org_id是本条记录的parent_id即本记录的父亲是上一条记录 条件3是过滤条件，用于对返回的所有记录进行过滤 在扫描树结构表时，需要依此访问树结构的每个节点，一个节点只能访问一次，其访问的步骤如下： 第一步：从根节点开始 第二步：访问该节点 第三步：判断该节点有无未被访问的子节点，若有，则转向它最左侧的未被访问的子节，并执行第二步，否则执行第四步 第四步：若该节点为根节点，则访问完毕，否则执行第五步 第五步：返回到该节点的父节点，并执行第三步骤 总之：扫描整个树结构的过程也即是中序遍历树的过程]]></content>
      <categories>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用操作]]></title>
    <url>%2F2018%2F08%2F23%2FLinux%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[文件的操作cpcp -a file1 file2 # 连同file1的所有特性把文件复制为file2cp file1 file2 dir1 # 将文件file1、file2复制到目录dir下 mv用于移动文件、目录、更名mv -f # 强制覆盖mv -i # 若目标文件存在，就询问是否进行覆盖操作 移动文件（多个） mv file1.txt file2.txt /home/test 移动目录 mv dir1/ /home/test 重命名文件或目录 mv file1.txt file2.txt # 文件mv dir1/ dir2/ # 目录 rm用于删除文件和目录rm -f # 强制删除rm -r # 递归删除，常用于目录删除（有提示）rm -i # 询问是否进行删除操作rm -rf dir # 强制删除目录dir中的所有文件和目录（不做提示） mkdir、touchmkdir dir1 # 在当前目录下创建目录touch file1 # 在当前目录下创建文件 tar用于对文件进行打包，默认不进行压缩。以下五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。-c: 建立压缩档案-x：解压-t：查看内容-r：向压缩归档文件末尾追加文件-u：更新原压缩包中的文件 下面的参数是根据需要在压缩或解压档案时可选的。-z：有gzip属性的-j：有bz2属性的-Z：有compress属性的-v：显示所有过程-O：将文件解开到标准输出 -f（必选参数）: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。tar -cf all.tar *.jpg # 将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。tar -rf all.tar *.gif # 将所有.gif的文件增加到all.tar的包里面去。-r是表示增加文件的意思。tar -uf all.tar logo.gif # 更新原来tar包all.tar中logo.gif文件，-u是表示更新文件的意思。tar -tf all.tar # 列出all.tar包中所有文件，-t是列出文件的意思tar -xf all.tar # 解出all.tar包中所有文件，-x是解开的意思 常用命令tar xvf FileName.tar # 解包tar cvf FileName.tar DirName # 打包 zip和unzip对文件打包、压缩或解压zip -r xxx.zip ./ # 压缩当前目录内文件为xxx.zip文件unzip filename.zip # 解压zip文件到当前目录 文件的编辑vi和vim插入模式：在此模式下可以输入字符，按ESC将回到命令模式。命令模式：可以移动光标、删除字符等。低行模式：可以保存文件、退出vi、设置vi、查找等功能(低行模式也可以看作是命令模式里的)。 打开文件、保存、关闭文件(vi命令模式下使用)vi filename # 打开filename文件:w # 保存文件 :w vpser.net # 保存至vpser.net文件 :q # 退出编辑器，如果文件已修改请使用下面的命令 :q! # 退出编辑器，且不保存 :wq # 退出编辑器，且保存文件 插入文本或行(vi命令模式下使用，执行下面命令后将进入插入模式，按ESC键可退出插入模式)a # 在当前光标位置的右边添加文本 i # 在当前光标位置的左边添加文本 A # 在当前行的末尾位置添加文本 I # 在当前行的开始处添加文本(非空字符的行首) O # 在当前行的上面新建一行 o # 在当前行的下面新建一行 R # 替换(覆盖)当前光标位置及后面的若干文本 J # 合并光标所在行及下一行为一行(依然在命令模式) 移动光标(vi命令模式下使用)上下左右方向键 h # 向左j # 向下k # 向上l # 向右空格键 # 向右Backspace # 向左Enter # 移动到下一行首- # 移动到上一行首 删除、恢复字符或行(vi命令模式下使用)x # 删除当前字符 nx # 删除从光标开始的n个字符 dd # 删除当前行 ndd # 向下删除当前行在内的n行 u # 撤销上一步操作 U # 撤销对当前行的所有操作 搜索(vi命令模式下使用)/vpser # 向光标下搜索vpser字符串 ?vpser # 向光标上搜索vpser字符串 n # 向下搜索前一个搜索动作 N # 向上搜索前一个搜索动作 跳至指定行的行首(vi命令模式下使用)n+ # 向下跳n行 n- # 向上跳n行 nG # 跳到行号为n的行 G # 跳至最后一行 设置行号(vi命令模式下使用):set nu # 显示行号 :set nonu # 取消显示行号 复制、粘贴(vi命令模式下使用)yy # 将当前行复制到缓存区，也可以用 &quot;ayy&quot; 复制，&quot;a&quot; 为缓冲区，a也可以替换为a到z的任意字母，可以完成多个复制任务。 nyy # 将当前行向下n行复制到缓冲区，也可以用 &quot;anyy&quot; 复制，&quot;a&quot; 为缓冲区，a也可以替换为a到z的任意字母，可以完成多个复制任务。 yw # 复制从光标开始到词尾的字符。 nyw # 复制从光标开始的n个单词。 y^ # 复制从光标到行首的内容。y$ # 复制从光标到行尾的内容。 p # 粘贴剪切板里的内容在光标后，如果使用了前面的自定义缓冲区，建议使用&quot;ap&quot; 进行粘贴。 P # 粘贴剪切板里的内容在光标前，如果使用了前面的自定义缓冲区，建议使用&quot;aP&quot; 进行粘贴。 替换(vi命令模式下使用):s/old/new # 用new替换行中首次出现的old :s/old/new/g # 用new替换行中所有的old :n,m s/old/new/g # 用new替换从n到m行里所有的old :%s/old/new/g # 用new替换当前文件里所有的old]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建博客常用操作]]></title>
    <url>%2F2018%2F08%2F22%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[写一篇文章输入创建文章命令，生成一个md文件(/blog/source/_posts/) hexo n &quot;hello&quot; == hexo new &quot;hello&quot; # 新建文章 用编辑器打开hello.md文件，编写完后保存 本地查看效果执行下面语句，执行完再登录localhost:4000查看效果（执行完不要按Ctrl+C，不然就停止了） hexo g == hexo generate # 生成网页hexo s == hexo server # 启动本地服务预览 部署hexo d == hexo deploy # 部署到github 部署后我们可以浏览器搜 username.github.io 查看自己的博客效果，比如我的monkeyip.github.io]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
